<!DOCTYPE html>
<html data-rh="lang" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/branch-latest.js"></script><script async="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/analytics.js"></script><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach((function(n){n(o,t)})),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach((function(e){n(e,l,f)}))}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener)</script><script defer="defer" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/16180790160.js"></script><title>Tutorial:
 Abdominal CT Image Synthesis with Variational Autoencoders using 
PyTorch | by Lasse Hansen | MICCAI Educational Initiative | Medium</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-11-18T22:42:40.932Z"><meta data-rh="true" name="title" content="Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch | by Lasse Hansen | MICCAI Educational Initiative | Medium"><meta data-rh="true" property="og:title" content="Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch"><meta data-rh="true" property="twitter:title" content="Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch"><meta data-rh="true" name="twitter:site" content="@MICCAIstudents"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/933c29bb1c90"><meta data-rh="true" property="al:android:url" content="medium://p/933c29bb1c90"><meta data-rh="true" property="al:ios:url" content="medium://p/933c29bb1c90"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany This is a short introduction on how to make CT image synthesis…"><meta data-rh="true" property="og:description" content="By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany"><meta data-rh="true" property="twitter:description" content="By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany"><meta data-rh="true" property="og:url" content="https://medium.com/miccai-educational-initiative/tutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90"><meta data-rh="true" property="al:web:url" content="https://medium.com/miccai-educational-initiative/tutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1076/1*WmZYkt6cQJ1cenl8kZTlFw.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1076/1*WmZYkt6cQJ1cenl8kZTlFw.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://medium.com/@hansenlasse"><meta data-rh="true" name="author" content="Lasse Hansen"><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="7 min read"><meta data-rh="true" name="parsely-post-id" content="933c29bb1c90"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://medium.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/unbound.css"><link data-rh="true" rel="author" href="https://medium.com/@hansenlasse"><link data-rh="true" rel="canonical" href="https://medium.com/miccai-educational-initiative/tutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/933c29bb1c90"><link data-rh="true" rel="icon" href="https://miro.medium.com/1*m-R_BkNf1Qjr1YbyOIJY2w.png"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="430" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k2{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k2{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:35px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{position:absolute}.u{top:0}.v{left:0}.w{right:0}.x{z-index:500}.y{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ah{max-width:1192px}.ai{min-width:0}.aj{width:100%}.ak{height:65px}.an{flex:1 0 auto}.ao{height:25px}.ap{fill:rgba(25, 25, 25, 1)}.aq{border-left:1px solid rgba(204, 204, 204, 1)}.ar{margin-left:15px}.as{margin-right:14px}.at{height:24px}.au{width:1px}.av{height:36px}.aw{width:89px}.ax{visibility:hidden}.ay{margin-left:16px}.az{display:none}.bb{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bc{font-size:14px}.bd{line-height:20px}.be{color:rgba(117, 117, 117, 1)}.bf{color:rgba(26, 137, 23, 1)}.bg{fill:rgba(26, 137, 23, 1)}.bh{font-size:inherit}.bi{border:inherit}.bj{font-family:inherit}.bk{letter-spacing:inherit}.bl{font-weight:inherit}.bm{padding:0}.bn{margin:0}.bo:hover{cursor:pointer}.bp:hover{color:rgba(15, 115, 12, 1)}.bq:hover{fill:rgba(15, 115, 12, 1)}.br:disabled{cursor:default}.bs:disabled{color:rgba(26, 137, 23, 0.3)}.bt:disabled{fill:rgba(26, 137, 23, 0.3)}.bu{flex:0 0 auto}.bv{fill:rgba(117, 117, 117, 1)}.bw{justify-content:flex-end}.bx{margin-left:12px}.by{margin-right:6px}.bz{margin-top:16px}.ca{margin-bottom:16px}.cb{display:inherit}.cc{max-width:210px}.cd{text-overflow:ellipsis}.ce{overflow:hidden}.cf{white-space:nowrap}.cg{display:inline-block}.ch{border:none}.ci{outline:none}.cj{font:inherit}.ck{font-size:16px}.cl{opacity:0}.cm{background-color:transparent}.cn{color:rgba(41, 41, 41, 1)}.co::placeholder{color:rgba(117, 117, 117, 1)}.cp{position:relative}.cq{padding:0px}.cr{width:0px}.cs{transition:width 140ms ease-in, padding 140ms ease-in}.ct{color:inherit}.cu{fill:inherit}.cv:hover{color:rgba(25, 25, 25, 1)}.cw:hover{fill:rgba(25, 25, 25, 1)}.cx:disabled{color:rgba(117, 117, 117, 1)}.cy:disabled{fill:rgba(117, 117, 117, 1)}.cz{padding:4px}.db{margin-right:16px}.de{margin:15px 0}.df{width:32px}.dg{height:32px}.dh{flex-direction:row}.di{width:calc(100% + 25px)}.dj{height:calc(100% + 25px)}.dk{top:50%}.dl{left:50%}.dm{transform:translateX(-50%) translateY(-50%)}.dn{pointer-events:none}.do{border-radius:50%}.dp{border-top:1px solid rgba(230, 230, 230, 1)}.dq{height:54px}.dr{margin-right:40px}.ds{width:88px}.dt{overflow:auto}.du{flex:0 1 auto}.dv{list-style-type:none}.dw{line-height:40px}.dx{overflow-x:auto}.dy{align-items:flex-start}.dz{margin-top:20px}.ea{padding-top:20px}.eb{height:80px}.ec{margin-bottom:0px}.ed{margin-top:0px}.eg{padding-left:24px}.eh{padding-right:24px}.ei{margin-left:auto}.ej{margin-right:auto}.ek{max-width:728px}.el{box-sizing:border-box}.em{top:calc(100vh + 100px)}.en{bottom:calc(100vh + 100px)}.eo{width:10px}.ep{word-break:break-word}.eq{word-wrap:break-word}.er:after{display:block}.es:after{content:""}.et:after{clear:both}.eu{max-width:680px}.ev{line-height:1.23}.ew{letter-spacing:0}.ex{font-style:normal}.ey{font-family:fell, Georgia, Cambria, "Times New Roman", Times, serif}.ft{margin-bottom:-0.27em}.fu{margin-top:32px}.fv{justify-content:space-between}.fz{height:48px}.ga{width:48px}.gb{margin-bottom:2px}.gd{max-height:20px}.ge{display:-webkit-box}.gf{-webkit-line-clamp:1}.gg{-webkit-box-orient:vertical}.gi:hover{text-decoration:underline}.gj{margin-left:8px}.gk{font-size:13px}.gl{padding:0px 8px 1px}.gm{background:0}.gn{border-color:rgba(26, 137, 23, 1)}.go:hover{border-color:rgba(15, 115, 12, 1)}.gp:disabled{cursor:inherit}.gq:disabled{opacity:0.3}.gr:disabled:hover{color:rgba(26, 137, 23, 1)}.gs:disabled:hover{fill:rgba(26, 137, 23, 1)}.gt:disabled:hover{border-color:rgba(26, 137, 23, 1)}.gu{border-radius:4px}.gv{border-width:1px}.gw{border-style:solid}.gx{text-decoration:none}.gy{align-items:flex-end}.hg{padding-right:6px}.hh{margin-right:8px}.hi{fill:rgba(61, 61, 61, 1)}.hj{margin-right:-6px}.hk:hover{fill:rgba(8, 8, 8, 1)}.hl:focus{fill:rgba(8, 8, 8, 1)}.hm{line-height:1.58}.hn{letter-spacing:-0.004em}.ho{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.ij{margin-bottom:-0.46em}.ik{font-style:italic}.il{text-decoration:underline}.im{max-width:1076px}.is{clear:both}.iu{cursor:zoom-in}.iv{z-index:auto}.iw:focus{transform:scale(1.01)}.ix{transition:opacity 100ms 400ms}.iy{height:100%}.iz{will-change:transform}.ja{transform:translateZ(0)}.jb{margin:auto}.jc{background-color:rgba(242, 242, 242, 1)}.jd{padding-bottom:25.8364312267658%}.je{height:0}.jf{filter:blur(20px)}.jg{transform:scale(1.1)}.jh{visibility:visible}.ji{margin-top:10px}.jj{text-align:center}.jm{font-weight:700}.jn{margin-bottom:14px}.jo{padding-top:24px}.jp{padding-bottom:10px}.jq{background-color:rgba(8, 8, 8, 1)}.jr{height:3px}.js{width:3px}.jt{margin-right:20px}.ju{padding-bottom:NaN%}.jv{padding-bottom:80.76208178438661%}.jw{font-weight:600}.jx{will-change:opacity}.jy{position:fixed}.jz{width:188px}.ka{transform:translateX(406px)}.kb{top:calc(65px + 54px + 14px)}.ke{top:159px}.kg{width:131px}.kh{flex-direction:column}.ki{font-weight:500}.kj{padding-bottom:28px}.kk{border-bottom:1px solid rgba(230, 230, 230, 1)}.kl{padding-bottom:20px}.km{padding-top:2px}.kn{max-height:120px}.ko{-webkit-line-clamp:6}.kp{padding:4px 12px 6px}.kq{padding-top:28px}.kr{margin-bottom:19px}.ks{margin-left:-3px}.ky{outline:0}.kz{border:0}.la{user-select:none}.lb{cursor:pointer}.lc> svg{pointer-events:none}.ld:active{border-style:none}.le{-webkit-user-select:none}.lf:focus{fill:rgba(117, 117, 117, 1)}.lg:hover{fill:rgba(117, 117, 117, 1)}.lo button{text-align:left}.lp{opacity:0.4}.lq{cursor:not-allowed}.lr{padding-right:9px}.ma{margin-top:40px}.mb{flex-wrap:wrap}.mc{margin-top:25px}.md{margin-bottom:8px}.me{line-height:22px}.mf{border-radius:3px}.mg{padding:5px 10px}.mh{background:rgba(242, 242, 242, 1)}.mi{max-width:155px}.mm{top:1px}.na{margin-left:-1px}.nb{margin-left:-4px}.nj{padding-right:8px}.nk{padding-top:32px}.nl{margin-bottom:25px}.nn{margin-bottom:32px}.no{min-height:80px}.nt{width:80px}.nu{padding-left:102px}.nw{line-height:18px}.nx{letter-spacing:0.077em}.ny{text-transform:uppercase}.nz{margin-bottom:6px}.oa{font-size:22px}.ob{line-height:28px}.oc{max-width:555px}.od{max-width:450px}.oe{line-height:24px}.og{max-width:550px}.oh{margin-top:5px}.oi{height:40px}.oj{width:40px}.ok{font-size:12px}.ol{line-height:16px}.om{letter-spacing:0.083em}.on{padding-top:8px}.oo{padding-top:25px}.oq{background:rgba(255, 255, 255, 1)}.or{padding:60px 0}.os{background-color:rgba(0, 0, 0, 0.9)}.ou{padding-bottom:48px}.ov{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.ow{margin:0 -12px}.ox{margin:0 12px}.oy{flex:1 1 0}.oz{padding-bottom:8px}.pa:hover{color:rgba(255, 255, 255, 0.99)}.pb:hover{fill:rgba(255, 255, 255, 0.99)}.pc:disabled{color:rgba(255, 255, 255, 0.7)}.pd:disabled{fill:rgba(255, 255, 255, 0.7)}.pe{font-size:20px}.pf{color:rgba(255, 255, 255, 0.98)}.pg{color:rgba(255, 255, 255, 0.7)}.ph{fill:rgba(255, 255, 255, 1)}.pi{height:22px}.pj{width:200px}.pp{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ag{margin:0 64px}.fp{font-size:48px}.fq{margin-top:0.55em}.fr{line-height:60px}.fs{letter-spacing:-0.011em}.hf{margin-left:30px}.if{font-size:21px}.ig{margin-top:2em}.ih{line-height:32px}.ii{letter-spacing:-0.003em}.ir{margin-top:56px}.kx{margin-right:5px}.ln{margin-top:5px}.lz{padding-left:6px}.mo{display:inline-block}.mt{margin-left:7px}.mu{margin-top:8px}.mz{width:25px}.nh{padding-left:7px}.ni{top:3px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.he{margin-left:30px}.jk{margin-left:auto}.jl{text-align:center}.kw{margin-right:5px}.lm{margin-top:5px}.ly{padding-left:6px}.mn{display:inline-block}.mr{margin-left:7px}.ms{margin-top:8px}.my{width:25px}.nf{padding-left:7px}.ng{top:3px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.da{display:flex}.hd{margin-left:30px}.kv{margin-right:5px}.ll{margin-top:5px}.lw{padding-left:6px}.lx{top:3px}.ml{display:inline-block}.mp{margin-left:7px}.mq{margin-top:8px}.mx{width:15px}.ne{padding-left:3px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.al{height:56px}.am{display:flex}.ba{display:block}.dc{margin-left:10px}.dd{margin-right:10px}.ee{margin-bottom:0px}.ef{height:110px}.fx{margin-top:32px}.fy{flex-direction:column-reverse}.hb{margin-bottom:30px}.hc{margin-left:0px}.ku{margin-left:8px}.lj{margin-top:2px}.lk{margin-right:8px}.lu{padding-left:6px}.lv{top:3px}.mk{display:inline-block}.mw{width:15px}.nd{padding-left:3px}.nm{padding-top:0}.np{margin-bottom:24px}.nq{align-items:center}.nr{width:102px}.ns{position:relative}.nv{padding-left:0}.of{margin-top:24px}.op{border-top:none}.ot{padding:32px 0}.pk{width:140px}.pl{margin-bottom:16px}.pm{margin-top:30px}.pn{width:100%}.po{flex-direction:row}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ab{margin:0 24px}.ez{font-size:34px}.fa{margin-top:0.56em}.fb{line-height:42px}.fc{letter-spacing:-0.016em}.fw{margin-top:32px}.gc{margin-bottom:0px}.gz{margin-bottom:30px}.ha{margin-left:0px}.hp{font-size:18px}.hq{margin-top:1.56em}.hr{line-height:28px}.hs{letter-spacing:-0.003em}.in{margin-top:40px}.kt{margin-left:8px}.lh{margin-top:2px}.li{margin-right:8px}.ls{padding-left:6px}.lt{top:3px}.mj{display:inline-block}.mv{width:15px}.nc{padding-left:3px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.af{margin:0 64px}.fl{font-size:48px}.fm{margin-top:0.55em}.fn{line-height:60px}.fo{letter-spacing:-0.011em}.ib{font-size:21px}.ic{margin-top:2em}.id{line-height:32px}.ie{letter-spacing:-0.003em}.iq{margin-top:56px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ae{margin:0 48px}.fh{font-size:48px}.fi{margin-top:0.55em}.fj{line-height:60px}.fk{letter-spacing:-0.011em}.hx{font-size:21px}.hy{margin-top:2em}.hz{line-height:32px}.ia{letter-spacing:-0.003em}.ip{margin-top:56px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ac{margin:0 24px}.fd{font-size:34px}.fe{margin-top:0.56em}.ff{line-height:42px}.fg{letter-spacing:-0.016em}.ht{font-size:18px}.hu{margin-top:1.56em}.hv{line-height:28px}.hw{letter-spacing:-0.003em}.io{margin-top:40px}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="print">.z{display:none}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.gh{max-height:none}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.it{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.kc{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 1230px)">.kd{display:none}</style><style type="text/css" data-fela-rehydration="430" data-fela-type="RULE" media="all and (max-width: 1198px)">.kf{display:none}</style><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*WmZYkt6cQJ1cenl8kZTlFw.png"],"url":"https:\u002F\u002Fmedium.com\u002Fmiccai-educational-initiative\u002Ftutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90","dateCreated":"2019-11-18T22:42:40.932Z","datePublished":"2019-11-18T22:42:40.932Z","dateModified":"2019-11-18T23:16:20.510Z","headline":"Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch","name":"Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch","description":"By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany This is a short introduction on how to make CT image synthesis…","identifier":"933c29bb1c90","keywords":["Lite:true","Tag:Machine Learning","Tag:Deep Learning","Tag:Variational Autoencoder","Tag:Pytorch","Tag:Medical Image Analysis","Topic:Machine Learning","Publication:miccai-educational-initiative","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:0"],"author":{"@type":"Person","name":"Lasse Hansen","url":"https:\u002F\u002Fmedium.com\u002F@hansenlasse"},"creator":["Lasse Hansen"],"publisher":{"@type":"Organization","name":"MICCAI Educational Initiative","url":"https:\u002F\u002Fmedium.com\u002Fmiccai-educational-initiative","logo":{"@type":"ImageObject","width":147,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F147\u002F1*HP_AOII2mV8xNr1Kj7cffA.png"}},"mainEntityOfPage":"https:\u002F\u002Fmedium.com\u002Fmiccai-educational-initiative\u002Ftutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><div class="s"><nav class="s t u v w c x y z"><div><div class="s c"><div class="n p"><div class="ab ac ae af ag ah ai aj"><div class="ak n o al am"><div class="n o an x"><a rel="noopener" href="https://medium.com/?source=post_page-----933c29bb1c90--------------------------------"><svg viewBox="0 0 1043.63 592.71" class="ao ap"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a><div class="aq ar as at au s g"></div><div class="s g"><a href="https://medium.com/miccai-educational-initiative?source=post_page-----933c29bb1c90--------------------------------" rel="noopener"><div class="av aw s"><img alt="MICCAI Educational Initiative" class="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1HP_AOII2mV8xNr1Kj7cffA_002.png" width="89" height="36"></div></a></div><div class="jh" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="ay az ba"><span class="bb b bc bd be"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F933c29bb1c90&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----933c29bb1c90--------------------------------" class="bf bg bh bi bj bk bl bm bn bo bp bq br bs bt" rel="noopener nofollow">Open in app</a></span></div></div></div><div class="s bu x"><div class="n o bw"><div class="bx by n f"><div class="cg" aria-hidden="false"><div class="n"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy"><span class="cz s"><svg width="25" height="25" viewBox="0 0 25 25" class="bv"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input class="ch ci cj ck bd cl cm cn co cp cq cr cs" placeholder="Search MICCAI Educational Initiative"></div></div></div><div class="az da"><a href="https://medium.com/search?source=post_page-----933c29bb1c90--------------------------------" aria-label="Search" class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener"><span class="ay db s dc dd"><svg width="25" height="25" viewBox="0 0 25 25" class="bv"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></a></div><a aria-label="Bookmark post" class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/me/list/queue?source=post_page-----933c29bb1c90--------------------------------"><span class="db s g"><svg width="25" height="25" viewBox="0 0 25 25" class="bv"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></span></a><div class="db n dd"><div class="cg" aria-hidden="false"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy s"><span class="de s"><svg width="25" height="25" viewBox="-293 409 25 25" class="bv"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></span></button></div></div><div class="n" aria-hidden="false"><div class="n o"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy"><div class="cp df dg"><div class="bg n dh o p t di dj dk dl dm dn"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Vesa Alexandru" class="s do dg df" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/0UPBPhpvitZdcnAku_002.png" width="32" height="32"></div></button></div></div></div></div></div></div></div></div><div class="dp az c ba"><div class="n p"><div class="ab ac ae af ag ah ai aj"><div class="dq ce n o"><div class="dr s bu"><a href="https://medium.com/miccai-educational-initiative?source=post_page-----933c29bb1c90--------------------------------" rel="noopener"><div class="av ds s"><img alt="MICCAI Educational Initiative" class="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1HP_AOII2mV8xNr1Kj7cffA.png" width="88" height="36"></div></a></div><div class="dt s du"><ul class="dv bn dw cf dx n dy g dz ea eb"></ul></div></div></div></div></div></div></nav><div class="ec ed ak s ee ef"></div><div class="s z"><div class="pw px aj iy jy py pz lb cl qa dn" aria-hidden="true"></div><div class="qb jy qc qd qe pw iy el dt qf qg pt qh qi qj pn qk ql qm qn" aria-hidden="true"><div class="qq qr n o dh fv"><h2 class="bb ki pe oe ew cn">Responses (2)</h2><div class="n dh"><div class="s cp qs"><div class="bn s cp w"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" data-testid="close-button" aria-label="close"><svg width="25" height="25" viewBox="0 0 25 25" class="bv"><path d="M18.13 6.11l-5.61 5.61-5.6-5.61-.81.8 5.61 5.61-5.61 5.61.8.8 5.61-5.6 5.61 5.6.8-.8-5.6-5.6 5.6-5.62"></path></svg></button></div></div></div></div><div class="qt sh s"><div class="qd ru gu n kh rv rw rx"><div class="n o fv cp ry rz cl sa sb"><div class="n o"><div class="cp df dg"><div class="bg n dh o p t di dj dk dl dm dn"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Vesa Alexandru" class="s do dg df" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/0UPBPhpvitZdcnAku.png" width="32" height="32"></div><div class="bx n dy kh p"><h4 class="bb b bc bd cn">Vesa Alexandru</h4></div></div></div><div class="n kh"><div class="sc sd"><div class="si s"><div data-gramm="false" role="textbox" data-slate-editor="true" data-slate-node="value" style="outline: currentcolor none medium; white-space: pre-wrap; overflow-wrap: break-word;" contenteditable="true"><div data-slate-node="element"><p><span data-slate-node="text"><span data-slate-leaf="true"><span style="pointer-events: none; display: inline-block; width: 0px; max-width: 100%; white-space: nowrap; opacity: 0.333; user-select: none; font-style: normal; font-weight: normal; text-decoration: none;" contenteditable="false">What are your thoughts?</span><span data-slate-zero-width="n" data-slate-length="0">﻿<br></span></span></span></p></div></div></div></div><div class="n se ry sf cl sa"><div class="sg"><button class="bb b bc bd cn kp r gm sj sk hk sl bo gp gq sm sn so gu gv gw el cg gx">Cancel</button></div><button class="bb b bc bd sp kp ph sq gn sr go bo gp gq ss gt gu gv gw el cg gx" disabled="disabled">Respond</button></div></div></div></div><div class="s"><div class="qt"><div class="aj iy"><div class="re oo kk s"><div class="n dh fv"><div class="n o dh"><div class="cp df dg"><div class="bg n dh o p t di dj dk dl dm dn"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Manuel Kurz" class="s do dg df" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/01lDF0axjxeq8Rkhj.jpg" width="32" height="32"></div><div class="rg s"><div class="n dh"><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@xmaniix91?source=responses-----933c29bb1c90----0----------------------------"><h4 class="bb b bc bd cn">Manuel Kurz</h4></a></div><a class="ct cu bh bi bj bk bl bm bn bo gi br cx cy" rel="noopener nofollow" href="https://medium.com/@xmaniix91/hello-ive-got-a-question-please-regarding-a-fully-connected-layer-implemented-as-conv2d-with-5509c860eb3d?source=responses-----933c29bb1c90----0----------------------------"><h4 class="bb b bc bd be">9 months ago</h4></a></div></div><button class="lb kz hi cw rf"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="oh s"><pre class="rh"><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">Hello,<br>I’ve got a question please, regarding “ A fully-connected layer (implemented as Conv2d) with 1024 channels and a kernel size of 8x8 is followed by two further fully-connected layers with 512 channels, each predicting one of the latent vectors…...</div></h4></div></pre></div><button class="ch rd lb"><span class="bb b gk bd bf">Read More</span></button><div class="rj n o dh"><div class="n o"><div class="s cp kt ku kv kw kx"><div class=""><button class="bm ky kz la lb lc ld pp r lf lg"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="rk s"></div><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@xmaniix91/hello-ive-got-a-question-please-regarding-a-fully-connected-layer-implemented-as-conv2d-with-5509c860eb3d?responsesOpen=true&amp;source=responses-----933c29bb1c90----0----------------------------"><button class="lb kz bm"><div class="rl n o dh"><div class="s cp rm"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s cp nc rn nd ro ne rp rq rr rs rt"></div></div></button></a></div></div></div></div><div class="qt"><div class="aj iy"><div class="re oo s"><div class="n dh fv"><div class="n o dh"><img alt="Sunny Yang" class="s do dg df" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/0_3xuCilj4p1UzAnl.jpg" width="32" height="32"><div class="rg s"><div class="n dh"><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@sunnyyang1576?source=responses-----933c29bb1c90----1----------------------------"><h4 class="bb b bc bd cn">Sunny Yang</h4></a></div><a class="ct cu bh bi bj bk bl bm bn bo gi br cx cy" rel="noopener nofollow" href="https://medium.com/@sunnyyang1576/thank-you-for-the-great-research-44659d1bf221?source=responses-----933c29bb1c90----1----------------------------"><h4 class="bb b bc bd be">4 months ago</h4></a></div></div><button class="lb kz hi cw rf"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="oh s"><pre class="rh"><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">Thank you for the great research.</div></h4></div><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">I have two question:</div></h4></div><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">1. Is there a particular reason for using the L1 loss as the reconstruction loss instead of L2 Loss?</div></h4></div><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">2. For the image, do we need to do any transformation before we feed into the VAE?</div></h4></div><div class="ri s"><h4 class="bb b bc bd cn"><div class="oe">3. In terms of…...</div></h4></div></pre></div><button class="ch rd lb"><span class="bb b gk bd bf">Read More</span></button><div class="rj n o dh"><div class="n o"><div class="s cp kt ku kv kw kx"><div class=""><button class="bm ky kz la lb lc ld pp r lf lg"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="rk s"></div><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@sunnyyang1576/thank-you-for-the-great-research-44659d1bf221?responsesOpen=true&amp;source=responses-----933c29bb1c90----1----------------------------"><button class="lb kz bm"><div class="rl n o dh"><div class="s cp rm"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s cp nc rn nd ro ne rp rq rr rs rt"></div></div></button></a></div></div></div></div></div></div></div><article><section class="eg eh ei ej aj ek el s"></section><span class="s"></span><div><div class="t v pq en eo dn"></div><section class="ep eq er es et"><div class="n p"><div class="ab ac ae af ag eu ai aj"><div><h1 id="d80f" class="ev ew ex ey b ez fa fb fc fd fe ff fg fh fi fj fk fl fm fn fo fp fq fr fs ft cn">Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch</h1><div class="fu"><div class="n fv fw fx fy"><div class="o n"><div><a rel="noopener" href="https://medium.com/@hansenlasse?source=post_page-----933c29bb1c90--------------------------------"><img alt="Lasse Hansen" class="s do fz ga" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1dmbNkD5D-u45r44go_cf0g_002.png" width="48" height="48"></a></div><div class="bx aj s"><div class="n"><div style="flex:1"><span class="bb b bc bd cn"><div class="gb n o gc"><span class="bb b bc bd ce gd cd ge gf gg gh cn"><a class="ct cu bh bi bj bk bl bm bn bo gi br cx cy" rel="noopener" href="https://medium.com/@hansenlasse?source=post_page-----933c29bb1c90--------------------------------">Lasse Hansen</a></span><div class="gj s bu h"><button class="bb b gk bd bf gl gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx">Follow</button></div></div></span></div></div><span class="bb b bc bd be"><span class="bb b bc bd ce gd cd ge gf gg gh be"><div><a class="ct cu bh bi bj bk bl bm bn bo gi br cx cy" rel="noopener" href="https://medium.com/miccai-educational-initiative/tutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90?source=post_page-----933c29bb1c90--------------------------------">Nov 19, 2019</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div><div class="n gy gz ha hb hc hd he hf z"><div class="n o"><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on twitter"><svg width="29" height="29" class="bv"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></button></div><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on linkedin"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="bv"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on facebook"><svg width="29" height="29" class="bv"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></button></div><div class="hh s"><div><div class="hi"><div><div class="cg" role="tooltip" aria-hidden="false" aria-describedby="1" aria-labelledby="1"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="hj s an"><div class="cg" aria-hidden="false"><div class="cg" aria-hidden="false"><div class="s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="More options"><svg class="r hk hl" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><p id="c422" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><em class="ik">By
 Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute
 of Medical Informatics at the University of Lübeck, Germany</em></p><p id="ce16" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">This
 is a short introduction on how to make CT image synthesis with 
variational autoencoders (VAEs) work using the excellent deep learning 
framework PyTorch¹. We will tackle common issues and solutions and 
provide code examples that enable you to run your own synthesis on real 
abdominal CT images. This blog post is accompanied by two GitHub 
repositories containing an <a href="https://github.com/multimodallearning/mec19_vae_tutorial" class="ct il" rel="noopener nofollow">exercise notebook</a> and a corresponding <a href="https://github.com/multimodallearning/mec19_vae_tutorial_solution" class="ct il" rel="noopener nofollow">runnable solution</a>.
 In addition, both repositories contain code versions that run directly 
in Google Colab² with GPU support. We recommend everyone who is 
interested in an improved understanding of the conceptual insights of 
VAEs, its application to medical image synthesis and implementation 
details in PyTorch to first work on the exercises and only then come 
back to this post.</p><figure class="in io ip iq ir is ei ej paragraph-image"><div role="button" tabindex="0" class="it iu cp iv aj iw"><div class="ei ej im"><div class="jb s cp jc"><div class="jd je s"><div class="cl ix t u v iy aj ce iz ja"><img alt="Image for post" class="t u v iy aj jf jg ax pv" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw_003.png" width="1076" height="278"></div><img alt="Image for post" class="pt pu t u v iy aj c" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw.png" srcset="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw_004.png 276w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw_005.png 552w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw_006.png 640w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1WmZYkt6cQJ1cenl8kZTlFw_002.png 700w" sizes="700px" width="1076" height="278"><noscript><img alt="Image for post" class="t u v iy aj" src="https://miro.medium.com/max/2152/1*WmZYkt6cQJ1cenl8kZTlFw.png" width="1076" height="278" srcSet="https://miro.medium.com/max/552/1*WmZYkt6cQJ1cenl8kZTlFw.png 276w, https://miro.medium.com/max/1104/1*WmZYkt6cQJ1cenl8kZTlFw.png 552w, https://miro.medium.com/max/1280/1*WmZYkt6cQJ1cenl8kZTlFw.png 640w, https://miro.medium.com/max/1400/1*WmZYkt6cQJ1cenl8kZTlFw.png 700w" sizes="700px"/></noscript></div></div></div></div><figcaption class="ji jj ek ei ej jk jl bb b bc bd be" data-selectable-paragraph="">Synthetically generated abdominal CT image slices.</figcaption></figure><p id="c4b0" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">Why do we need synthesis in medical imaging?<br></strong>Data
 scarcity and privacy concerns are common in medical image analysis that
 prevent researchers from having access to large annotated datasets. 
Common data augmentation, including geometric and intensity 
transformations and distortions, provide only limited realism and cannot
 fill the missing gaps of the required widely distributed and densely 
sampled space of training images. Synthesis of “new” medical images is 
one solution to overcome this issue.</p><p id="8dd3" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">What are VAEs and how can we use them for image synthesis?</strong><br>Classical autoencoders are not suitable for synthetic image generation. When training an encoder and decoder net with a small<strong class="ho jm"> </strong>but
 unrestricted latent space, sampling from this latent space does not 
produce realistic images. To overcome this problem, a variational 
autoencoder³<strong class="ho jm"> </strong>aims to learn a probability density function and restrict<strong class="ho jm"> </strong>it<strong class="ho jm"><em class="ik"> </em></strong>to<strong class="ho jm"> </strong>a<strong class="ho jm"> </strong>multivariate normal<strong class="ho jm"> </strong>distribution.
 Therefore, the encoder net predicts reasonable distribution parameters 
(μ and σ) and the decoder net maps encodings from the interpretable<strong class="ho jm"> </strong>to a hidden distribution<strong class="ho jm"> </strong>and
 finally, to the image space. To learn the network parameters we 
optimize the reconstruction loss and the Kullback-Leibler divergence, 
that measures how well the learned density function approximates a 
normal distribution.</p><p id="eb59" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">What are common issues when training VAEs and how to solve them?</strong><br>Generative
 models, e.g. GANs, enable improved image synthesis but are hard to 
train — VAEs are mathematically elegant and thus superior architectures 
that are more robust, but produce synthetic images of limited spatial 
sharpness. Perceptual losses that penalise deviations from not only the 
output images themselves but also their early feature representations 
can be included in training VAEs to reduce blurry outcomes and obtain 
sharp predictions. However, ImageNet pre-trained networks commonly used 
in computer vision (e.g. VGG-16) are incapable of supporting medical VAE
 training, thus a task-specific pre-training for perceptual feature 
extraction is necessary. Finally, architectural choices of the expanding
 decoder architecture have a strong influence on the obtained outcome. 
We show how replacing commonly used transposed convolutions with 
bilinear interpolation layers can improve the visual outcome of VAEs for
 medical image synthesis.</p></div></div></section><div class="n p fu jn jo jp" role="separator"><span class="jq do cg jr js jt"></span><span class="jq do cg jr js jt"></span><span class="jq do cg jr js"></span></div><section class="ep eq er es et"><div class="n p"><div class="ab ac ae af ag eu ai aj"><p id="f0b7" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">In
 the following section we will describe how to train VAEs for the 
synthesis of abdominal CT images, solving common problems such as blurry
 image outcomes and explicitly adressing implementation details in 
PyTorch.</p><p id="8fe6" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">Training Data</strong><br>As
 training data we sampled approximately 2500 2D slices from 43 patients 
of the TCIA pancreas data set⁴ and randomly cropped patches of 256x256 
pixels. We restricted the sampling to the region of the pancreas and 
additionally, extracted 2 further slices from above and below the 
current image slice to provide more contextual information. Thus, a 
single training image has dimensions 3x256x256.</p><figure class="in io ip iq ir is ei ej paragraph-image"><div role="button" tabindex="0" class="it iu cp iv aj iw"><div class="ei ej im"><div class="jb s cp jc"><div class="jd je s"><div class="cl ix t u v iy aj ce iz ja"><img alt="Image for post" class="t u v iy aj jf jg ax pv" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ.png" width="1076" height="278"></div><img alt="Image for post" class="pt pu t u v iy aj c" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ_003.png" srcset="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ_004.png 276w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ_006.png 552w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ_005.png 640w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Rxk6rl0HHHJhDiva6bZqHQ_002.png 700w" sizes="700px" width="1076" height="278"><noscript><img alt="Image for post" class="t u v iy aj" src="https://miro.medium.com/max/2152/1*Rxk6rl0HHHJhDiva6bZqHQ.png" width="1076" height="278" srcSet="https://miro.medium.com/max/552/1*Rxk6rl0HHHJhDiva6bZqHQ.png 276w, https://miro.medium.com/max/1104/1*Rxk6rl0HHHJhDiva6bZqHQ.png 552w, https://miro.medium.com/max/1280/1*Rxk6rl0HHHJhDiva6bZqHQ.png 640w, https://miro.medium.com/max/1400/1*Rxk6rl0HHHJhDiva6bZqHQ.png 700w" sizes="700px"/></noscript></div></div></div></div><figcaption class="ji jj ek ei ej jk jl bb b bc bd be" data-selectable-paragraph="">Exemplary abdominal CT image slices from the TCIA pancreas data set.</figcaption></figure><p id="818d" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">VAE implementation<br></strong>The
 gist given below shows the complete implementation of the VAE in 
PyTorch. The encoder takes image batches of size Bx3x256x256 and 
produces two 512 dimensional latent vectors (μ and σ). It consists of 
nine blocks of Conv2d&gt;BatchNorm&gt;LeakyReLU operations with a kernel
 size of 3x3 and an increasing number of filter channels from 16 to 64. 
Every other layer halves the feature maps resolution by using 
convolutions with stride 2. A fully-connected layer (implemented as 
Conv2d) with 1024 channels and a kernel size of 8x8 is followed by two 
further fully-connected layers with 512 channels, each predicting one of
 the latent vectors (μ and σ). Next, we use the reparameterization trick
 that adds noise to the latent distribution thus introducing a new 
source of randomness, enabling gradients to backpropagate through μ and 
σ. The decoder part takes a Bx512x1x1 and generates a full-sized 
(Bx3x256x256) output image. A fully-connected 1x1 Conv2d layer maps the 
interpretable 512 dimensional latent vector to a hidden state with 1024 
channels, followed by a transposed convolution with kernel size 8 and 64
 channels. Then again, blocks of Conv2d&gt;BatchNorm&gt;LeakyReLU with 
kernel size 3x3 are used, but are alternated with blocks of 
ConvTranspose2d&gt;BatchNorm&gt;LeakyReLU with kernel size 4x4 and 
stride 2 to increase the feature maps resolution. The network 
architecture is finished with a Conv2d with 3 output channels and a tanh
 activation. In addition to the generated output image, the network 
returns the predicted latent vectors μ and σ that are needed for loss 
calculations.</p><figure class="in io ip iq ir is"><div class="jb s cp"><div class="ju je s"></div></div></figure><p id="0cb6" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">VAE training with KLD loss<br></strong>The
 VAE network is trained in an unsupervised fashion by optimizing an L1 
reconstruction loss on the input and generated images. An additional 
loss term is given by the Kullback-Leibler divergence that ensures that 
the learned density function follows a normal distribution. For full 
training code we again refer to the <a href="https://github.com/multimodallearning/mec19_vae_tutorial_solution" class="ct il" rel="noopener nofollow">provided notebooks</a>.</p><figure class="in io ip iq ir is"><div class="jb s cp"><div class="ju je s"></div></div></figure><p id="4ca6" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">Visual
 results of synthetic images sampled randomly from the learned latent 
space are shown in the figure below. The VAE trained with the 
reconstruction and KLD loss already generates anatomically reasonable 
results. Outlines of different organs and anatomical structures such as 
liver, kidneys and vertebra are clearly visible. However, despite being 
realistic, images synthesized with this VAE variant are blurry and miss 
finer details.</p><figure class="in io ip iq ir is ei ej paragraph-image"><div role="button" tabindex="0" class="it iu cp iv aj iw"><div class="ei ej im"><div class="jb s cp jc"><div class="jv je s"><div class="cl ix t u v iy aj ce iz ja"><img alt="Image for post" class="t u v iy aj jf jg ax pv" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA.png" width="1076" height="869"></div><img alt="Image for post" class="pt pu t u v iy aj c" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA_002.png" srcset="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA_004.png 276w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA_005.png 552w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA_006.png 640w, Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/19cAOB2KDKwk8oseiID7kGA_003.png 700w" sizes="700px" width="1076" height="869"><noscript><img alt="Image for post" class="t u v iy aj" src="https://miro.medium.com/max/2152/1*9cAOB2KDKwk8oseiID7kGA.png" width="1076" height="869" srcSet="https://miro.medium.com/max/552/1*9cAOB2KDKwk8oseiID7kGA.png 276w, https://miro.medium.com/max/1104/1*9cAOB2KDKwk8oseiID7kGA.png 552w, https://miro.medium.com/max/1280/1*9cAOB2KDKwk8oseiID7kGA.png 640w, https://miro.medium.com/max/1400/1*9cAOB2KDKwk8oseiID7kGA.png 700w" sizes="700px"/></noscript></div></div></div></div><figcaption class="ji jj ek ei ej jk jl bb b bc bd be" data-selectable-paragraph="">Synthetically generated abdominal CT image slices. <strong class="bb jw">Top</strong>: VAE. <strong class="bb jw">Middle</strong>: VAE with additional perceptual loss. <strong class="bb jw">Bottom</strong>: VAE with additional perceptual loss and bilinear upsampling instead of transposed convolutions.</figcaption></figure><p id="95d1" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">VAE Training with additional perceptual loss<br></strong>To improve the detail-preservation VAEs can be trained with an additional perceptual loss. The perceptual loss<strong class="ho jm"> </strong>(first
 introduced for style transfer and superresolution) adapts the 
reconstruction criterion to more visually meaningful similarity. 
Commonly, pre-trained networks on ImageNet (e.g. VGG-16) are used for 
perceptual feature extraction but due to the different nature of the 
image domains they are incabable of supporting medical VAE training. 
Thus, we trained a fully-convolutional network which roughly follows the
 VGG architecture for CT segmentation of the pancreas. This pre-trained 
model is then used for feature extraction in the perceptual loss 
calculation. Here, mid-level features are extracted from after the ReLUs
 in layers 2, 5 and 9 by registering forward hooks for the corresponding
 layers. After a forward path of the input images through the VGG model 
the feature output tensors can be copied to a list. The same procedure 
is repeated after passing the reconstructed images through the network 
and an L1 loss is employed for each corresponding feature maps from all 
three layers. The generated images after training with the additional 
perceptual loss are less blurry and more details (e.g. of the kidneys or
 the vertebra canal) are recognizable. However, now we observe 
pattern-like image artifacts, which brings us to the last part of this 
tutorial.</p><figure class="in io ip iq ir is"><div class="jb s cp"><div class="ju je s"></div></div></figure><p id="a4d7" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">VAE implementation addressing checkerboard pattern<br></strong>As described in the excellent <a href="https://distill.pub/2016/deconv-checkerboard/" class="ct il" rel="noopener nofollow">interactive blog post from Odena, et al.</a>⁵
 the observed checkerboard pattern stems from the use of the transposed 
convolutional operator in the decoder part of the VAE and can be easily 
reduced by replacing transposed convolutions with a bilinear upsampling 
layer and a subsequent convolution. We can therefore replace every 
transposed convolution in our VAE model with the module definition given
 below and retrain our network. The results show the effectiveness of 
this small modification. Generated images look even sharper and no more 
artifacts can be observed.</p><figure class="in io ip iq ir is"><div class="jb s cp"><div class="ju je s"></div></div></figure></div></div></section><div class="n p fu jn jo jp" role="separator"><span class="jq do cg jr js jt"></span><span class="jq do cg jr js jt"></span><span class="jq do cg jr js"></span></div><section class="ep eq er es et"><div class="n p"><div class="ab ac ae af ag eu ai aj"><p id="db80" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph=""><strong class="ho jm">Where to go from here?<br></strong>To
 make the provided code for the exercises lightweight and fast, we only 
used relatively few training images and small VAE and VGG models. But we
 hope this tutorial sparked someones interest in synthetic medical image
 generation and thus, we are excited to see more detailed, sharper and 
realistic images from the community using more training images, better 
data augmentation techniques, bigger models, improved perceptual loss 
weighting, new network architectures, etc. For state-of-the-art methods 
using VAEs a number of this years MICCAI papers are excellent starting 
points for further reading with applications ranging from joint modality
 completion and segmentation⁶ over frame rate up-conversion in 
echocardiography⁷ to unsupervised anomaly localization⁸.</p></div></div></section><div class="n p fu jn jo jp" role="separator"><span class="jq do cg jr js jt"></span><span class="jq do cg jr js jt"></span><span class="jq do cg jr js"></span></div><section class="ep eq er es et"><div class="n p"><div class="ab ac ae af ag eu ai aj"><p id="00dd" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[1] Paszke, Adam, et al. “Automatic differentiation in pytorch.” (2017). <a href="https://pytorch.org/" class="ct il" rel="noopener nofollow">https://pytorch.org</a></p><p id="84ff" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[2] Google Colab. <a href="https://colab.research.google.com/notebooks/welcome.ipynb" class="ct il" rel="noopener nofollow">https://colab.research.google.com/notebooks/welcome.ipynb</a></p><p id="0c69" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[3] Kingma, Diederik P., et al. “Auto-encoding variational bayes.” <em class="ik">arXiv preprint arXiv:1312.6114</em> (2013). <a href="https://arxiv.org/pdf/1312.6114.pdf" class="ct il" rel="noopener nofollow">https://arxiv.org/pdf/1312.6114.pdf</a></p><p id="af4b" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[4] Roth, Holger R., et al. Data From Pancreas-CT. The Cancer Imaging Archive (2016). <a href="http://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU" class="ct il" rel="noopener nofollow">http://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU</a></p><p id="66b3" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[5] Odena, et al., “Deconvolution and Checkerboard Artifacts”, Distill (2016). <a href="https://distill.pub/2016/deconv-checkerboard/" class="ct il" rel="noopener nofollow">https://distill.pub/2016/deconv-checkerboard/</a></p><p id="d662" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[6] Dorent, Reuben, et al. “Hetero-Modal Variational Encoder-Decoder for Joint Modality Completion and Segmentation.” <em class="ik">arXiv preprint arXiv:1907.11150</em> (2019). <a href="https://arxiv.org/pdf/1907.11150" class="ct il" rel="noopener nofollow">https://arxiv.org/pdf/1907.11150</a></p><p id="6ece" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[7]
 Dezaki, Fatemeh T., et al. “Frame Rate Up-Conversion in 
Echocardiography Using a Conditioned Variational Autoencoder and 
Generative Adversarial Model.” (2019).</p><p id="91a1" class="hm hn ex ho b hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ep cn" data-selectable-paragraph="">[8] Zimmerer, David, et al. “Context-encoding Variational Autoencoder for Unsupervised Anomaly Detection.” <em class="ik">arXiv preprint arXiv:1812.05941</em> (2018). <a href="https://arxiv.org/pdf/1907.02796.pdf" class="ct il" rel="noopener nofollow">https://arxiv.org/pdf/1907.02796.pdf</a></p></div></div></section></div></article><div class="pt dn jy jx aj ps kc kf" data-test-id="post-sidebar"><div class="n p"><div class="ab ac ae af ag ah ai aj"><div class="kg n kh"><div class="st"><div><div class="kj kk s"><a href="https://medium.com/miccai-educational-initiative?source=post_sidebar--------------------------post_sidebar-----------" class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener"><h2 class="bb ki ck bd ew cn ep">MICCAI Educational Initiative</h2></a><div class="kl km s"><h4 class="bb b bc bd ce kn cd ge ko gg gh be">Blogs and more from the annual Medical Image Computing and…</h4></div><div class="cg" aria-hidden="false"><button class="bb b bc bd bf kp gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx"><div class="n dh">Follow</div></button></div></div><div class="kq kr ks n"><div class="n o"><div class="s cp kt ku kv kw kx"><div class=""><button class="bm ky kz la lb lc ld le r lf lg"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s lh li lj lk ll lm ln"><div class="lo"><h4 class="bb b bc bd be"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy">58 </button></h4></div></div></div></div><div class="kr s"><button class="lb kz bm"><div class="lr n o dh"><svg width="25" height="25" class="r" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><div class="s cp ls lt lu lv lw lx ly lz"><h4 class="bb b bc bd be">2<!-- --> </h4></div></div></button></div><div><div class="hi"><div><div class="cg" role="tooltip" aria-hidden="false" aria-describedby="2" aria-labelledby="2"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="pt st jx jy jz dl ka kb kc kd"></div><div><div class="ma is n kh p"><div class="n p"><div class="ab ac ae af ag eu ai aj"><div class="n mb"></div><div class="n o mb"></div><div class="mc s"><ul class="bm bn"><li class="cg dv hh md"><a href="https://medium.com/miccai-educational-initiative/tagged/machine-learning" class="bb b gk me be mf mg gx s mh">Machine Learning</a></li><li class="cg dv hh md"><a href="https://medium.com/miccai-educational-initiative/tagged/deep-learning" class="bb b gk me be mf mg gx s mh">Deep Learning</a></li><li class="cg dv hh md"><a href="https://medium.com/miccai-educational-initiative/tagged/variational-autoencoder" class="bb b gk me be mf mg gx s mh">Variational Autoencoder</a></li><li class="cg dv hh md"><a href="https://medium.com/miccai-educational-initiative/tagged/pytorch" class="bb b gk me be mf mg gx s mh">Pytorch</a></li><li class="cg dv hh md"><a href="https://medium.com/miccai-educational-initiative/tagged/medical-image-analysis" class="bb b gk me be mf mg gx s mh">Medical Image Analysis</a></li></ul></div><div class="mc n fv z"><div class="n dh"><div class="mi s"><span class="s mj mk ml e d"><div class="n o"><div class="s cp kt ku kv kw kx"><div class=""><button class="bm ky kz la lb lc ld le r lf lg"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s lh li lj lk ll lm ln"><div class="cp mm lo"><h4 class="bb b bc bd cn"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy">58<span class="s h g f mn mo">&nbsp;claps</span></button><span class="s h g f mn mo"></span></h4></div></div></div></span><span class="s h g f mn mo"><div class="n dy"><div class="s cp kt ku"><div class=""><button class="bm ky kz la lb lc ld le r lf lg"><svg width="33" height="33" viewBox="0 0 33 33" aria-label="clap"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div><div class="s lh li lj lk mp mq mr ms mt mu"><div class="cp mm lo"><h4 class="bb b bc bd cn"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy">58<span class="s h g f mn mo">&nbsp;claps</span></button><span class="s h g f mn mo"></span></h4></div></div></div></span></div><div class="s mv mw mx my mz"></div><button class="lb kz bm"><div class="lr n o dh"><span class="na s h g f mn mo"><svg width="33" height="33" viewBox="0 0 33 33" fill="none" class="r" aria-label="responses"><path clip-rule="evenodd" d="M24.28 25.5l.32-.29c2.11-1.94 3.4-4.61 3.4-7.56C28 11.83 22.92 7 16.5 7S5 11.83 5 17.65s5.08 10.66 11.5 10.66c1.22 0 2.4-.18 3.5-.5l.5-.15.41.33a8.86 8.86 0 0 0 4.68 2.1 7.34 7.34 0 0 1-1.3-4.15v-.43zm1 .45c0 1.5.46 2.62 1.69 4.44.22.32.01.75-.38.75a9.69 9.69 0 0 1-6.31-2.37c-1.2.35-2.46.54-3.78.54C9.6 29.3 4 24.09 4 17.65 4 11.22 9.6 6 16.5 6S29 11.22 29 17.65c0 3.25-1.42 6.18-3.72 8.3z"></path></svg></span><span class="nb s mj mk ml e d"><svg width="25" height="25" class="r" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></span><div class="s cp nc lt nd lv ne lx nf ng nh ni"><h4 class="bb b bc bd cn">2<!-- --> <span class="s h g f mn mo">response<!-- -->s</span></h4></div></div></button></div><div class="n o"><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on twitter"><svg width="29" height="29" class="bv"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></button></div><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on linkedin"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="bv"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="hg s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Share on facebook"><svg width="29" height="29" class="bv"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></button></div><div class="nj s bu"><div><div class="hi"><div><div class="cg" role="tooltip" aria-hidden="false" aria-describedby="3" aria-labelledby="3"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="cg" aria-hidden="false"><div class="cg" aria-hidden="false"><div class="s bu"><button class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" aria-label="More options"><svg class="r hk hl" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div><div><div class="n p"><div class="ab ac ae af ag eu ai aj"><div class="nk dp nl mc s nm z"><div class="s g"><div class="nn no s cp"><span class="s np am nq"><div class="s t nr ns"><a rel="noopener" href="https://medium.com/@hansenlasse?source=follow_footer-------------------------------------"><img alt="Lasse Hansen" class="s do eb nt" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1dmbNkD5D-u45r44go_cf0g_003.png" width="80" height="80"></a></div><span class="s"><div class="nu s nv"><p class="bb b gk nw nx be ny">Written by</p></div><div class="nu nz n nv"><div class="aj n o fv"><h2 class="bb ki oa ob ew cn"><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@hansenlasse?source=follow_footer-------------------------------------">Lasse Hansen</a></h2><div class="s g"><button class="bb b bc bd bf kp gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx">Follow</button></div></div></div></span></span><div class="nu oc s nv ba"><div class="od s"><h4 class="bb b ck oe be"></h4></div><div class="az of ba"><button class="bb b bc bd bf kp gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx">Follow</button></div></div></div><div class="nk s"></div><div class="nn no s cp"><span class="s np am nq"><div class="s t nr ns"><a href="https://medium.com/miccai-educational-initiative?source=follow_footer-------------------------------------" rel="noopener"><img alt="MICCAI Educational Initiative" class="gu nt eb" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/12uwx_TptFlxCoN7HnpNAeA.png" width="80" height="80"></a></div><span class="s"><div class="nu nz n nv"><div class="aj n o fv"><h2 class="bb ki oa ob ew cn"><a href="https://medium.com/miccai-educational-initiative?source=follow_footer-------------------------------------" class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener">MICCAI Educational Initiative</a></h2><div class="s g"><div class="cg" aria-hidden="false"><button class="bb b bc bd bf kp gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx"><div class="n dh">Follow</div></button></div></div></div></div></span></span><div class="nu og s nv ba"><div class="od s"><h4 class="bb b ck oe be">Blogs and more from the annual Medical Image Computing and Computer-Assisted Intervention (MICCAI) educational challenge</h4></div><div class="az of ba"><div class="cg" aria-hidden="false"><button class="bb b bc bd bf kp gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx"><div class="n dh">Follow</div></button></div></div></div></div></div><div class="az ba"><div class="jo s"><div class="n dh"><div class="oh s"><a rel="noopener" href="https://medium.com/@hansenlasse?source=follow_footer-------------------------------------"><img alt="Lasse Hansen" class="s do oi oj" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1dmbNkD5D-u45r44go_cf0g.png" width="40" height="40"></a></div><div class="bx s"><p class="bb b ok ol om be ny">Written by</p><div class="n dh"><h2 class="bb ki ck bd ew cn"><a class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener" href="https://medium.com/@hansenlasse?source=follow_footer-------------------------------------">Lasse Hansen</a></h2><div class="bx s"><button class="bb b gk bd bf gl gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx">Follow</button></div></div><div class="on s"><h4 class="bb b bc bd be"></h4></div></div></div><div class="jo s"><div class="n dh"><a href="https://medium.com/miccai-educational-initiative?source=follow_footer-------------------------------------" rel="noopener"><img alt="MICCAI Educational Initiative" class="gu oj oi" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/12uwx_TptFlxCoN7HnpNAeA_002.png" width="40" height="40"></a><div class="bx s"><div class="n dh"><h2 class="bb ki ck bd ew cn"><a href="https://medium.com/miccai-educational-initiative?source=follow_footer-------------------------------------" class="ct cu bh bi bj bk bl bm bn bo cv cw br cx cy" rel="noopener">MICCAI Educational Initiative</a></h2><div class="bx s"><div class="cg" aria-hidden="false"><button class="bb b gk bd bf gl gm bg gn bp bq go bo gp gq gr gs gt gu gv gw el cg gx"><div class="n dh">Follow</div></button></div></div></div><div class="on s"><h4 class="bb b bc bd be">Blogs and more from the annual Medical Image Computing and Computer-Assisted Intervention (MICCAI) educational challenge</h4></div></div></div></div></div></div></div><div class="oo dp s nm op z"></div></div></div><div class="s oq z"><div class="n p"><div class="ab ac ae af ag ah ai aj"></div></div></div></div></div></div><div class="or s os ot"><div class="n p"><div class="ab ac ae af ag ah ai aj"><div class="ou ov nn n fv g"><div class="ow n fv"><div class="ox s oy"><div class="oz s"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener"><h2 class="bb ki pe oe ew pf">Learn more.</h2></a></div><h4 class="bb b bc bd pg">Medium
 is an open platform where 170 million readers come to find insightful 
and dynamic thinking.
                        Here, expert and undiscovered voices alike dive 
into the heart of any topic and bring new ideas to the surface. <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo br pc pd il" rel="noopener">Learn more</a></h4></div><div class="ox s oy"><div class="oz s"><a href="https://medium.com/topics?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener"><h2 class="bb ki pe oe ew pf">Make <!-- -->Medium<!-- --> yours<!-- -->.</h2></a></div><h4 class="bb b bc bd pg">Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. <a href="https://medium.com/topics?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo br pc pd il" rel="noopener">Explore</a></h4></div><div class="ox s oy"><div class="oz s"><a href="https://about.medium.com/creators/?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener"><h2 class="bb ki pe oe ew pf">Share your thinking.</h2></a></div><h4 class="bb b bc bd pg">If you have a story to tell, knowledge to share, or a perspective to offer — welcome home.
        It’s easy and free to post your thinking on any topic. <a href="https://about.medium.com/creators/?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo br pc pd il" rel="noopener">Write on Medium</a></h4></div></div></div><div class="n kh"><div class="n o fv"><a aria-label="Go to homepage" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener" href="https://medium.com/?source=post_page-----933c29bb1c90--------------------------------"><svg viewBox="0 0 3940 610" class="ph pi"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><h4 class="bb b bc bd pg"><div class="on pj n fv pk am"><h4 class="bb b ck oe pf"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo gi br pc pd" rel="noopener">About</a></h4><h4 class="bb b ck oe pf"><a href="https://help.medium.com/hc/en-us?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo gi br pc pd" rel="noopener">Help</a></h4><h4 class="bb b ck oe pf"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo gi br pc pd" rel="noopener">Legal</a></h4></div></h4></div><div class="az pl pm am"><h4 class="bb b ck oe pg">Get the Medium app</h4></div><div class="az pl pn am po"><div class="db s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener nofollow"><img alt="A button that says 'Download on the App Store', and if clicked it will lead you to the iOS App store" class="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----933c29bb1c90--------------------------------" class="ct cu bh bi bj bk bl bm bn bo pa pb br pc pd" rel="noopener nofollow"><img alt="A button that says 'Get it on, Google Play', and if clicked it will lead you to the Google Play store" class="" src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__ = "main-20201204-191518-0ced06a182"</script><script>window.__GRAPHQL_URI__ = "https://medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"auroraPage":{"isAuroraPageEnabled":false},"config":{"nodeEnv":"production","version":"main-20201204-191518-0ced06a182","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20201204-191518-0ced06a182"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"main-20201204-191518-0ced06a182","commit":"0ced06a182716869f1c3c0d30aa6d9ef3361fb44"}},"datacenter":"us"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e","9dc80918cc93","8a9336e5bb4","cef6983b292","54c98c43354d","193b68bd4fba","b7e45b22fec3","55760f21cdc5"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","errorTracking":"bugsnag"},"debug":{"requestId":"8a209662-2037-4ea8-bdd5-6a40add5b205","branchDeployConfig":null,"originalSpanCarrier":{"ot-tracer-spanid":"69b4e9d033f1b173","ot-tracer-traceid":"244101d46499b1c4","ot-tracer-sampled":"true"}},"session":{"user":{"id":"a0286b59c498"},"xsrf":"a9c4440b8315","isSpoofed":false},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fmedium.com\u002Fmiccai-educational-initiative\u002Ftutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90","host":"medium.com","hostname":"medium.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register"},"postRead":false},"client":{"isBot":false,"isEu":true,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"inAppBrowserName":"","routingEntity":{"type":"DEFAULT","explicit":false},"supportsWebp":true,"useNeedForSpeed":false},"multiVote":{"clapsPerPost":{}},"tracing":{}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_resume_reading_toast","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_admin_braintree_discounts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_follow_pages","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_client_error_tracking","valueType":{"__typename":"VariantFlagString","value":"bugsnag"}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_curation_priority_queue_experiment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_daily_read_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_dedicated_series_tab_api_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_detailed_billing_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_different_grid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_disregard_trunc_state_for_footer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_following_publications_list","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_ranked_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_write_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_post_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_daily_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_lo_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pay_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_cd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_highlights_view_only","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_profile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_header_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_more_on_coronavirus","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_checkout_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_suspended_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_seo_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_table_of_contents","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sohne","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ticks_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_topic_lifecycle_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unbound","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unfiltered_cf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_embed_commands","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_iceland_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"STRIPE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:a0286b59c498"},"meterPost({\"postId\":\"933c29bb1c90\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"933c29bb1c90\"})":{"__ref":"Post:933c29bb1c90"}},"User:a0286b59c498":{"id":"a0286b59c498","__typename":"User","username":"vesaalexandru95","name":"Vesa Alexandru","imageId":"0*UPBPhpvitZdcnAku.","mediumMemberAt":1586155340000,"hasPastMemberships":true,"isPartnerProgramEnrolled":false,"email":"vesaalexandru95@gmail.com","unverifiedEmail":"","createdAt":1527340254190,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasDomain":false,"dismissableFlags":[]},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":3},"ImageMetadata:":{"id":"","__typename":"ImageMetadata"},"ImageMetadata:1*HP_AOII2mV8xNr1Kj7cffA.png":{"id":"1*HP_AOII2mV8xNr1Kj7cffA.png","__typename":"ImageMetadata","originalWidth":246,"originalHeight":100},"User:589b95840bd2":{"id":"589b95840bd2","__typename":"User"},"ImageMetadata:1*2uwx_TptFlxCoN7HnpNAeA.png":{"id":"1*2uwx_TptFlxCoN7HnpNAeA.png","__typename":"ImageMetadata"},"Collection:fccddfb484c0":{"id":"fccddfb484c0","__typename":"Collection","domain":null,"googleAnalyticsId":null,"slug":"miccai-educational-initiative","colorBehavior":"ACCENT_COLOR","isAuroraVisible":false,"favicon":{"__ref":"ImageMetadata:"},"name":"MICCAI Educational Initiative","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFFFFFFF","point":0},{"__typename":"ColorPoint","color":"#FFE8F3E8","point":0.1},{"__typename":"ColorPoint","color":"#FFE8F3E8","point":0.2},{"__typename":"ColorPoint","color":"#FFD1E7D1","point":0.6},{"__typename":"ColorPoint","color":"#FFA3D0A2","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF1A8917","point":0},{"__typename":"ColorPoint","color":"#FF11800E","point":0.1},{"__typename":"ColorPoint","color":"#FF0F730C","point":0.2},{"__typename":"ColorPoint","color":"#FF095407","point":1}]},"tintBackgroundSpectrum":null},"customStyleSheet":null,"tagline":"Blogs and more from the annual Medical Image Computing and…","isAuroraEligible":false,"viewerIsEditor":false,"logo":{"__ref":"ImageMetadata:1*HP_AOII2mV8xNr1Kj7cffA.png"},"navItems":[],"creator":{"__ref":"User:589b95840bd2"},"subscriberCount":71,"avatar":{"__ref":"ImageMetadata:1*2uwx_TptFlxCoN7HnpNAeA.png"},"isEnrolledInHightower":false,"newsletterV3":null,"viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":false,"isUserSubscribedToCollectionEmails":false,"viewerIsMuting":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"Blogs and more from the annual Medical Image Computing and Computer-Assisted Intervention (MICCAI) educational challenge","ampEnabled":false,"twitterUsername":"MICCAIstudents","facebookPageId":null},"User:1e6972113baf":{"id":"1e6972113baf","__typename":"User","isSuspended":false,"name":"Lasse Hansen","hasCompletedProfile":false,"bio":"","imageId":"1*dmbNkD5D-u45r44go_cf0g.png","customStyleSheet":null,"hasDomain":false,"username":"hansenlasse","isAuroraVisible":true,"socialStats":{"__typename":"SocialStats","followerCount":2},"isBlocking":false,"mediumMemberAt":0,"isMuting":false,"isFollowing":false,"allowNotes":true,"newsletterV3":null,"viewerIsUser":false,"twitterScreenName":"","isPartnerProgramEnrolled":false},"Paragraph:812aa795d8a6_0":{"id":"812aa795d8a6_0","__typename":"Paragraph","name":"d80f","text":"Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_1":{"id":"812aa795d8a6_1","__typename":"Paragraph","name":"c422","text":"By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":134,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_2":{"id":"812aa795d8a6_2","__typename":"Paragraph","name":"ce16","text":"This is a short introduction on how to make CT image synthesis with variational autoencoders (VAEs) work using the excellent deep learning framework PyTorch¹. We will tackle common issues and solutions and provide code examples that enable you to run your own synthesis on real abdominal CT images. This blog post is accompanied by two GitHub repositories containing an exercise notebook and a corresponding runnable solution. In addition, both repositories contain code versions that run directly in Google Colab² with GPU support. We recommend everyone who is interested in an improved understanding of the conceptual insights of VAEs, its application to medical image synthesis and implementation details in PyTorch to first work on the exercises and only then come back to this post.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":370,"end":387,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fmultimodallearning\u002Fmec19_vae_tutorial","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":408,"end":425,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fmultimodallearning\u002Fmec19_vae_tutorial_solution","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_3":{"id":"812aa795d8a6_3","__typename":"Paragraph","name":"f348","text":"Synthetically generated abdominal CT image slices.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*WmZYkt6cQJ1cenl8kZTlFw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_4":{"id":"812aa795d8a6_4","__typename":"Paragraph","name":"c4b0","text":"Why do we need synthesis in medical imaging?\nData scarcity and privacy concerns are common in medical image analysis that prevent researchers from having access to large annotated datasets. Common data augmentation, including geometric and intensity transformations and distortions, provide only limited realism and cannot fill the missing gaps of the required widely distributed and densely sampled space of training images. Synthesis of “new” medical images is one solution to overcome this issue.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_5":{"id":"812aa795d8a6_5","__typename":"Paragraph","name":"8dd3","text":"What are VAEs and how can we use them for image synthesis?\nClassical autoencoders are not suitable for synthetic image generation. When training an encoder and decoder net with a small but unrestricted latent space, sampling from this latent space does not produce realistic images. To overcome this problem, a variational autoencoder³ aims to learn a probability density function and restrict it to a multivariate normal distribution. Therefore, the encoder net predicts reasonable distribution parameters (μ and σ) and the decoder net maps encodings from the interpretable to a hidden distribution and finally, to the image space. To learn the network parameters we optimize the reconstruction loss and the Kullback-Leibler divergence, that measures how well the learned density function approximates a normal distribution.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":58,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":184,"end":185,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":335,"end":336,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":393,"end":394,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":396,"end":397,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":399,"end":400,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":401,"end":402,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":421,"end":422,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":574,"end":575,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":599,"end":600,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":396,"end":397,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_6":{"id":"812aa795d8a6_6","__typename":"Paragraph","name":"eb59","text":"What are common issues when training VAEs and how to solve them?\nGenerative models, e.g. GANs, enable improved image synthesis but are hard to train — VAEs are mathematically elegant and thus superior architectures that are more robust, but produce synthetic images of limited spatial sharpness. Perceptual losses that penalise deviations from not only the output images themselves but also their early feature representations can be included in training VAEs to reduce blurry outcomes and obtain sharp predictions. However, ImageNet pre-trained networks commonly used in computer vision (e.g. VGG-16) are incapable of supporting medical VAE training, thus a task-specific pre-training for perceptual feature extraction is necessary. Finally, architectural choices of the expanding decoder architecture have a strong influence on the obtained outcome. We show how replacing commonly used transposed convolutions with bilinear interpolation layers can improve the visual outcome of VAEs for medical image synthesis.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":64,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_7":{"id":"812aa795d8a6_7","__typename":"Paragraph","name":"f0b7","text":"In the following section we will describe how to train VAEs for the synthesis of abdominal CT images, solving common problems such as blurry image outcomes and explicitly adressing implementation details in PyTorch.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_8":{"id":"812aa795d8a6_8","__typename":"Paragraph","name":"8fe6","text":"Training Data\nAs training data we sampled approximately 2500 2D slices from 43 patients of the TCIA pancreas data set⁴ and randomly cropped patches of 256x256 pixels. We restricted the sampling to the region of the pancreas and additionally, extracted 2 further slices from above and below the current image slice to provide more contextual information. Thus, a single training image has dimensions 3x256x256.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_9":{"id":"812aa795d8a6_9","__typename":"Paragraph","name":"60f5","text":"Exemplary abdominal CT image slices from the TCIA pancreas data set.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Rxk6rl0HHHJhDiva6bZqHQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_10":{"id":"812aa795d8a6_10","__typename":"Paragraph","name":"818d","text":"VAE implementation\nThe gist given below shows the complete implementation of the VAE in PyTorch. The encoder takes image batches of size Bx3x256x256 and produces two 512 dimensional latent vectors (μ and σ). It consists of nine blocks of Conv2d\u003EBatchNorm\u003ELeakyReLU operations with a kernel size of 3x3 and an increasing number of filter channels from 16 to 64. Every other layer halves the feature maps resolution by using convolutions with stride 2. A fully-connected layer (implemented as Conv2d) with 1024 channels and a kernel size of 8x8 is followed by two further fully-connected layers with 512 channels, each predicting one of the latent vectors (μ and σ). Next, we use the reparameterization trick that adds noise to the latent distribution thus introducing a new source of randomness, enabling gradients to backpropagate through μ and σ. The decoder part takes a Bx512x1x1 and generates a full-sized (Bx3x256x256) output image. A fully-connected 1x1 Conv2d layer maps the interpretable 512 dimensional latent vector to a hidden state with 1024 channels, followed by a transposed convolution with kernel size 8 and 64 channels. Then again, blocks of Conv2d\u003EBatchNorm\u003ELeakyReLU with kernel size 3x3 are used, but are alternated with blocks of ConvTranspose2d\u003EBatchNorm\u003ELeakyReLU with kernel size 4x4 and stride 2 to increase the feature maps resolution. The network architecture is finished with a Conv2d with 3 output channels and a tanh activation. In addition to the generated output image, the network returns the predicted latent vectors μ and σ that are needed for loss calculations.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_11":{"id":"812aa795d8a6_11","__typename":"Paragraph","name":"4060","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:c39c00aaa64574c8ff8116b5f8b528cb"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_12":{"id":"812aa795d8a6_12","__typename":"Paragraph","name":"0cb6","text":"VAE training with KLD loss\nThe VAE network is trained in an unsupervised fashion by optimizing an L1 reconstruction loss on the input and generated images. An additional loss term is given by the Kullback-Leibler divergence that ensures that the learned density function follows a normal distribution. For full training code we again refer to the provided notebooks.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":347,"end":365,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fmultimodallearning\u002Fmec19_vae_tutorial_solution","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_13":{"id":"812aa795d8a6_13","__typename":"Paragraph","name":"3dc4","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:089035c0a701d9e4507175324ad4136b"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_14":{"id":"812aa795d8a6_14","__typename":"Paragraph","name":"4ca6","text":"Visual results of synthetic images sampled randomly from the learned latent space are shown in the figure below. The VAE trained with the reconstruction and KLD loss already generates anatomically reasonable results. Outlines of different organs and anatomical structures such as liver, kidneys and vertebra are clearly visible. However, despite being realistic, images synthesized with this VAE variant are blurry and miss finer details.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_15":{"id":"812aa795d8a6_15","__typename":"Paragraph","name":"a1c1","text":"Synthetically generated abdominal CT image slices. Top: VAE. Middle: VAE with additional perceptual loss. Bottom: VAE with additional perceptual loss and bilinear upsampling instead of transposed convolutions.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*9cAOB2KDKwk8oseiID7kGA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":51,"end":54,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":61,"end":67,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":106,"end":112,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_16":{"id":"812aa795d8a6_16","__typename":"Paragraph","name":"95d1","text":"VAE Training with additional perceptual loss\nTo improve the detail-preservation VAEs can be trained with an additional perceptual loss. The perceptual loss (first introduced for style transfer and superresolution) adapts the reconstruction criterion to more visually meaningful similarity. Commonly, pre-trained networks on ImageNet (e.g. VGG-16) are used for perceptual feature extraction but due to the different nature of the image domains they are incabable of supporting medical VAE training. Thus, we trained a fully-convolutional network which roughly follows the VGG architecture for CT segmentation of the pancreas. This pre-trained model is then used for feature extraction in the perceptual loss calculation. Here, mid-level features are extracted from after the ReLUs in layers 2, 5 and 9 by registering forward hooks for the corresponding layers. After a forward path of the input images through the VGG model the feature output tensors can be copied to a list. The same procedure is repeated after passing the reconstructed images through the network and an L1 loss is employed for each corresponding feature maps from all three layers. The generated images after training with the additional perceptual loss are less blurry and more details (e.g. of the kidneys or the vertebra canal) are recognizable. However, now we observe pattern-like image artifacts, which brings us to the last part of this tutorial.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":155,"end":156,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_17":{"id":"812aa795d8a6_17","__typename":"Paragraph","name":"aafd","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:fab5d0f1465dd3292a331db38643e1ea"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_18":{"id":"812aa795d8a6_18","__typename":"Paragraph","name":"a4d7","text":"VAE implementation addressing checkerboard pattern\nAs described in the excellent interactive blog post from Odena, et al.⁵ the observed checkerboard pattern stems from the use of the transposed convolutional operator in the decoder part of the VAE and can be easily reduced by replacing transposed convolutions with a bilinear upsampling layer and a subsequent convolution. We can therefore replace every transposed convolution in our VAE model with the module definition given below and retrain our network. The results show the effectiveness of this small modification. Generated images look even sharper and no more artifacts can be observed.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":81,"end":121,"type":"A","href":"https:\u002F\u002Fdistill.pub\u002F2016\u002Fdeconv-checkerboard\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_19":{"id":"812aa795d8a6_19","__typename":"Paragraph","name":"2173","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:9b2eadbda51e4b8ca7b4282de60a7998"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_20":{"id":"812aa795d8a6_20","__typename":"Paragraph","name":"db80","text":"Where to go from here?\nTo make the provided code for the exercises lightweight and fast, we only used relatively few training images and small VAE and VGG models. But we hope this tutorial sparked someones interest in synthetic medical image generation and thus, we are excited to see more detailed, sharper and realistic images from the community using more training images, better data augmentation techniques, bigger models, improved perceptual loss weighting, new network architectures, etc. For state-of-the-art methods using VAEs a number of this years MICCAI papers are excellent starting points for further reading with applications ranging from joint modality completion and segmentation⁶ over frame rate up-conversion in echocardiography⁷ to unsupervised anomaly localization⁸.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_21":{"id":"812aa795d8a6_21","__typename":"Paragraph","name":"00dd","text":"[1] Paszke, Adam, et al. “Automatic differentiation in pytorch.” (2017). https:\u002F\u002Fpytorch.org","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":73,"end":92,"type":"A","href":"https:\u002F\u002Fpytorch.org","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_22":{"id":"812aa795d8a6_22","__typename":"Paragraph","name":"84ff","text":"[2] Google Colab. https:\u002F\u002Fcolab.research.google.com\u002Fnotebooks\u002Fwelcome.ipynb","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":18,"end":75,"type":"A","href":"https:\u002F\u002Fcolab.research.google.com\u002Fnotebooks\u002Fwelcome.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_23":{"id":"812aa795d8a6_23","__typename":"Paragraph","name":"0c69","text":"[3] Kingma, Diederik P., et al. “Auto-encoding variational bayes.” arXiv preprint arXiv:1312.6114 (2013). https:\u002F\u002Farxiv.org\u002Fpdf\u002F1312.6114.pdf","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":106,"end":141,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1312.6114.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":67,"end":97,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_24":{"id":"812aa795d8a6_24","__typename":"Paragraph","name":"af4b","text":"[4] Roth, Holger R., et al. Data From Pancreas-CT. The Cancer Imaging Archive (2016). http:\u002F\u002Fdoi.org\u002F10.7937\u002FK9\u002FTCIA.2016.tNB1kqBU","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":86,"end":130,"type":"A","href":"http:\u002F\u002Fdoi.org\u002F10.7937\u002FK9\u002FTCIA.2016.tNB1kqBU","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_25":{"id":"812aa795d8a6_25","__typename":"Paragraph","name":"66b3","text":"[5] Odena, et al., “Deconvolution and Checkerboard Artifacts”, Distill (2016). https:\u002F\u002Fdistill.pub\u002F2016\u002Fdeconv-checkerboard\u002F","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":79,"end":124,"type":"A","href":"https:\u002F\u002Fdistill.pub\u002F2016\u002Fdeconv-checkerboard\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_26":{"id":"812aa795d8a6_26","__typename":"Paragraph","name":"d662","text":"[6] Dorent, Reuben, et al. “Hetero-Modal Variational Encoder-Decoder for Joint Modality Completion and Segmentation.” arXiv preprint arXiv:1907.11150 (2019). https:\u002F\u002Farxiv.org\u002Fpdf\u002F1907.11150","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":158,"end":190,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1907.11150","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":118,"end":149,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:812aa795d8a6_27":{"id":"812aa795d8a6_27","__typename":"Paragraph","name":"6ece","text":"[7] Dezaki, Fatemeh T., et al. “Frame Rate Up-Conversion in Echocardiography Using a Conditioned Variational Autoencoder and Generative Adversarial Model.” (2019).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:812aa795d8a6_28":{"id":"812aa795d8a6_28","__typename":"Paragraph","name":"91a1","text":"[8] Zimmerer, David, et al. “Context-encoding Variational Autoencoder for Unsupervised Anomaly Detection.” arXiv preprint arXiv:1812.05941 (2018). https:\u002F\u002Farxiv.org\u002Fpdf\u002F1907.02796.pdf","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":147,"end":183,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1907.02796.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":107,"end":138,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*WmZYkt6cQJ1cenl8kZTlFw.png":{"id":"1*WmZYkt6cQJ1cenl8kZTlFw.png","__typename":"ImageMetadata","originalHeight":278,"originalWidth":1076,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Rxk6rl0HHHJhDiva6bZqHQ.png":{"id":"1*Rxk6rl0HHHJhDiva6bZqHQ.png","__typename":"ImageMetadata","originalHeight":278,"originalWidth":1076,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:c39c00aaa64574c8ff8116b5f8b528cb":{"id":"c39c00aaa64574c8ff8116b5f8b528cb","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"VAE model"},"MediaResource:089035c0a701d9e4507175324ad4136b":{"id":"089035c0a701d9e4507175324ad4136b","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"kld loss"},"ImageMetadata:1*9cAOB2KDKwk8oseiID7kGA.png":{"id":"1*9cAOB2KDKwk8oseiID7kGA.png","__typename":"ImageMetadata","originalHeight":869,"originalWidth":1076,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:fab5d0f1465dd3292a331db38643e1ea":{"id":"fab5d0f1465dd3292a331db38643e1ea","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"perceptual loss"},"MediaResource:9b2eadbda51e4b8ca7b4282de60a7998":{"id":"9b2eadbda51e4b8ca7b4282de60a7998","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"conv upsample"},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning"},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning"},"Tag:variational-autoencoder":{"id":"variational-autoencoder","__typename":"Tag","displayTitle":"Variational Autoencoder"},"Tag:pytorch":{"id":"pytorch","__typename":"Tag","displayTitle":"Pytorch"},"Tag:medical-image-analysis":{"id":"medical-image-analysis","__typename":"Tag","displayTitle":"Medical Image Analysis"},"Post:933c29bb1c90":{"id":"933c29bb1c90","__typename":"Post","canonicalUrl":"","collection":{"__ref":"Collection:fccddfb484c0"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:812aa795d8a6_0"},{"__ref":"Paragraph:812aa795d8a6_1"},{"__ref":"Paragraph:812aa795d8a6_2"},{"__ref":"Paragraph:812aa795d8a6_3"},{"__ref":"Paragraph:812aa795d8a6_4"},{"__ref":"Paragraph:812aa795d8a6_5"},{"__ref":"Paragraph:812aa795d8a6_6"},{"__ref":"Paragraph:812aa795d8a6_7"},{"__ref":"Paragraph:812aa795d8a6_8"},{"__ref":"Paragraph:812aa795d8a6_9"},{"__ref":"Paragraph:812aa795d8a6_10"},{"__ref":"Paragraph:812aa795d8a6_11"},{"__ref":"Paragraph:812aa795d8a6_12"},{"__ref":"Paragraph:812aa795d8a6_13"},{"__ref":"Paragraph:812aa795d8a6_14"},{"__ref":"Paragraph:812aa795d8a6_15"},{"__ref":"Paragraph:812aa795d8a6_16"},{"__ref":"Paragraph:812aa795d8a6_17"},{"__ref":"Paragraph:812aa795d8a6_18"},{"__ref":"Paragraph:812aa795d8a6_19"},{"__ref":"Paragraph:812aa795d8a6_20"},{"__ref":"Paragraph:812aa795d8a6_21"},{"__ref":"Paragraph:812aa795d8a6_22"},{"__ref":"Paragraph:812aa795d8a6_23"},{"__ref":"Paragraph:812aa795d8a6_24"},{"__ref":"Paragraph:812aa795d8a6_25"},{"__ref":"Paragraph:812aa795d8a6_26"},{"__ref":"Paragraph:812aa795d8a6_27"},{"__ref":"Paragraph:812aa795d8a6_28"}],"sections":[{"__typename":"Section","name":"0d41","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"5332","startIndex":7,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"fa9a","startIndex":20,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"c187","startIndex":21,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"creator":{"__ref":"User:1e6972113baf"},"customStyleSheet":null,"firstPublishedAt":1574116960932,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":0,"primaryTopic":null,"title":"Tutorial: Abdominal CT Image Synthesis with Variational Autoencoders using PyTorch","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fmiccai-educational-initiative\u002Ftutorial-abdominal-ct-image-synthesis-with-variational-autoencoders-using-pytorch-933c29bb1c90","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:variational-autoencoder"},{"__ref":"Tag:pytorch"},{"__ref":"Tag:medical-image-analysis"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning","slug":"machine-learning"}],"viewerClapCount":null,"showSubscribeToProfilePromo":false,"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1574116960932,"readingTime":6.323584905660377,"previewContent":{"__typename":"PreviewContent","subtitle":"By Lasse Hansen, Maximilian Blendowski and Mattias P. Heinrich — Institute of Medical Informatics at the University of Lübeck, Germany"},"previewImage":{"__ref":"ImageMetadata:1*WmZYkt6cQJ1cenl8kZTlFw.png"},"creatorPartnerProgramEnrollmentStatus":"PERMISSION_DENIED","clapCount":58,"lockedSource":"LOCKED_POST_SOURCE_NONE","isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":1566683978102,"shareKey":null,"responsesCount":2,"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","updatedAt":1574118980510,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","postResponses":{"__typename":"PostResponses","count":2},"latestPublishedVersion":"812aa795d8a6","readingList":"READING_LIST_NONE","voterCount":4,"recommenders":[]}}</script><script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/manifest.js"></script><script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/9121.js"></script><script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/main.js"></script><script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/5573.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/instrumentation.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/reporting.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1826.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/4464.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/8342.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1148.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/5064.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/9274.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/2846.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/4328.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/7993.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/6839.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/353.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/8751.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/2054.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/8127.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/7131.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/8825.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/5279.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/9978.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/3721.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/2514.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/2602.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/6585.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/1838.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/9889.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/3981.js"></script>
<script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/Post.js"></script><script>window.main();</script><script src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/p.js" async="" id="parsely-cf"></script><iframe src="Tutorial%20Abdominal%20CT%20Image%20Synthesis%20with%20Variational%20Autoencoders%20using%20PyTorch%20by%20Lasse%20Hansen%20MICCAI%20Educational%20Initiative%20Medium_fi%C8%99iere/a16180790160.htm" tabindex="-1" title="Optimizely Internal Frame" style="display: none;" width="0" hidden="" height="0"></iframe></body></html>