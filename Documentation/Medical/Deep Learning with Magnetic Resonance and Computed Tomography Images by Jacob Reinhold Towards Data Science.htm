<!DOCTYPE html>
<html data-rh="lang" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><script async="" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/branch-latest.js"></script><script async="" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/analytics.js"></script><script>!function(n,e){var t,o,i,c=[],f={passive:!0,capture:!0},r=new Date,a="pointerup",u="pointercancel";function p(n,c){t||(t=c,o=n,i=new Date,w(e),s())}function s(){o>=0&&o<i-r&&(c.forEach((function(n){n(o,t)})),c=[])}function l(t){if(t.cancelable){var o=(t.timeStamp>1e12?new Date:performance.now())-t.timeStamp;"pointerdown"==t.type?function(t,o){function i(){p(t,o),r()}function c(){r()}function r(){e(a,i,f),e(u,c,f)}n(a,i,f),n(u,c,f)}(o,t):p(o,t)}}function w(n){["click","mousedown","keydown","touchstart","pointerdown"].forEach((function(e){n(e,l,f)}))}w(n),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){c.push(n),s()}}(addEventListener,removeEventListener)</script><script defer="defer" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/16180790160.js"></script><title>Deep Learning with Magnetic Resonance and Computed Tomography Images | by Jacob Reinhold | Towards Data Science</title><meta data-rh="true" charset="utf-8"><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2020-05-07T13:04:39.680Z"><meta data-rh="true" name="title" content="Deep Learning with Magnetic Resonance and Computed Tomography Images | by Jacob Reinhold | Towards Data Science"><meta data-rh="true" property="og:title" content="Deep Learning with Magnetic Resonance and Computed Tomography Images"><meta data-rh="true" property="twitter:title" content="Deep Learning with Magnetic Resonance and Computed Tomography Images"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/e9f32273dcb5"><meta data-rh="true" property="al:android:url" content="medium://p/e9f32273dcb5"><meta data-rh="true" property="al:ios:url" content="medium://p/e9f32273dcb5"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Getting started with applying deep learning to magnetic resonance (MR) or computed tomography (CT) images is not straightforward; finding appropriate data sets, preprocessing the data, and creating…"><meta data-rh="true" property="og:description" content="An introduction to the data, preprocessing techniques and deep network design for medical images"><meta data-rh="true" property="twitter:description" content="An introduction to the data, preprocessing techniques and deep network design for medical images"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/512/1*BaCHM97Tmm5p292b4xk1yw.jpeg"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/512/1*BaCHM97Tmm5p292b4xk1yw.jpeg"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://medium.com/@jcreinhold"><meta data-rh="true" name="twitter:creator" content="@JacobCReinhold"><meta data-rh="true" name="author" content="Jacob Reinhold"><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="18 min read"><meta data-rh="true" name="parsely-post-id" content="e9f32273dcb5"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/unbound.css"><link data-rh="true" rel="author" href="https://medium.com/@jcreinhold"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/e9f32273dcb5"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="546" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:35px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.w{min-height:115px}.x{flex-direction:column}.y{background-color:#355876}.z{display:none}.ac{border-bottom:1px solid rgba(99, 127, 153, 1)}.ae{position:relative}.af{z-index:500}.al{max-width:1192px}.am{min-width:0}.an{height:62px}.ao{flex-direction:row}.ap{flex:1 0 auto}.aq{visibility:hidden}.ar{margin-left:0px}.as{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.at{font-size:14px}.au{line-height:20px}.av{color:rgba(197, 210, 225, 1)}.aw{color:rgba(53, 88, 118, 1)}.ax{fill:rgba(53, 88, 118, 1)}.ay{font-size:inherit}.az{border:inherit}.ba{font-family:inherit}.bb{letter-spacing:inherit}.bc{font-weight:inherit}.bd{padding:0}.be{margin:0}.bf:hover{cursor:pointer}.bg:hover{color:rgba(99, 127, 153, 1)}.bh:hover{fill:rgba(99, 127, 153, 1)}.bi:disabled{cursor:default}.bj:disabled{color:rgba(26, 137, 23, 0.3)}.bk:disabled{fill:rgba(26, 137, 23, 0.3)}.bl{height:25px}.bm{fill:rgba(233, 241, 250, 1)}.bn{justify-content:space-between}.bs{align-items:flex-start}.bt{margin-bottom:0px}.bu{margin-top:-32px}.bv{flex-wrap:wrap}.by{margin-top:32px}.bz{margin-right:24px}.cb{width:112px}.cd{font-size:16px}.ce{line-height:24px}.cf:before{margin-bottom:-10px}.cg:before{content:""}.ch:before{display:table}.ci:before{border-collapse:collapse}.cj:after{margin-top:-6px}.ck:after{content:""}.cl:after{display:table}.cm:after{border-collapse:collapse}.cn{display:inline}.co{color:inherit}.cp{fill:inherit}.cq:hover{color:rgba(242, 248, 253, 1)}.cr:hover{fill:rgba(242, 248, 253, 1)}.cs:disabled{color:rgba(197, 210, 225, 1)}.ct:disabled{fill:rgba(197, 210, 225, 1)}.cu{margin-left:12px}.cv{margin-right:12px}.cw{margin-left:24px}.cy{display:inline-block}.cz{padding:7px 16px 9px}.da{background:0}.db{border-color:rgba(53, 88, 118, 1)}.dc:hover{border-color:rgba(99, 127, 153, 1)}.dd:disabled{cursor:inherit}.de:disabled{opacity:0.3}.df:disabled:hover{color:rgba(53, 88, 118, 1)}.dg:disabled:hover{fill:rgba(53, 88, 118, 1)}.dh:disabled:hover{border-color:rgba(53, 88, 118, 1)}.di{border-radius:4px}.dj{border-width:1px}.dk{border-style:solid}.dl{box-sizing:border-box}.dm{text-decoration:none}.dn{margin-top:-3px}.do{flex:0 0 auto}.dp{justify-self:flex-end}.dq{height:32px}.dr{overflow:visible}.ds{border-radius:1000px}.dt{background-color:rgba(53, 88, 118, 0.8)}.du{fill:rgba(197, 210, 225, 1)}.dv:hover{fill:rgba(251, 255, 255, 1)}.dw{fill:rgba(117, 117, 117, 1)}.dx{outline:none}.dy{padding:4px}.dz{margin-left:8px}.ea{margin-right:10px}.eb{margin-top:16px}.ec{margin-bottom:16px}.ed{display:inherit}.ee{max-width:210px}.ef{text-overflow:ellipsis}.eg{overflow:hidden}.eh{white-space:nowrap}.ei{border:none}.ej{font:inherit}.ek{opacity:0}.el{background-color:transparent}.em{color:rgba(233, 241, 250, 1)}.en::placeholder{color:rgba(197, 210, 225, 1)}.eo{padding:0px}.ep{width:0px}.eq{transition:width 140ms ease-in, padding 140ms ease-in}.er{fill:rgba(251, 255, 255, 1)}.es{border-top:1px solid rgba(230, 230, 230, 1)}.et{border-bottom:1px solid rgba(230, 230, 230, 1)}.eu{left:0}.ev{position:fixed}.ew{right:0}.ex{top:0}.fa{height:60px}.fb{color:rgba(117, 117, 117, 1)}.fc{color:rgba(102, 138, 170, 1)}.fd{fill:rgba(102, 138, 170, 1)}.fe:hover{color:rgba(90, 118, 144, 1)}.ff:hover{fill:rgba(90, 118, 144, 1)}.fg{padding-left:24px}.fh{padding-right:24px}.fi{margin-left:auto}.fj{margin-right:auto}.fk{max-width:728px}.fl{position:absolute}.fm{top:calc(100vh + 100px)}.fn{bottom:calc(100vh + 100px)}.fo{width:10px}.fp{pointer-events:none}.fq{word-break:break-word}.fr{word-wrap:break-word}.fs:after{display:block}.ft:after{clear:both}.fu{max-width:680px}.fv{line-height:1.23}.fw{letter-spacing:0}.fx{font-style:normal}.fy{font-weight:700}.gt{margin-bottom:-0.27em}.gu{color:rgba(41, 41, 41, 1)}.gv{line-height:1.394}.hl{margin-bottom:-0.42em}.hp{border-radius:50%}.hq{height:28px}.hr{width:28px}.hs{margin:0 4px}.ht{margin:0 7px}.hu{align-items:flex-end}.id{padding-right:6px}.ie:hover{color:rgba(25, 25, 25, 1)}.if:hover{fill:rgba(25, 25, 25, 1)}.ig:disabled{color:rgba(117, 117, 117, 1)}.ih:disabled{fill:rgba(117, 117, 117, 1)}.ii{margin-right:8px}.ij{fill:rgba(61, 61, 61, 1)}.ik{margin-right:-6px}.il:hover{fill:rgba(8, 8, 8, 1)}.im:focus{fill:rgba(8, 8, 8, 1)}.in{line-height:1.58}.io{letter-spacing:-0.004em}.ip{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.jd{margin-top:24px}.je{margin-bottom:-0.46em}.jf{text-decoration:underline}.jg{line-height:1.12}.jh{letter-spacing:-0.022em}.ji{font-weight:500}.kb{margin-bottom:-0.28em}.kh{font-style:italic}.kn{clear:both}.lb{margin-top:0px}.lc{width:55.65%}.ld{padding-top:5px}.le{padding-bottom:5px}.lf:last-of-type{margin-right:0}.lg{transition:opacity 100ms 400ms}.lh{height:100%}.li{will-change:transform}.lj{transform:translateZ(0)}.lk{margin:auto}.ll{background-color:rgba(242, 242, 242, 1)}.lm{padding-bottom:100%}.ln{height:0}.lo{filter:blur(20px)}.lp{transform:scale(1.1)}.lq{visibility:visible}.lr{width:44.35%}.ls{padding-bottom:125.49019607843137%}.lt{margin-top:10px}.lu{text-align:center}.lx{width:225%}.ly{left:calc(-12.5% - 8px)}.lz{transform:translateX(-50%)}.ma{list-style-type:disc}.mb{margin-left:30px}.mc{padding-left:0px}.mi{padding:20px}.mj{background:rgba(242, 242, 242, 1)}.mk{overflow-x:auto}.ml{line-height:1.18}.mm{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.mn{margin-top:-0.09em}.mo{margin-bottom:-0.09em}.mp{white-space:pre-wrap}.mq{padding:2px 4px}.mr{font-size:75%}.ms> strong{font-family:inherit}.mt{width:50%}.mv{cursor:zoom-in}.mw{z-index:auto}.mx:focus{transform:scale(1.01)}.my{padding-bottom:83.33333333333334%}.mz{width:200%}.na{left:calc(0% - 8px)}.ni{margin-bottom:-0.31em}.nj{padding-bottom:NaN%}.nk{max-width:2554px}.nl{padding-bottom:41.26859827721221%}.nm{will-change:opacity}.nn{width:188px}.no{left:50%}.np{transform:translateX(406px)}.nq{top:calc(65px + 54px + 14px)}.nt{top:159px}.nv{width:131px}.nw{padding-bottom:28px}.nx{font-size:12px}.ny{line-height:16px}.nz{letter-spacing:0.083em}.oa{text-transform:uppercase}.ob{padding-top:2px}.oc{padding-top:14px}.od{padding:4px 12px 6px}.oe{border-color:rgba(102, 138, 170, 1)}.of:hover{border-color:rgba(90, 118, 144, 1)}.og:disabled:hover{color:rgba(102, 138, 170, 1)}.oh:disabled:hover{fill:rgba(102, 138, 170, 1)}.oi:disabled:hover{border-color:rgba(102, 138, 170, 1)}.oj{padding-top:28px}.ok{margin-bottom:19px}.ol{margin-left:-3px}.or{outline:0}.os{border:0}.ot{user-select:none}.ou{cursor:pointer}.ov> svg{pointer-events:none}.ow:active{border-style:none}.ox{-webkit-user-select:none}.oy:focus{fill:rgba(117, 117, 117, 1)}.oz:hover{fill:rgba(117, 117, 117, 1)}.ph button{text-align:left}.pi{opacity:0.4}.pj{cursor:not-allowed}.pk{padding-right:9px}.pt{margin-top:40px}.pu{border-top:3px solid rgba(102, 138, 170, 1)}.pv{padding:32px 32px 26px 32px}.pw{margin-top:8px}.px{margin-bottom:25px}.py{background-color:rgba(250, 250, 250, 1)}.qa{padding-bottom:0px}.qb{padding-top:4px}.qc{font-size:13px}.qd{padding-bottom:10px}.qe{padding-top:8px}.qp{margin:10px 20px 10px 0}.qr{color:rgba(255, 255, 255, 1)}.qs{padding:7px 20px 9px}.qt{fill:rgba(255, 255, 255, 1)}.qu{background:rgba(102, 138, 170, 1)}.qv:hover{background:rgba(90, 118, 144, 1)}.qw:disabled:hover{background:rgba(102, 138, 170, 1)}.qx{margin:10px 0 10px 0}.qy{max-width:380px}.qz{padding-bottom:25px}.ra{margin-top:25px}.rb{max-width:155px}.rf{top:1px}.rt{margin-left:-1px}.ru{margin-left:-4px}.sc{padding-right:8px}.sd{padding-bottom:40px}.se{list-style-type:none}.sf{margin-bottom:8px}.sg{line-height:22px}.sh{border-radius:3px}.si{padding:5px 10px}.sl{padding-bottom:4px}.sm{padding-top:32px}.sx{display:-webkit-box}.sy{-webkit-line-clamp:1}.sz{-webkit-box-orient:vertical}.tb{padding-right:168px}.tc{padding-top:25px}.tf{max-width:100%}.tg{margin-bottom:96px}.th{background:rgba(255, 255, 255, 1)}.ti{padding:32px 0}.tj{background-color:rgba(0, 0, 0, 0.9)}.tl:hover{color:rgba(255, 255, 255, 0.99)}.tm:hover{fill:rgba(255, 255, 255, 0.99)}.tn:disabled{color:rgba(255, 255, 255, 0.7)}.to:disabled{fill:rgba(255, 255, 255, 0.7)}.tp{height:22px}.tq{color:rgba(255, 255, 255, 0.7)}.tr{width:200px}.tu{color:rgba(255, 255, 255, 0.98)}.tv:hover{text-decoration:underline}.ua{margin-right:16px}.ub{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ak{margin:0 64px}.gp{font-size:46px}.gq{margin-top:0.6em}.gr{line-height:56px}.gs{letter-spacing:-0.011em}.hi{font-size:22px}.hj{margin-top:0.92em}.hk{line-height:28px}.ib{margin-left:30px}.ja{font-size:21px}.jb{line-height:32px}.jc{letter-spacing:-0.003em}.jx{font-size:30px}.jy{margin-top:1.95em}.jz{line-height:36px}.ka{letter-spacing:0}.kg{margin-top:0.86em}.km{margin-top:2em}.kv{max-width:1192px}.la{margin-top:56px}.mh{margin-top:1.05em}.nh{margin-top:1.72em}.oq{margin-right:5px}.pg{margin-top:5px}.ps{padding-left:6px}.qn{font-size:16px}.qo{line-height:24px}.rh{display:inline-block}.rm{margin-left:7px}.rn{margin-top:8px}.rs{width:25px}.sa{padding-left:7px}.sb{top:3px}.sv{font-size:20px}.sw{max-height:24px}.te{margin:0}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.ia{margin-left:30px}.lv{margin-left:auto}.lw{text-align:center}.op{margin-right:5px}.pf{margin-top:5px}.pr{padding-left:6px}.rg{display:inline-block}.rk{margin-left:7px}.rl{margin-top:8px}.rr{width:25px}.ry{padding-left:7px}.rz{top:3px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.hz{margin-left:30px}.oo{margin-right:5px}.pe{margin-top:5px}.pp{padding-left:6px}.pq{top:3px}.re{display:inline-block}.ri{margin-left:7px}.rj{margin-top:8px}.rq{width:15px}.rx{padding-left:3px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.ab{display:block}.bo{min-height:115px}.bp{align-items:flex-start}.bq{flex-direction:column}.br{justify-content:center}.bw{margin-bottom:31px}.bx{margin-top:0}.ca{margin-top:46px}.cc{margin-top:26px}.cx{margin-left:24px}.ez{margin-top:0px}.hn{margin-top:32px}.ho{flex-direction:column-reverse}.hx{margin-bottom:30px}.hy{margin-left:0px}.on{margin-left:8px}.pc{margin-top:2px}.pd{margin-right:8px}.pn{padding-left:6px}.po{top:3px}.pz{padding:24px 24px 28px 24px}.rd{display:inline-block}.rp{width:15px}.rw{padding-left:3px}.sj{padding-top:0}.sk{border-top:none}.tk{padding:32px 0}.ts{width:140px}.tt{display:flex}.tw{margin-bottom:16px}.tx{margin-top:30px}.ty{width:100%}.tz{flex-direction:row}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ag{margin:0 24px}.fz{font-size:32px}.ga{margin-top:0.64em}.gb{line-height:40px}.gc{letter-spacing:-0.016em}.gw{font-size:18px}.gx{margin-top:0.79em}.gy{line-height:24px}.hm{margin-top:32px}.hv{margin-bottom:30px}.hw{margin-left:0px}.iq{line-height:28px}.ir{letter-spacing:-0.003em}.jj{font-size:22px}.jk{margin-top:1.2em}.jl{letter-spacing:0}.kc{margin-top:0.67em}.ki{margin-top:1.56em}.ko{margin:0}.kp{max-width:100%}.kw{margin-top:40px}.md{margin-top:1.34em}.nb{font-size:20px}.nc{margin-top:1.23em}.om{margin-left:8px}.pa{margin-top:2px}.pb{margin-right:8px}.pl{padding-left:6px}.pm{top:3px}.qf{font-size:14px}.qg{line-height:20px}.qq{margin:10px 0 0 0}.rc{display:inline-block}.ro{width:15px}.rv{padding-left:3px}.sn{font-size:16px}.so{max-height:20px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.aj{margin:0 64px}.gl{font-size:46px}.gm{margin-top:0.6em}.gn{line-height:56px}.go{letter-spacing:-0.011em}.hf{font-size:22px}.hg{margin-top:0.92em}.hh{line-height:28px}.ix{font-size:21px}.iy{line-height:32px}.iz{letter-spacing:-0.003em}.jt{font-size:30px}.ju{margin-top:1.95em}.jv{line-height:36px}.jw{letter-spacing:0}.kf{margin-top:0.86em}.kl{margin-top:2em}.ku{max-width:1192px}.kz{margin-top:56px}.mg{margin-top:1.05em}.ng{margin-top:1.72em}.ql{font-size:16px}.qm{line-height:24px}.st{font-size:20px}.su{max-height:24px}.td{margin:0}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.ai{margin:0 48px}.gh{font-size:46px}.gi{margin-top:0.6em}.gj{line-height:56px}.gk{letter-spacing:-0.011em}.hc{font-size:22px}.hd{margin-top:0.92em}.he{line-height:28px}.iu{font-size:21px}.iv{line-height:32px}.iw{letter-spacing:-0.003em}.jp{font-size:30px}.jq{margin-top:1.95em}.jr{line-height:36px}.js{letter-spacing:0}.ke{margin-top:0.86em}.kk{margin-top:2em}.ks{margin:0}.kt{max-width:100%}.ky{margin-top:56px}.mf{margin-top:1.05em}.nf{margin-top:1.72em}.qj{font-size:16px}.qk{line-height:24px}.sr{font-size:20px}.ss{max-height:24px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ah{margin:0 24px}.gd{font-size:32px}.ge{margin-top:0.64em}.gf{line-height:40px}.gg{letter-spacing:-0.016em}.gz{font-size:18px}.ha{margin-top:0.79em}.hb{line-height:24px}.is{line-height:28px}.it{letter-spacing:-0.003em}.jm{font-size:22px}.jn{margin-top:1.2em}.jo{letter-spacing:0}.kd{margin-top:0.67em}.kj{margin-top:1.56em}.kq{margin:0}.kr{max-width:100%}.kx{margin-top:40px}.me{margin-top:1.34em}.nd{font-size:20px}.ne{margin-top:1.23em}.qh{font-size:14px}.qi{line-height:20px}.sp{font-size:16px}.sq{max-height:20px}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="print">.ic{display:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ey{animation:k2 .2s ease-in-out both}.mu{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.nr{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 1230px)">.ns{display:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="all and (max-width: 1198px)">.nu{display:none}</style><style type="text/css" data-fela-rehydration="546" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.ta{max-height:none}</style><link rel="icon" href="https://miro.medium.com/fit/c/154/154/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*BaCHM97Tmm5p292b4xk1yw.jpeg"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5","dateCreated":"2019-01-06T16:38:19.587Z","datePublished":"2019-01-06T16:38:19.587Z","dateModified":"2020-05-07T13:04:40.013Z","headline":"Deep Learning with Magnetic Resonance and Computed Tomography Images","name":"Deep Learning with Magnetic Resonance and Computed Tomography Images","description":"Getting started with applying deep learning to magnetic resonance (MR) or computed tomography (CT) images is not straightforward; finding appropriate data sets, preprocessing the data, and creating…","identifier":"e9f32273dcb5","keywords":["Lite:true","Tag:Machine Learning","Tag:Deep Learning","Tag:Medical Imaging","Tag:Python","Tag:Pytorch","Topic:Machine Learning","Topic:Data Science","Topic:Programming","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_NONE","LayerCake:3"],"author":{"@type":"Person","name":"Jacob Reinhold","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@jcreinhold"},"creator":["Jacob Reinhold"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F198\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5"}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><div class="s"><div class="t s u"><div class="w s"><div class="n x y"><div class="z ab"><div class="ac s ae af"><div class="n p"><div class="ag ah ai aj ak al am v"><div class="an n o"><div class="n o ao ap"><div class="lq" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="ar z ab"><span class="as b at au av"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe9f32273dcb5&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----e9f32273dcb5--------------------------------" class="aw ax ay az ba bb bc bd be bf bg bh bi bj bk" rel="noopener nofollow">Open in app</a></span></div></div></div><a href="https://medium.com/?source=post_page-----e9f32273dcb5--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="bl bm"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="ag ah ai aj ak al am v"><div class="w n o ao bn bo bp bq br"><div class="v n bs bn"><div class="n v"><div class="bt bu v n o ao bv bw bx bp bq"><div class="by bz s ca"><a href="https://towardsdatascience.com/?source=post_page-----e9f32273dcb5--------------------------------" aria-label="Publication Homepage" rel="noopener"><div class="q cb s"><img alt="Towards Data Science" class="" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"></div></a></div><div class="by s cc"><h4 class="as b cd ce cf cg ch ci cj ck cl cm av"><div class="n o"><div class="cn"><a class="co cp ay az ba bb bc bd be bf cq cr bi cs ct" rel="noopener" href="https://towardsdatascience.com/followers?source=post_page-----e9f32273dcb5--------------------------------">505K Followers</a></div><div class="cu cv s ab">·</div><div class="s ab"><a href="https://towardsdatascience.com/about?source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf cq cr bi cs ct" rel="noopener">About</a></div><div class="cw s cx"><div class="cy" aria-hidden="false"><button class="as b at au aw cz da ax db bg bh dc bf dd de df dg dh di dj dk dl cy dm"><div class="n ao">Follow</div></button></div></div></div></h4></div></div></div><div class="dn n o do dp af g"><div class="cv dq dr n o"><div class="ds dt"><div class="n" aria-hidden="false"><button aria-label="Publication Menu" class="co dw ay az ba bb bc bd be dx bf"><div class="dy s"><svg class="du dv" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></div></button></div></div><div class="dz ea s"><div class="ds dt"><div class="cy" aria-hidden="false"><div class="n"><button class="co cp ay az ba bb bc bd be bf cq cr bi cs ct"><span class="dy s"><svg width="25" height="25" viewBox="0 0 25 25" class="du"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input class="ei dx ej cd au ek el em en ae eo ep eq" placeholder="Search Towards Data Science"></div></div></div></div></div><a href="https://medium.com/?source=post_page-----e9f32273dcb5--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="bl er"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div></div><div class="z yt ab"><div class="es et c eu uf ev ew ex lq af ys"><div class="n p"><div class="ag ah ai aj ak al am v"><div class="fa v n o ex af"><div class="n o ap"><div class="lq" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="ar z ab"><span class="as b at au fb"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe9f32273dcb5&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----e9f32273dcb5--------------------------------" class="fc fd ay az ba bb bc bd be bf fe ff bi bj bk" rel="noopener nofollow">Open in app</a></span></div></div></div><a href="https://medium.com/?source=post_page-----e9f32273dcb5--------------------------------" aria-label="Homepage" rel="noopener"><svg viewBox="0 0 1043.63 592.71" class="bl r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s ic"><div class="un uo v lh ev up uq ou ek ur fp" aria-hidden="true"></div><div class="us ev ut uu uh un lh dl uv uw ux uf uy uz va ty vb vc vd ve" aria-hidden="true"><div class="vh vi n o ao bn"><h2 class="as ji vj ce fw gu">Responses (10)</h2><div class="n ao"><div class="s ae vk"><div class="be s ae ew"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" data-testid="close-button" aria-label="close"><svg width="25" height="25" viewBox="0 0 25 25" class="dw"><path d="M18.13 6.11l-5.61 5.61-5.6-5.61-.81.8 5.61 5.61-5.61 5.61.8.8 5.61-5.6 5.61 5.6.8-.8-5.6-5.6 5.6-5.62"></path></svg></button></div></div></div></div><div class="vm ye s"><div class="uu xr di n x xs xt xu"><div class="n o bn ae xv xw ek xx xy"><div class="n o"><div class="ae vp dq"><div class="wx n ao o p fl wy wz xa no xb fp"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Vesa Alexandru" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/0UPBPhpvitZdcnAku.png" width="32" height="32"></div><div class="cu n bs x p"><h4 class="as b at au gu">Vesa Alexandru</h4></div></div></div><div class="n x"><div class="xz ya"><div class="yf s"><div data-gramm="false" role="textbox" data-slate-editor="true" data-slate-node="value" style="outline: currentcolor none medium; white-space: pre-wrap; overflow-wrap: break-word;" contenteditable="true"><div data-slate-node="element"><p><span data-slate-node="text"><span data-slate-leaf="true"><span style="pointer-events: none; display: inline-block; width: 0px; max-width: 100%; white-space: nowrap; opacity: 0.333; user-select: none; font-style: normal; font-weight: normal; text-decoration: none;" contenteditable="false">What are your thoughts?</span><span data-slate-zero-width="n" data-slate-length="0">﻿<br></span></span></span></p></div></div></div></div><div class="n yb xv yc ek xx"><div class="yd"><button class="as b at au gu od r da yg yh il yi bf dd de yj yk yl di dj dk dl cy dm">Cancel</button></div><button class="as b at au qr od qt ym yn yo yp bf dd de yq yr di dj dk dl cy dm" disabled="disabled">Respond</button></div></div></div></div><div class="s"><div class="vm"><div class="v lh"><div class="wb tc et s"><div class="n ao bn"><div class="n o ao"><img alt="J. Andrade" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1pe8YgHE7bSZLLQUCUO_6jQ.jpeg" width="32" height="32"><div class="wd s"><div class="n ao"><a href="https://medium.com/@j.andrade?source=responses-----e9f32273dcb5----0----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">J. Andrade</h4></a></div><a href="https://medium.com/@j.andrade/fantastic-piece-jacob-7702077d310f?source=responses-----e9f32273dcb5----0----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">almost 2 years ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><a href="https://medium.com/@j.andrade/fantastic-piece-jacob-7702077d310f?source=responses-----e9f32273dcb5----0----------------------------" rel="noopener"><div class="vx vy sh vz eb c"><div class="dw wa"><h4 class="as b qc au fb"><div class="we n o do"><div class="n o"><svg width="19" height="19" viewBox="0 0 19 19" class="wf s"><g fill-rule="evenodd"><path d="M6.93 15.34l-2.06-2.06c-2.16-2.16-3.53-3.15-2.9-3.79.33-.32.8-.39 1.2 0l2.13 2.2c.08.08.16.07.38.13.21.05.36-.05.54-.23.17-.16.07-.6-.09-.77L3.5 8.2c-.37-.36-.5-1-.1-1.39.37-.37.8-.19 1.14.15.39.38 2.7 2.76 2.7 2.76a.42.42 0 0 0 .3.13.54.54 0 0 0 .33-.16c.17-.16.25-.46.09-.63 0 0-1.34-1.4-1.82-1.88-.71-.72-.77-1.22-.46-1.54.45-.44 1.05-.3 1.86.62l3.58 3.89-.75-1.95s-.47-1.25 0-1.5.84.4 1.17.9l1.86 3.34c1.01 1.65.69 3.8-.73 5.19-1.87 1.87-4.07.87-5.73-.78zM10.26.04H8.73l.77 3.3zM13.93 1.2L12.5.7l-.4 3.36zM6.5.57l-1.44.52L6.9 3.93z"></path><path d="M14.3 7.03c-.34-.5-.9-.52-1.25-.24-.25.19-.21.6-.2.9l1.51 2.64c1.17 1.9 1.33 3.66 0 5.45.4-.19.52-.23.9-.61 1.52-1.52 1.86-3.32.85-4.96l-1.8-3.18z"></path></g></svg><div>3</div></div><div class="n o"><svg width="19" height="19" viewBox="0 0 19 19" class="dz wf s"><path d="M16.19 9.42c0 1.67-.72 2.84-2.03 4l-.1.09v.13a4.3 4.3 0 0 0 .15.85c.17.6.46 1.2.86 1.8a4.94 4.94 0 0 1-2.46-.84c-.26-.17-.33-.2-.54-.4l-.13-.12-.16.05c-.68.2-2.75.46-4.88-.16a5.9 5.9 0 0 1-4.1-5.4c0-3.37 3-6.12 6.7-6.12s6.69 2.75 6.69 6.12z" fill-rule="evenodd"></path></svg><div>1</div></div></div></h4></div><h4 class="as b at au eg wg ef sx wh sz ta gu"><div class="s">Fantastic piece Jacob.</div><div class="gu">I’m
 also working with CNN applications on structural MRI images — for 
Alzheimer classification — and related to many of the issues described 
here.</div></h4></div></a></div></div></div><div class="vm"><div class="v lh"><div class="wb tc et s"><div class="n ao bn"><div class="n o ao"><img alt="Fernando Pérez-García" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2ukwoKX1Iy0PgaCpk9JyQtQ.jpeg" width="32" height="32"><div class="wd s"><div class="n ao"><a href="https://medium.com/@fepegar?source=responses-----e9f32273dcb5----1----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">Fernando Pérez-García</h4></a></div><a href="https://medium.com/@fepegar/hi-jacob-ceb2dcdf8fce?source=responses-----e9f32273dcb5----1----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">11 months ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><a href="https://medium.com/@fepegar/hi-jacob-ceb2dcdf8fce?source=responses-----e9f32273dcb5----1----------------------------" rel="noopener"><div class="vx vy sh vz eb c"><div class="dw wa"><h4 class="as b qc au fb"><div class="we n o do"><div class="n o"><svg width="19" height="19" viewBox="0 0 19 19" class="wf s"><g fill-rule="evenodd"><path d="M6.93 15.34l-2.06-2.06c-2.16-2.16-3.53-3.15-2.9-3.79.33-.32.8-.39 1.2 0l2.13 2.2c.08.08.16.07.38.13.21.05.36-.05.54-.23.17-.16.07-.6-.09-.77L3.5 8.2c-.37-.36-.5-1-.1-1.39.37-.37.8-.19 1.14.15.39.38 2.7 2.76 2.7 2.76a.42.42 0 0 0 .3.13.54.54 0 0 0 .33-.16c.17-.16.25-.46.09-.63 0 0-1.34-1.4-1.82-1.88-.71-.72-.77-1.22-.46-1.54.45-.44 1.05-.3 1.86.62l3.58 3.89-.75-1.95s-.47-1.25 0-1.5.84.4 1.17.9l1.86 3.34c1.01 1.65.69 3.8-.73 5.19-1.87 1.87-4.07.87-5.73-.78zM10.26.04H8.73l.77 3.3zM13.93 1.2L12.5.7l-.4 3.36zM6.5.57l-1.44.52L6.9 3.93z"></path><path d="M14.3 7.03c-.34-.5-.9-.52-1.25-.24-.25.19-.21.6-.2.9l1.51 2.64c1.17 1.9 1.33 3.66 0 5.45.4-.19.52-.23.9-.61 1.52-1.52 1.86-3.32.85-4.96l-1.8-3.18z"></path></g></svg><div>5</div></div><div class="n o"><svg width="19" height="19" viewBox="0 0 19 19" class="dz wf s"><path d="M16.19 9.42c0 1.67-.72 2.84-2.03 4l-.1.09v.13a4.3 4.3 0 0 0 .15.85c.17.6.46 1.2.86 1.8a4.94 4.94 0 0 1-2.46-.84c-.26-.17-.33-.2-.54-.4l-.13-.12-.16.05c-.68.2-2.75.46-4.88-.16a5.9 5.9 0 0 1-4.1-5.4c0-3.37 3-6.12 6.7-6.12s6.69 2.75 6.69 6.12z" fill-rule="evenodd"></path></svg><div>1</div></div></div></h4></div><h4 class="as b at au eg wg ef sx wh sz ta gu"><div class="s">Hi Jacob,</div><div class="gu">You might be interested in this new library for medical images and PyTorch:</div></h4></div></a></div></div></div><div class="vm"><div class="v lh"><div class="wi tc et s"><div class="n ao bn"><div class="n o ao"><img alt="EricCBohn" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1dmbNkD5D-u45r44go_cf0g.png" width="32" height="32"><div class="wd s"><div class="n ao"><a href="https://medium.com/@ericcbohn?source=responses-----e9f32273dcb5----2----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">EricCBohn</h4></a></div><a href="https://medium.com/@ericcbohn/what-would-you-consider-to-be-memory-limited-with-respect-to-the-gpu-in-doing-this-kind-of-work-f6a3c70573c4?source=responses-----e9f32273dcb5----2----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">12 months ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="wj s"><pre class="mp"><div class="wk s"><h4 class="as b at au gu"><div class="ce">What would you consider to be “memory limited” with respect to the GPU in doing this kind of work? What would the ideal GPU memory size be, as well as configuration for desktop deep learning machine? Thanks!</div></h4></div></pre></div><div class="wl n o ao"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ub r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="wm s"></div><a href="https://medium.com/@ericcbohn/what-would-you-consider-to-be-memory-limited-with-respect-to-the-gpu-in-doing-this-kind-of-work-f6a3c70573c4?responsesOpen=true&amp;source=responses-----e9f32273dcb5----2----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="ou os bd"><div class="pk n o ao"><div class="s ae wn"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s ae rv wo rw wp rx wq wr ws wt wu"><h4 class="as b at au gu">1 </h4></div></div></button></a></div></div></div></div><div class="vm"><div class="v lh"><div class="wi tc et s"><div class="n ao bn"><div class="n o ao"><img alt="xu zhang" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1dmbNkD5D-u45r44go_cf0g.png" width="32" height="32"><div class="wd s"><div class="n ao"><a href="https://medium.com/@xuzhang5788?source=responses-----e9f32273dcb5----3----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">xu zhang</h4></a></div><a href="https://medium.com/@xuzhang5788/fantastic-work-f66b1c9cd341?source=responses-----e9f32273dcb5----3----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">over 1 year ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="wj s"><pre class="mp"><div class="wk s"><h4 class="as b at au gu"><div class="ce">Fantastic work!</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">I think you train your model from scratch, right? I built a 3D CNN model for the non-image problem using Keras. I don’t know why the learning rates which I found using lr_finder are always too high. I have to reduce them 10 times at…...</div></h4></div></pre></div><button class="ei wv ou"><span class="as b qc au ww">Read More</span></button><div class="wl n o ao"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ub r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="wm s"></div><a href="https://medium.com/@xuzhang5788/fantastic-work-f66b1c9cd341?responsesOpen=true&amp;source=responses-----e9f32273dcb5----3----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="ou os bd"><div class="pk n o ao"><div class="s ae wn"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s ae rv wo rw wp rx wq wr ws wt wu"><h4 class="as b at au gu">1 </h4></div></div></button></a></div></div></div></div><div class="vm"><div class="v lh"><div class="wi tc et s"><div class="n ao bn"><div class="n o ao"><div class="ae vp dq"><div class="wx n ao o p fl wy wz xa no xb fp"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Danny Wilde" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1yEKZGwB_t7oYVzjq0kXl0Q.jpeg" width="32" height="32"></div><div class="wd s"><div class="n ao"><a href="https://medium.com/@hextra_19712?source=responses-----e9f32273dcb5----4----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">Danny Wilde</h4></a></div><a href="https://medium.com/@hextra_19712/why-are-the-list-items-separated-by-pluses-and-not-commas-f4305d0c74f9?source=responses-----e9f32273dcb5----4----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">10 months ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="wj s"><pre class="mp"><div class="wk s"><h4 class="as b at au gu"><div class="ce">Why are the list items separated by pluses and not commas?</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">layers = ([res3d_block(1,15,7,norm=norm,dense=True)] + [res3d_block(16,16,norm=norm) for _ in range(4)] + [conv3d(16,1,ks=1,pad=0,norm=None)])</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">How can nn.Modules be added here?</div></h4></div></pre></div><div class="wl n o ao"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ub r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="wm s"></div><a href="https://medium.com/@hextra_19712/why-are-the-list-items-separated-by-pluses-and-not-commas-f4305d0c74f9?responsesOpen=true&amp;source=responses-----e9f32273dcb5----4----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="ou os bd"><div class="pk n o ao"><div class="s ae wn"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s ae rv wo rw wp rx wq wr ws wt wu"><h4 class="as b at au gu">1 </h4></div></div></button></a></div></div></div></div><div class="vm"><div class="v lh"><div class="wi tc et s"><div class="n ao bn"><div class="n o ao"><img alt="Guillaume Fradet" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2_03l5AjD2I8dUq7eBRu8_Q.jpeg" width="32" height="32"><div class="wd s"><div class="n ao"><a href="https://medium.com/@guillaumefradet?source=responses-----e9f32273dcb5----5----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">Guillaume Fradet</h4></a></div><a href="https://medium.com/@guillaumefradet/great-article-thank-you-c72b7c6514cd?source=responses-----e9f32273dcb5----5----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">over 1 year ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div role="button" tabindex="0" class="vx vy sh xc xd xe ou c"><p id="embedded-quote-4b621176bd78-3184" class="xf xg fx ip b gw xh gz xi xj xk xl xm xn xo je fq xp gu" data-selectable-paragraph=""><mark class="xq uj ou">If
 you are not working on brain images, then you may want to look at 
either the Nyúl &amp; Udupa method or simple z-score normalization.</mark></p></div><div class="wj s"><pre class="mp"><div class="wk s"><h4 class="as b at au gu"><div class="ce">Great article, thank you!</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">Any reference(s) about this ?</div></h4></div></pre></div><div class="wl n o ao"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ub r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="wm s"></div><a href="https://medium.com/@guillaumefradet/great-article-thank-you-c72b7c6514cd?responsesOpen=true&amp;source=responses-----e9f32273dcb5----5----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="ou os bd"><div class="pk n o ao"><div class="s ae wn"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s ae rv wo rw wp rx wq wr ws wt wu"><h4 class="as b at au gu">1 </h4></div></div></button></a></div></div></div></div><div class="vm"><div class="v lh"><div class="wi tc s"><div class="n ao bn"><div class="n o ao"><div class="ae vp dq"><div class="wx n ao o p fl wy wz xa no xb fp"><svg width="39" height="39" viewBox="0 0 39 39"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.5 1C11.83 1 5.17 5.75 1.9 12.71L1 12.3C4.4 5.02 11.4 0 19.5 0S34.6 5.02 38 12.29l-.9.42C33.82 5.75 27.16 1 19.5 1zM1.9 26.29C5.18 33.25 11.84 38 19.5 38c7.67 0 14.33-4.75 17.6-11.71l.9.42C34.6 33.98 27.6 39 19.5 39S4.4 33.98 1 26.71l.9-.42z"></path></svg></div><img alt="Danny Wilde" class="s hp dq vp" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1yEKZGwB_t7oYVzjq0kXl0Q.jpeg" width="32" height="32"></div><div class="wd s"><div class="n ao"><a href="https://medium.com/@hextra_19712?source=responses-----e9f32273dcb5----6----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h4 class="as b at au gu">Danny Wilde</h4></a></div><a href="https://medium.com/@hextra_19712/great-article-98f9138884a9?source=responses-----e9f32273dcb5----6----------------------------" class="co cp ay az ba bb bc bd be bf tv bi ig ih" rel="noopener nofollow"><h4 class="as b at au fb">11 months ago</h4></a></div></div><button class="ou os ij if wc"><svg class="overflow-dots-filled-25px_svg__svgIcon-use" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div><div class="wj s"><pre class="mp"><div class="wk s"><h4 class="as b at au gu"><div class="ce">Great article. Its one of the only introductory resources on this topic articulated in an understandable way. And on top of that, you are using FastAI.</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">I was wondering if you could answer a question for me…</div></h4></div><div class="wk s"><h4 class="as b at au gu"><div class="ce">I’ve downloaded the code and trying to run…...</div></h4></div></pre></div><button class="ei wv ou"><span class="as b qc au ww">Read More</span></button><div class="wl n o ao"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ub r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div><div class="wm s"></div><a href="https://medium.com/@hextra_19712/great-article-98f9138884a9?responsesOpen=true&amp;source=responses-----e9f32273dcb5----6----------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="ou os bd"><div class="pk n o ao"><div class="s ae wn"><svg width="25" height="25" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></div><div class="s ae rv wo rw wp rx wq wr ws wt wu"><h4 class="as b at au gu">1 </h4></div></div></button></a></div></div></div></div></div></div></div><article><section class="fg fh fi fj v fk dl s"></section><span class="s"></span><div><div class="fl eu uc fn fo fp"></div><div class="fi fj fk ae"><div class="s h g f e"><aside class="uh fl ex" style="width: 411.083px;"><div class="uk tf fl ul eh v"><h4 class="as b qc au fb"><span class="cy tf eh eg ef">Top highlight</span></h4></div></aside></div></div><section class="fq fr fs ck ft"><div class="n p"><div class="ag ah ai aj ak fu am v"><div><h1 id="d086" class="fv fw fx as fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm gn go gp gq gr gs gt gu">Deep Learning with Magnetic Resonance and Computed Tomography Images</h1></div><h2 id="74d1" class="gv fw fx as b gw gx gy gz ha hb hc hd he hf hg hh hi hj hk hl fb">An introduction to the data, preprocessing techniques and deep network design for medical images</h2><div class="by"><div class="n bn hm hn ho"><div class="o n"><div><a href="https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------" rel="noopener"><img alt="Jacob Reinhold" class="s hp hq hr" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1fEG3lcZeY_zHxK2piY856A.jpeg" width="28" height="28"></a></div><div class="cu v n bv"><div class="n"><div style="flex:1"><span class="as b at au gu"><a href="https://medium.com/@jcreinhold?source=post_page-----e9f32273dcb5--------------------------------" class="" rel="noopener"><h4 class="as b at au fc">Jacob Reinhold</h4></a></span></div></div><span class="as b at au fb"><a class="" rel="noopener" href="https://towardsdatascience.com/deep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5?source=post_page-----e9f32273dcb5--------------------------------"><h4 class="as b at au fb"><span class="hs"></span>Jan 6, 2019<span class="ht">·</span>18 min read</h4></a></span></div></div><div class="n hu hv hw hx hy hz ia ib ic"><div class="n o"><div class="id s"><div class="cy" aria-hidden="false"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="ii s"><div><div class="ij"><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="1" aria-labelledby="1"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="ik s ap"><div class="cy" aria-hidden="false"><div class="cy" aria-hidden="false"><div class="s do"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="More options"><svg class="r il im" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div><p id="ca62" class="in io fx ip b gw iq ir gz is it iu iv iw ix iy iz ja jb jc jd je fq gu" data-selectable-paragraph="">Getting
 started with applying deep learning to magnetic resonance (MR) or 
computed tomography (CT) images is not straightforward; finding 
appropriate data sets, preprocessing the data, and creating the data 
loader structures necessary to do the work is a pain to figure out. In 
this post I hope to alleviate some of that pain for newcomers. To do so,
 I’ll link to several freely-available datasets, review some 
common/necessary preprocessing techniques specific to MR and CT, and 
show how to use the <a href="https://github.com/fastai/fastai" class="co jf" rel="noopener nofollow">fastai</a> library to load (structural) MR images and train a deep neural network for a synthesis task.</p><h1 id="8171" class="jg jh fx as ji jj jk iq jl jm jn is jo jp jq jr js jt ju jv jw jx jy jz ka kb gu" data-selectable-paragraph="">Overview of MR and CT images</h1><p id="96e5" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph="">Bef<span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm">o</span></span></span></span></span></span></span></span></span>re
 we get into the meat and bones of this post, it will be useful to do a 
quick overview of the medical images that we’ll be talking about and 
some idiosyncrasies of the types of images in discussion. I’ll only be 
talking about <em class="kh">structural </em>MR images and (to a lesser 
degree) computed tomography (CT) images. Both of these types of imaging 
modalities are used to view the <em class="kh">structure</em> of the 
tissue; this is opposed to functional MR images (fMRI) or positron 
emission tomography (PET) scans which image blood flow activity and 
metabolic activity, respectively.</p><p id="5ea6" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">For people not acquainted with medical images at all, note that <em class="kh">medical image statistics are different from natural image statistics</em>.
 For example, a mammography image looks nothing like any picture that a 
human would take with their smart phone. This is obvious of course; 
however, I think it is important to have this in mind when designing 
networks and working with the data to make some sort of machine learning
 (ML) algorithm. That is not to say that using common networks or 
transfer learning from domains outside medical imaging won’t work; it is
 only to say that knowing the characteristics of common issues regarding
 medical imaging will help you debug your algorithm. I’ll discuss 
specific examples of these characteristics in the preprocessing section 
below and show ways to reduce the impact of some of these unique 
problems.</p><p id="f660" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">I’m
 not going to go into much detail about the intricacies of structural MR
 imaging. A good place to start for more in-depth details of MR is <a href="https://www.mriquestions.com/index.html" class="co jf" rel="noopener nofollow">this website,</a>
 which goes into depth regarding any topic that an ML practitioner 
working with MR would care about. I’ll note that there are many 
different types of MR images that an MR scanner can produce. For 
instance, there are T1-weighted, T2-weighted, PD-weighted, 
FLuid-Attenuated Inversion Recovery (FLAIR), among others. To make 
things more complicated, there are sub-types of those types of images 
(e.g., T1-weighted images come in the flavors: MPRAGE, SPGR, etc.). 
Depending on your task, this information may be extremely useful because
 of the unique characteristics of each of these types and sub-types of 
images. The reason for all these different types of images is because MR
 scanners are flexible machines that can be programmed to collect 
different information according to different <a href="http://mriquestions.com/hellippulse-sequences.html" class="co jf" rel="noopener nofollow">pulse sequences</a>.
 The upshot is that all of these images are not just redundant 
information; they contain useful and unique information regarding 
clinical markers that radiologists (or us as image processors) care 
about. Again, I’ll discuss more details regarding unique aspects of MR 
in the preprocessing section.</p><p id="7b5d" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">While
 there is contrast and non-contrast CT, there are not as many varieties 
of images that can be created with a CT scanner. Vaguely, the CT scanner
 shoots high-energy photons through you whose energy is calculated via a
 detector on the other side of your body which the photons hit. When 
images like this are taken from a variety of angles, we can use our 
knowledge of the geometry at which the images were acquired to 
reconstruct the image into a 3D volume. The physical representation of 
the energy lets us map the found intensity values to a standard scale 
which also simplifies our life and is discussed more in the 
preprocessing section. I should note that while MR is good at 
soft-tissue contrast (e.g., the ability to discern between gray-matter 
and white-matter in the brain), CT has somewhat poor soft-tissue 
contrast. See the below head scans from an MR image and a CT image as an
 example, noting the contrast between the grey-matter (along the outside
 of the brain) and white-matter (the brighter tissue interior to the 
grey-matter) as well as the general noise level present in the brain for
 both images.</p></div></div><div class="kn"><div class="n p"><div class="ko kp kq kr ks kt aj ku ak kv am v"><div class="kw kx ky kz la n ao"><figure class="lb kn lc ea ld le lf paragraph-image"><div class="lk s ae ll"><div class="lm ln s"><div class="uf ug fl ex eu lh v eg li lj"><img alt="Image for post" class="fl ex eu lh v lo lp lq" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1BaCHM97Tmm5p292b4xk1yw.jpeg" width="512" height="512"></div><img alt="Image for post" class="ek lg fl ex eu lh v c" width="512" height="512"><noscript><img alt="Image for post" class="fl ex eu lh v" src="https://miro.medium.com/max/1024/1*BaCHM97Tmm5p292b4xk1yw.jpeg" width="512" height="512" srcSet="https://miro.medium.com/max/552/1*BaCHM97Tmm5p292b4xk1yw.jpeg 276w, https://miro.medium.com/max/1024/1*BaCHM97Tmm5p292b4xk1yw.jpeg 512w" sizes="512px"/></noscript></div></div></figure><figure class="lb kn lr ea ld le lf paragraph-image"><div class="lk s ae ll"><div class="ls ln s"><div class="uf ug fl ex eu lh v eg li lj"><img alt="Image for post" class="fl ex eu lh v lo lp lq" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1jTX0eIVUVngUmiP8GN-lPg.png" width="204" height="256"></div><img alt="Image for post" class="ek lg fl ex eu lh v c" width="204" height="256"><noscript><img alt="Image for post" class="fl ex eu lh v" src="https://miro.medium.com/max/408/1*jTX0eIVUVngUmiP8GN-lPg.png" width="204" height="256"/></noscript></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb lx ae ly lz" data-selectable-paragraph="">Example of a CT scan (left) and (T1-weighted) MR scan (right) [from the <a href="http://headctstudy.qure.ai/dataset" class="co jf" rel="noopener nofollow">Qure.ai</a> and <a href="https://www.nitrc.org/projects/multimodal/" class="co jf" rel="noopener nofollow">Kirby 21</a> dataset, respectively]</figcaption></figure></div></div></div></div><div class="n p"><div class="ag ah ai aj ak fu am v"><p id="118f" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Some
 of the reasons that MR scans are not always used are: 1) some people 
can’t due to a variety of reasons (e.g., no access, certain types of 
metal implants, etc.), 2) MR scans take a relatively long time compared 
to CT scans and 3) radiologists are interested in the particular 
measurements that CT can provide (e.g., looking at bone structure). Now 
that we have a basic understanding of the data and some of the 
intricacies of the imaging modalities, let’s discuss some datasets.</p><h1 id="f988" class="jg jh fx as ji jj jk iq jl jm jn is jo jp jq jr js jt ju jv jw jx jy jz ka kb gu" data-selectable-paragraph="">Datasets</h1><p id="d99c" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph="">Labeled
 data is somewhat sparse for medical images because radiologists are 
expensive, hospitals are concerned about lawsuits, and researchers are 
(often overly) protective of their data. As a result, there is not an 
ImageNet-equivalent in MR or CT. However, there are many commonly used 
datasets depending on the application domain. Since I mostly work with 
brain MR images, I’ll supply a small list of easily accessible datasets 
for MR and CT (brain) images along with the data format in parenthesis 
at the end of the bullet:</p><ul class=""><li id="bf81" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="http://brainweb.bic.mni.mcgill.ca/brainweb/" class="co jf" rel="noopener nofollow">Brainweb</a>: Simulated normal and MS brains with tissue/lesion segmentations (MINC)</li><li id="27b8" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://www.nitrc.org/projects/multimodal/" class="co jf" rel="noopener nofollow">Kirby 21</a>: Set of 21 healthy patients scanned twice (NIfTI)</li><li id="cff5" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://brain-development.org/ixi-dataset/" class="co jf" rel="noopener nofollow">IXI dataset</a>: Set of 600 healthy subject scans (NIfTI)</li><li id="966e" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="http://headctstudy.qure.ai/dataset" class="co jf" rel="noopener nofollow">Qure.ai CT head scan data</a>: Set of 491 head CT scans with pathology [no segmentation, but radiology report] (DICOM)</li></ul><p id="c960" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Here is a list of <em class="kh">not so easy</em> to download (but very useful) datasets.</p><ul class=""><li id="9456" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="http://braintumorsegmentation.org/" class="co jf" rel="noopener nofollow">BraTS 2018 Brain Tumor data</a>: Large set of patients with brain tumors along with the tumor segmentation (mha)</li><li id="25cd" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://smart-stats-tools.org/lesion-challenge" class="co jf" rel="noopener nofollow">ISBI 2015 Multiple Sclerosis Challenge data</a>: Set of 20 patients with multiple sclerosis (MS) [only 5 have lesion segmentations] (NIfTI)</li></ul><p id="a61e" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">I have not worked with the set of datasets below, but I know people who have and am including them for completeness.</p><ul class=""><li id="c017" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://www.oasis-brains.org/#dictionary" class="co jf" rel="noopener nofollow">OASIS</a></li><li id="b4be" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://www.aapm.org/GrandChallenge/LowDoseCT/" class="co jf" rel="noopener nofollow">Low Dose CT</a></li><li id="3562" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="https://fastmri.med.nyu.edu/" class="co jf" rel="noopener nofollow">fastMRI</a></li><li id="7152" class="in io fx ip b gw md iq ir gz me is it iu mf iv iw ix mg iy iz ja mh jb jc je ma mb mc gu" data-selectable-paragraph=""><a href="http://adni.loni.usc.edu/data-samples/access-data/" class="co jf" rel="noopener nofollow">ADNI</a></li></ul><p id="2dd4" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Another place to look for datasets is in <a href="https://openneuro.org/" class="co jf" rel="noopener nofollow">OpenNeuro</a>
 which is a repository for researchers to host their brain imaging 
datasets; it mostly consists of fMRI from what I can tell. If your 
passion lies somewhere besides MR and CT <em class="kh">brain </em>images, then I’m unfortunately not a great resource. My first guess would be to look at the “grand challenges” listed <a href="https://grand-challenge.org/challenges/" class="co jf" rel="noopener nofollow">here</a> and see if it is possible to gain access to the data.</p><p id="9b8b" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Not to bury the lede too much, but perhaps the easiest way to get access to some of the above data is through <a href="http://academictorrents.com/" class="co jf" rel="noopener nofollow">this website</a>. I’m not sure that everything is sanctioned to be on there, which is why I have delayed to bring this up.</p><h1 id="4091" class="jg jh fx as ji jj jk iq jl jm jn is jo jp jq jr js jt ju jv jw jx jy jz ka kb gu" data-selectable-paragraph="">Preprocessing</h1><p id="da1e" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph="">The
 amount of data wrangling and preprocessing required to work with MR and
 CT can be considerable. I’ll outline the bare necessities below.</p><p id="42bc" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">The first thing to consider is how to load the images into python. The simplest route is to use <a href="http://nipy.org/nibabel/" class="co jf" rel="noopener nofollow">nibabel</a>. Then you can simply use</p><pre class="kw kx ky kz la mi mj mk"><span id="7d38" class="gu ml jh fx mm b cd mn mo s mp" data-selectable-paragraph="">import nibabel as nib<br>data = nib.load('mydata.nii.gz').get_data()</span></pre><p id="07fa" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">to get a numpy array containing the data inside the <code class="ll mq mr ms mm b">mydata.nii.gz</code>
 file. Note that I’ll refer to the indices of this 3D volume as a voxel 
which is the 3D-equivalent of a pixel for a 2D image. For work with 
brain images at least, I’d recommend always converting the files to 
NIfTI (which corresponds to the .nii or .nii.gz extension). I find 
converting everything to NIfTI first makes my life easier since I can 
assume all input images are of type NIfTI. <a href="http://nipy.org/nibabel/dicom/dcm2nii_algorithms.html" class="co jf" rel="noopener nofollow">Here is a tool</a> to convert DICOM to NIfTI, <a href="https://gist.github.com/jcreinhold/a26d6555b0e7aa28b79757f766640dd6" class="co jf" rel="noopener nofollow">here is a script</a> to convert MHA to NIfTI and <a href="https://gist.github.com/jcreinhold/fdd701211191450284c5718502eabbd4" class="co jf" rel="noopener nofollow">here is a script to convert PAR/REC</a>
 files to NIfTI. There are more file formats that you’ll probably need 
to work with, and you can use some of those scripts as inspiration to 
convert those file types.</p><p id="d275" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">We’ll
 first outline resampling, bias-field correction and registration which 
are staples of any medical image analysis. For these preprocessing 
steps, I’d recommend ANTs and specifically the ANTsPy variety (assuming 
you are coming from a python background). ANTs is actively maintained 
and has reliable tools to solve all of these (and many more) problems. 
Unfortunately, ANTsPy is not always easy to install, but I believe work 
is being done on it to solve some of the issues and once you are up and 
running with it you can access most of the tools ANTs offers natively 
from python. In particular, it supports the resampling, bias-field 
correction and registration preprocessing steps I’ll be discussing next.</p><p id="df11" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">As
 with natural images, MR and CT images do not have a standard resolution
 or standard image size. I’d argue that this fact is of greater 
importance in MR and CT though and must be considered for optimal ML 
performance. Consider the following: you train a 3D convolutional neural
 network with data acquired at 1x1x3 mm³ resolution and then you input 
an image into the network with 1x1x1 mm³. I would expect the result to 
be sub-optimal since the convolutional kernels will not be using the 
same spatial information. This is debatable and I haven’t examined the 
problem closely, but the non-standard resolution is something to keep in
 mind if you run into problems at test time. We can naively address the 
non-standard resolution problem by resampling the image to a desired, 
standard resolution (with cubic B-splines, of course, for the best 
quality).</p><p id="f691" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">For many applications, both MR and CT often require a process called <em class="kh">registration</em>
 in order to align objects across a set of images for direct comparison.
 Why would we want to do this? Let’s say you want to learn a function 
that takes an MR image and outputs an estimate of what the CT image 
would look like. If you have paired data (that is, an MR and CT image 
from the same patient), then a simple way of approaching this problem 
would be to learn the voxel-wise map between the image intensities. 
However, if the anatomy is not aligned in the image space, then we 
cannot learn this map in a supervised way. We solve this problem by 
registering the images and, in fact, we examine this problem in the 
experiment section.</p><p id="dd09" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">The next two problems (described in the next two paragraphs) are specific to MR. First is that we have <a href="http://johnmuschelli.com/imaging_in_r/inhomogeneity_correction_ms/index.pdf" class="co jf" rel="noopener nofollow">inhomogeneous image intensities due to the scanner</a>
 in MR images. Since this inhomogeneity is not a biological feature, we 
generally want to remove it and we do so with a process referred to as <mark class="ui uj ou"><em class="kh">bias-field correction</em></mark><em class="kh"> </em>(I’ll discuss one solution in the experiment section).</p><p id="f9ed" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Another
 issue in MR are inconsistent tissue intensities across different MR 
scanners. While CT images have a standard intensity scale (see <a href="https://en.wikipedia.org/wiki/Hounsfield_scale" class="co jf" rel="noopener nofollow">Hounsfield units</a>), we are not so lucky with MR images. MR images absolutely do<em class="kh"> not </em>have a standard scale, and the impact on algorithm performance can be <a href="https://arxiv.org/pdf/1812.04652.pdf" class="co jf" rel="noopener nofollow">quite large</a>
 if not accounted for in preprocessing. See the images below for an 
example where we plot the histograms of a set of T1-weighted MR images 
without any intensity normalization applied (see the image with “Raw” in
 the title). This variation is due to effects caused by the scanner and 
not due to the biology, which is the thing we generally care about.</p></div></div><div class="kn"><div class="n p"><div class="ko kp kq kr ks kt aj ku ak kv am v"><div class="kw kx ky kz la n ao"><figure class="lb kn mt ea ld le lf paragraph-image"><div role="button" tabindex="0" class="mu mv ae mw v mx"><div class="lk s ae ll"><div class="my ln s"><div class="uf ug fl ex eu lh v eg li lj"><img alt="Image for post" class="fl ex eu lh v lo lp lq" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1emSiC9f7Osn8ixkqUvOK9A.png" width="1200" height="1000"></div><img alt="Image for post" class="ek lg fl ex eu lh v c" width="1200" height="1000"><noscript><img alt="Image for post" class="fl ex eu lh v" src="https://miro.medium.com/max/2400/1*emSiC9f7Osn8ixkqUvOK9A.png" width="1200" height="1000" srcSet="https://miro.medium.com/max/552/1*emSiC9f7Osn8ixkqUvOK9A.png 276w, https://miro.medium.com/max/1000/1*emSiC9f7Osn8ixkqUvOK9A.png 500w" sizes="500px"/></noscript></div></div></div></figure><figure class="lb kn mt ea ld le lf paragraph-image"><div role="button" tabindex="0" class="mu mv ae mw v mx"><div class="lk s ae ll"><div class="my ln s"><div class="uf ug fl ex eu lh v eg li lj"><img alt="Image for post" class="fl ex eu lh v lo lp lq" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1e-05WKBZqnWyce97pbAjYw.png" width="1200" height="1000"></div><img alt="Image for post" class="ek lg fl ex eu lh v c" width="1200" height="1000"><noscript><img alt="Image for post" class="fl ex eu lh v" src="https://miro.medium.com/max/2400/1*e-05WKBZqnWyce97pbAjYw.png" width="1200" height="1000" srcSet="https://miro.medium.com/max/552/1*e-05WKBZqnWyce97pbAjYw.png 276w, https://miro.medium.com/max/1000/1*e-05WKBZqnWyce97pbAjYw.png 500w" sizes="500px"/></noscript></div></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb mz ae na lz" data-selectable-paragraph="">Histograms
 of image intensities from a cohort of T1-weighted brain images (left: 
white-matter mean normalized, right: no normalization applied)</figcaption></figure></div></div></div></div><div class="n p"><div class="ag ah ai aj ak fu am v"><p id="3184" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">There
 are a litany of intensity normalization techniques that attempt to 
remove this scanner variation (several of which I have collected in <a href="https://github.com/jcreinhold/intensity-normalization" class="co jf" rel="noopener nofollow">this repository</a> called <code class="ll mq mr ms mm b">intensity-normalization</code>). The techniques range from the very simple (e.g., simple <a href="https://en.wikipedia.org/wiki/Feature_scaling#Standardization" class="co jf" rel="noopener nofollow">standardization</a> which I’ll refer to as z-score normalization) to the fairly technical (e.g., <a href="https://github.com/Jfortin1/RAVEL" class="co jf" rel="noopener nofollow">RAVEL</a>).
 For neuroimaging, a good combination of speed and quality can be found 
in the Fuzzy C-Means (FCM) normalization technique which creates a rough
 tissue-class segmentation between the white-matter (WM), grey-matter 
and cerebrospinal fluid based on the T1-weighted image. The WM 
segmentation mask is then used to calculate the mean of the WM in the 
image which is set to some user-defined constant. This normalization 
technique seems to almost always produce the desired result in brain 
images. If you are not working on brain images, then you may want to 
look at either the <a href="https://www.ncbi.nlm.nih.gov/pubmed/10571928" class="co jf" rel="noopener nofollow">Nyúl &amp; Udupa method</a> or simple <a href="https://intensity-normalization.readthedocs.io/en/latest/algorithm.html#z-score" class="co jf" rel="noopener nofollow">z-score normalization</a>. All of these normalization methods are available as command-line interfaces (or importable modules) in the <code class="ll mq mr ms mm b">intensity-normalization</code> repository.</p><p id="80b1" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">The
 last preprocessing step we’ll consider is specific to brain images. In 
brain images, we generally only care about the brain and not necessarily
 the tissues outside of brain (e.g., the skull, fat and skin surrounding
 the brain). Furthermore, this extraneous tissue can complicate the 
learning procedure and trip up classification, segmentation, or 
regression tasks. To get around this we can use skull-stripping 
algorithms to create a mask of the brain and zero out the background. 
The simplest way to go about this (in MR) — with reasonable results — is
 with <a href="https://www.nitrc.org/projects/robex" class="co jf" rel="noopener nofollow">ROBEX</a>:
 a command-line tool that generally does a good job at extracting the 
brain from the image. I’ve seen it fail a few times on some data 
containing large pathologies or imaging artifacts, but other than that 
it is usually good enough for most machine learning tasks. For what it’s
 worth, I’d try to avoid skull-stripping your data since it is just 
another point of possible failure in your preprocessing routine, but 
sometimes it substantially helps.</p><p id="8387" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Since
 MR and CT images aren’t standard like JPEG, your computer doesn’t have a
 native way to display it. If you want to visualize your data, take a 
look at <a href="https://mipav.cit.nih.gov/" class="co jf" rel="noopener nofollow">MIPAV</a> for non-DICOM images (e.g., NIfTI) and <a href="https://horosproject.org/" class="co jf" rel="noopener nofollow">Horos</a>
 for DICOM images. It is always good to look at your data, especially 
after preprocessing so we can verify that everything looks reasonable. 
For instance, perhaps the registration failed (it often does) or perhaps
 the skull-stripping failed (again, it often occurs). If you pipe your 
crappy data into your ML algorithm, you’re probably going to get crappy 
output and you’ll waste a lot of time doing unnecessary debugging. So be
 kind to yourself and examine the data.</p><h1 id="a658" class="jg jh fx as ji jj jk iq jl jm jn is jo jp jq jr js jt ju jv jw jx jy jz ka kb gu" data-selectable-paragraph="">Training a deep network for MR or CT applications</h1><p id="ae21" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph="">While
 deep neural networks applied to MR and CT are increasingly moving to 3D
 models, there has been good success with 2D models. If you have limited
 memory on your GPU or you have very limited training data, you may want
 to use a 2D network to squeeze the most performance out of the network.
 If you use a 3D network, you will quickly run into memory issues when 
passing a full image or patches through the network.</p><p id="4ecf" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">If
 you decide a 2D network is the way to go for your application (a 
reasonable choice), you’ll need to figure out/design a data loader to 
handle this. After fussing around with complicated data loaders that 
take the 3D image to a 2D image patch or slice for a while, I realized 
that that was all an unnecessary burden that made it harder to use 
pre-built data loader/data augmentation tools that aid in training. Thus
 my recommended solution to this problem is to simply convert the 3D 
volumes to 2D images. Since the original volumes are floating point 
numbers, I went with the TIFF image format which supports such types. 
Here is a <a href="https://gist.github.com/jcreinhold/01daf54a6002de7bd8d58bad78b4022b" class="co jf" rel="noopener nofollow">command-line script</a>
 which takes a directory of NIfTI images and creates a directory of 
corresponding 2D TIFF images (with some options to create slices based 
on axis and to only create slices from a portion of the image in order 
to avoid background slices).</p><p id="c72e" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">In
 the following section, I’ll build a deep neural network with 3D 
convolutional layers. I’m doing this as opposed to using 2D 
convolutional layers because — once you convert the 3D volume to 2D 
images like TIFF — you can basically just use any 2D architecture you 
have lying around substituting the head for the appropriate application.
 Since the 3D problem is slightly more tricky to approach, I’ll dig into
 it below.</p><h2 id="e329" class="ml jh fx as ji nb nc gy jl nd ne hb jo hc nf he js hf ng hh jw hi nh hk ka ni gu" data-selectable-paragraph="">Experiment</h2><p id="78aa" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph=""><strong class="ip fy"><em class="kh">***
 If you are just coming to this blog post (after 05/07/20), note that 
the fastai package has changed significantly and the code below may not 
work as expected. However, the code examples and general experimental 
setup below should still be useful for learning purposes. For what it’s 
worth, I’d recommend using PyTorch over fastai for future deep learning 
projects. If you want NIfTI support in PyTorch, I have an actively 
maintained package which has working code examples and importable 
functions </em></strong><a href="https://github.com/jcreinhold/niftidataset" class="co jf" rel="noopener nofollow"><strong class="ip fy"><em class="kh">here</em></strong></a><strong class="ip fy"><em class="kh">. ***</em></strong></p><p id="c6c5" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">In this section, I’ll outline the steps required to train a 3D convolutional neural network for a MR-to-MR synthesis task using <a href="https://pytorch.org/" class="co jf" rel="noopener nofollow">pytorch</a> and <a href="https://github.com/fastai/fastai" class="co jf" rel="noopener nofollow">fastai</a>.
 If you just want to look at the code, then there is also a notebook 
which contains most of the experiment (excluding preprocessing) <a href="https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d" class="co jf" rel="noopener nofollow">here</a>.</p><p id="98f9" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">The setup is as follows: we’ll train a very small <a href="https://arxiv.org/abs/1512.03385" class="co jf" rel="noopener nofollow">resnet</a>
 to take an entire 3D volume from one MR contrast to another MR 
contrast; we’ll be learning the transform to map T1-weighted images to 
FLAIR images. This task is called <em class="kh">MR image synthesis </em>and we’ll refer to the network as a <em class="kh">synthesis network</em>.
 There are a variety of applications for this type of synthesis, but 
motivation for this problem is mostly that: MR scan time is limited, so 
not all contrasts can be collected. But we want to eat our cake and have
 it too, and we sometimes want those uncollected contrasts for image 
processing purposes. Thus we create some fake data using the data that 
actually was collected, where the fake data will be the result of our 
synthesis network.</p><p id="d3e7" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">In this experiment, I’ll be using 11 and 7 images as training and validation, respectively, from the <a href="https://www.nitrc.org/projects/multimodal/" class="co jf" rel="noopener nofollow">Kirby 21</a> dataset. All images have been resampled to 1x1x1 mm³, bias-field corrected using <a href="https://www.ncbi.nlm.nih.gov/pubmed/20378467" class="co jf" rel="noopener nofollow">N4</a>, and the FLAIR images have been (affine) registered to the T1-weighted images using ANTsPy. Look <a href="https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/utilities/preprocess.py" class="co jf" rel="noopener nofollow">here</a> and <a href="https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/exec/coregister.py" class="co jf" rel="noopener nofollow">here</a> for the actual code I used to do the preprocessing (both are available as command-line interfaces when the <a href="https://github.com/jcreinhold/intensity-normalization" class="co jf" rel="noopener nofollow">intensity-normalization</a> package is installed along with ANTsPy). Finally, all the images were individually <a href="https://github.com/jcreinhold/intensity-normalization/blob/master/intensity_normalization/normalize/zscore.py" class="co jf" rel="noopener nofollow">z-score normalized</a> using the entire image.</p><p id="faaf" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Now
 that we have motivated the problem somewhat and talked about the data 
we will use, let’s get to the code. The code block below defines some 
necessary constructs to work with fastai, specifically to use the <a href="https://docs.fast.ai/data_block.html" class="co jf" rel="noopener nofollow">data_block</a> API.</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Dataloaders classes and functions for NIfTI images in fastai</figcaption></figure><p id="2be5" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">There
 is nothing particular to remark on here, except that once you figure 
out how to setup these types of structures, they are quite convenient 
(see the <a href="https://docs.fast.ai/tutorial.itemlist.html" class="co jf" rel="noopener nofollow">ItemList tutorial</a>
 for more details). Note that not all functionality is supported with 
the current setup — I stripped it down to make it as simple as possible —
 but it’ll get the job done. I’ll show how this creates the training and
 validation dataloaders below. First, let’s define a preprocessing 
transform:</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Relevant data transforms for our application</figcaption></figure><p id="00eb" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Why
 am I defining this odd cropping function? The reason is two-fold. The 
first reason is that the neck is not present in the FLAIR images but <em class="kh">is</em>
 present in the T1-weighted images. I don’t want the network to learn to
 take tissue to zero, so I’m removing that part of the data by only 
using the data in the 20–80 percent range along the axis corresponding 
to the <a href="https://en.wikipedia.org/wiki/Anatomical_plane" class="co jf" rel="noopener nofollow">axial plane</a>.
 The second reason is that I can fit twice as many samples into a batch 
(that means a batch size of 2). The reason for the small batch size is 
that, like I previously mentioned, 3D networks with large images are 
memory-intensive. Why no other data augmentation? Unfortunately, 3D 
transforms are not natively supported with pytorch or fastai so I’d have
 to incorporate my own and I am not doing this for simplicity. Now let’s
 use the data_block API of fastai to create the training and validation 
dataloaders:</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Define the dataloader with the data block API in fastai</figcaption></figure><p id="346f" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">You can see the <a href="https://nbviewer.jupyter.org/gist/jcreinhold/78943cdeca1c5fca4a5af5d066bd8a8d" class="co jf" rel="noopener nofollow">notebook</a> for more details, but essentially I have the T1-weighted images in one directory with <code class="ll mq mr ms mm b">train, valid, test</code> subdirectories and a parallel directory with the FLAIR images. The <code class="ll mq mr ms mm b">get_y_fn</code> function grabs the FLAIR image corresponding to the source T1-weighted image. Look <a href="https://docs.fast.ai/data_block.html" class="co jf" rel="noopener nofollow">here</a> for more in-depth explanation of the remaining commands. Note that the <code class="ll mq mr ms mm b">(tfms,tfms)</code>
 means that I am applying the previously defined crop to both the 
training and validation set. Applying that transform to the validation 
set isn’t ideal, but is required because of memory-constraints. Now 
let’s create some 3D convolutional and residual block layers which we’ll
 use to define our model:</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Define some custom 3D layers</figcaption></figure><p id="0f4a" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">I’m closely following the definition of the 2D convolutional and residual block layers as defined in the <a href="https://github.com/fastai/fastai/blob/master/fastai/layers.py#L94" class="co jf" rel="noopener nofollow">fastai repository</a>. As a side note, I left the spectral normalization and weight normalization routines in the <code class="ll mq mr ms mm b">conv3d</code>
 definition, but disappointingly received worse results with those 
methods than when using batch norm (and I’m still not sure whether batch
 norm is applied before or after the activation). Now let’s define our 
model using the above layers:</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Define our network</figcaption></figure><p id="14fd" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Here
 I have just defined the very small resnet model. Why so few layers? I 
am using as large of a network as my GPU can contain in memory. The 
creation of many channels with the entire 3D volume and the residual 
connections are burdens on the GPU memory. The only other thing of 
possible intrigue is that I use a 1x1x1 kernel at the end, which 
empirically produces crisper images (and I think is fairly standard). As
 a note, I realize that I should have removed the activation from the 
final layer; however, it is not a problem because I am z-score 
normalizing (i.e., mean subtracting and dividing by the standard 
deviation) the images <em class="kh">with</em> their backgrounds. The 
backgrounds, which are approximately zero, take up the vast majority of 
the volume of the image. Thus the z-score normalization essentially puts
 the background (corresponding to the mean) at zero, which makes the 
intensities of the head greater than zero. A fine result for the ReLU. 
Now let’s train this network:</p><figure class="kw kx ky kz la kn"><div class="lk s ae"><div class="nj ln s"></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb">Training</figcaption></figure><p id="5c94" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">Again
 fairly normal. Mean square error is used because we want each voxel 
intensity in our source T1-weighted image to match the voxel intensity 
of the corresponding target FLAIR image. We use <code class="ll mq mr ms mm b">lr_find</code> to help us pick a larger learning rate (as described <a href="https://arxiv.org/abs/1506.01186" class="co jf" rel="noopener nofollow">here</a>) for faster training, in addition to using the <a href="https://sgugger.github.io/the-1cycle-policy.html" class="co jf" rel="noopener nofollow">one-cycle</a> <a href="https://arxiv.org/pdf/1803.09820.pdf" class="co jf" rel="noopener nofollow">policy</a>.
 I always collect my training and validation data into a CSV file to 
look at how the network is converging, especially on machines where 
launching a jupyter notebook is a pain. I picked 100 epochs because I 
ran this a couple times and did not notice a great amount performance 
gain with more epochs.</p><p id="6965" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">After
 training completes, we input an entire image (not seen in either 
training or validation) into the network. An example is shown in below 
figure, where the synthesis result is the right-most image (with the 
title “Syn”).</p><figure class="kw kx ky kz la kn fi fj paragraph-image"><div role="button" tabindex="0" class="mu mv ae mw v mx"><div class="fi fj nk"><div class="lk s ae ll"><div class="nl ln s"><div class="uf ug fl ex eu lh v eg li lj"><img alt="Image for post" class="fl ex eu lh v lo lp lq" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1vG9FkLP9QctoxmI6j72gUQ.png" width="2554" height="1054"></div><img alt="Image for post" class="ek lg fl ex eu lh v c" width="2554" height="1054"><noscript><img alt="Image for post" class="fl ex eu lh v" src="https://miro.medium.com/max/5108/1*vG9FkLP9QctoxmI6j72gUQ.png" width="2554" height="1054" srcSet="https://miro.medium.com/max/552/1*vG9FkLP9QctoxmI6j72gUQ.png 276w, https://miro.medium.com/max/1104/1*vG9FkLP9QctoxmI6j72gUQ.png 552w, https://miro.medium.com/max/1280/1*vG9FkLP9QctoxmI6j72gUQ.png 640w, https://miro.medium.com/max/1400/1*vG9FkLP9QctoxmI6j72gUQ.png 700w" sizes="700px"/></noscript></div></div></div></div><figcaption class="lt lu fk fi fj lv lw as b at au fb" data-selectable-paragraph="">Example
 synthesis result with the network defined above (from left to right: 
the input T1-weighted image, the true FLAIR image, and the synthesized 
FLAIR image)</figcaption></figure><p id="e029" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">While the above figure could have been had better <a href="http://www.upstate.edu/radiology/education/rsna/intro/display.php" class="co jf" rel="noopener nofollow">window/level settings</a>
 for better comparison, we see that the T1-weighted image does take on 
many of the characteristics of the true FLAIR image. Most notably, 
inside the brain, we see that the white-matter becomes less bright than 
the grey-matter while the cerebrospinal fluid remains dark. The noise 
characteristics are <em class="kh">not </em>the same though and there are some bright spots in the true FLAIR that are not captured in the synthesized image.</p><p id="f6fd" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">This
 result is not state-of-the-art by any means, but it’s interesting to 
see that we can learn an approximate transform with such an incredibly 
small dataset, no data augmentation, and a very small network. This 
network would assuredly be better with more data, data augmentation and a
 larger network, but this is just a simple, pedagogical toy example. I 
should note that unless you have a particularly large GPU (and 
contradicting my last statement), you may not be able to train this 
network with the full images. You’ll probably have to use either 3D 
patches or 2D slices (or 2D patches).</p><h1 id="9e54" class="jg jh fx as ji jj jk iq jl jm jn is jo jp jq jr js jt ju jv jw jx jy jz ka kb gu" data-selectable-paragraph="">Conclusion</h1><p id="8f8a" class="in io fx ip b gw kc iq ir gz kd is it iu ke iv iw ix kf iy iz ja kg jb jc je fq gu" data-selectable-paragraph="">Hopefully
 this post provided you with a starting point for applying deep learning
 to MR and CT images with fastai. Like most machine learning tasks, 
there is a considerable amount of domain-specific knowledge, 
data-wrangling and preprocessing that is required to get started, but 
once you have this under your belt, it is fairly easy to get 
up-and-running with training a network with pytorch and fastai. Where to
 go from here? I’d download a dataset from one of the links I posted in 
the Datasets section and try to do something similar to what I showed 
above, or even just try to recreate what I did. If you can get to that 
stage, you’ll be in a comfortable place to apply deep learning to other 
problems in MR and CT.</p><p id="d72f" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">I
 should note that there is work being done to create standard code bases
 from which you can apply deep learning to MR and CT images. The two 
that I am aware of are <a href="http://www.niftynet.io/" class="co jf" rel="noopener nofollow">NiftyNet</a> and <a href="https://github.com/perone/medicaltorch" class="co jf" rel="noopener nofollow">medicaltorch</a>.
 NiftyNet abstracts away most of the neural network design and data 
handling, so that the user only has to call some command-line interfaces
 by which to download a pre-trained network, fine-tune it, and do 
whatever. So if that is good enough for your needs, then go right ahead;
 it seems like a great tool and has some pre-trained networks available.
 medicaltorch provides some dataloaders and generic deep learning models
 with medical images in pytorch. I have not tested either extensively, 
so I cannot comment on their utility.</p><p id="562e" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">If you don’t like python, there is <a href="https://neuroconductor.org/" class="co jf" rel="noopener nofollow">neuroconductor</a> in R or <a href="https://github.com/JuliaIO/NIfTI.jl" class="co jf" rel="noopener nofollow">NIfTI.jl</a> and <a href="https://fluxml.ai/" class="co jf" rel="noopener nofollow">Flux.jl</a>
 packages in Julia which can read NIfTI images and build neural 
networks, respectively. There are countless other relevant software 
packages, but those are the ones the first come to mind and that I’ve 
worked with.</p><p id="ff52" class="in io fx ip b gw ki iq ir gz kj is it iu kk iv iw ix kl iy iz ja km jb jc je fq gu" data-selectable-paragraph="">As
 a final note, if you have luck creating a nice application for MR or CT
 make sure to share your work! Write a paper/blog post, put it up on a 
forum, <a href="https://pytorch.org/docs/master/hub.html" class="co jf" rel="noopener nofollow">share the network weights</a>.
 It would be great to see more people apply deep learning techniques to 
this domain and push the boundary of the field where possible. Best of 
luck.</p></div></div></section></div></article><div class="uf fp ev nm v yu nr nu" data-test-id="post-sidebar"><div class="n p"><div class="ag ah ai aj ak al am v"><div class="nv n x"><div class="yv"><div><div class="nw et s"><p class="as b nx ny nz fb oa">Written by</p><div class="le ld s"><a href="https://medium.com/@jcreinhold?source=post_sidebar--------------------------post_sidebar-----------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><h2 class="as ji cd au fw gu fq">Jacob Reinhold</h2></a></div><div class="ob s"><h4 class="as b at au fb">Electrical engineering Ph.D. student | <a href="http://jcreinhold.com/" class="co cp ay az ba bb bc bd be bf bi ig ih jf fr" rel="noopener nofollow">jcreinhold.com</a> | @JacobCReinhold</h4></div><div class="oc s"><button class="as b at au fc od da fd oe fe ff of bf dd de og oh oi di dj dk dl cy dm">Follow</button></div></div><div class="oj ok ol n"><div class="n o"><div class="s ae om on oo op oq"><div class=""><button class="bd or os ot ou ov ow ox r oy oz"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s pa pb pc pd pe pf pg"><div class="ph"><h4 class="as b at au fb"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih">814 </button></h4></div></div></div></div><div class="ok s"><button class="ou os bd"><div class="pk n o ao"><svg width="25" height="25" class="r" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><div class="s ae pl pm pn po pp pq pr ps"><h4 class="as b at au fb">10<!-- --> </h4></div></div></button></div><div><div class="ij"><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="2" aria-labelledby="2"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></div><div class="uf yv nm ev nn no np nq nr ns"></div><div><div class="pt kn n x p"><div class="n p"><div class="ag ah ai aj ak fu am v"><div class="pu pv pw px py pz"><div class="qa s"><h2 class="as ji nb gy jl nd hb jo hc he js hf hh jw hi hk ka gu">Sign up for The Daily Pick</h2></div><div class="qb s"><h3 class="as b qc au gu">By Towards Data Science</h3></div><div class="qd qe s"><p class="as b qf qg qh qi qj qk ql qm qn qo gu">Hands-on
 real-world examples, research,  tutorials, and cutting-edge techniques 
delivered Monday to Thursday. Make learning your daily ritual.&nbsp;<a href="https://medium.com/towards-data-science/newsletters/the-daily-pick?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="co cp ay az ba bb bc bd be bf bi ig ih jf" rel="noopener">Take a look</a></p></div><div class="n bv"><div class="qp s qq"><button class="as b cd ce qr qs qt qu oe qv of bf dd de qw oi di dj dk dl cy dm"><span class="ii" aria-hidden="true"><svg width="20" height="16" viewBox="0 0 20 16"><path d="M0 .35v15.3h20V.35H0zm6.95 9.38l3.05 2.5 3.05-2.5 4.88 4.73H2.07l4.88-4.73zM1.2 13.64V5.02l4.82 3.94-4.82 4.68zm12.78-4.68l4.82-3.94v8.62l-4.82-4.68zm4.82-7.42v1.94l-8.8 7.2-8.8-7.2V1.54h17.6z"></path></svg></span>Get this newsletter</button></div><div class="qx qy s"><p class="as b qc au gu">Emails will be sent to vesaalexandru95@gmail.com.<div class="s"><span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5&amp;source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener"><button class="co cp ay az ba bb bc bd be bf bi ig ih jf" target="_blank">Not you?</button></a></span></div></p></div></div></div><div class="n bv"></div><div class="n o bv"></div><div class="qz ra s"><div class="ra n bn ic"><div class="n ao"><div class="rb s"><span class="s rc rd re e d"><div class="n o"><div class="s ae om on oo op oq"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="3" aria-labelledby="3"><button class="bd or os ot ou ov ow ox r oy oz"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s pa pb pc pd pe pf pg"><div class="ae rf ph"><h4 class="as b at au gu"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih">814<span class="s h g f rg rh">&nbsp;</span></button><span class="s h g f rg rh"></span></h4></div></div></div></span><span class="s h g f rg rh"><div class="n bs"><div class="s ae om on"><div class=""><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="4" aria-labelledby="4"><button class="bd or os ot ou ov ow ox r oy oz"><svg width="33" height="33" viewBox="0 0 33 33" aria-label="clap"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="s pa pb pc pd ri rj rk rl rm rn"><div class="ae rf ph"><h4 class="as b at au gu"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih">814<span class="s h g f rg rh">&nbsp;</span></button><span class="s h g f rg rh"></span></h4></div></div></div></span></div><div class="s ro rp rq rr rs"></div><button class="ou os bd"><div class="pk n o ao"><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="5" aria-labelledby="5"><span class="rt s h g f rg rh"><svg width="33" height="33" viewBox="0 0 33 33" fill="none" class="r" aria-label="responses"><path clip-rule="evenodd" d="M24.28 25.5l.32-.29c2.11-1.94 3.4-4.61 3.4-7.56C28 11.83 22.92 7 16.5 7S5 11.83 5 17.65s5.08 10.66 11.5 10.66c1.22 0 2.4-.18 3.5-.5l.5-.15.41.33a8.86 8.86 0 0 0 4.68 2.1 7.34 7.34 0 0 1-1.3-4.15v-.43zm1 .45c0 1.5.46 2.62 1.69 4.44.22.32.01.75-.38.75a9.69 9.69 0 0 1-6.31-2.37c-1.2.35-2.46.54-3.78.54C9.6 29.3 4 24.09 4 17.65 4 11.22 9.6 6 16.5 6S29 11.22 29 17.65c0 3.25-1.42 6.18-3.72 8.3z"></path></svg></span><span class="ru s rc rd re e d"><svg width="25" height="25" class="r" aria-label="responses"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg></span></div></div><div class="s ae rv pm rw po rx pq ry rz sa sb"><h4 class="as b at au gu">10<!-- --> </h4></div></div></button></div><div class="n o"><div class="id s"><div class="cy" aria-hidden="false"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div><div class="sc s do"><div><div class="ij"><div><div class="cy" role="tooltip" aria-hidden="false" aria-describedby="6" aria-labelledby="6"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="Bookmark Post"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="cy" aria-hidden="false"><div class="cy" aria-hidden="false"><div class="s do"><button class="co cp ay az ba bb bc bd be bf ie if bi ig ih" aria-label="More options"><svg class="r il im" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div><div class="sd ra s"><ul class="bd be"><li class="cy se ii sf"><a href="https://towardsdatascience.com/tagged/machine-learning" class="as b qc sg fb sh si dm s mj">Machine Learning</a></li><li class="cy se ii sf"><a href="https://towardsdatascience.com/tagged/deep-learning" class="as b qc sg fb sh si dm s mj">Deep Learning</a></li><li class="cy se ii sf"><a href="https://towardsdatascience.com/tagged/medical-imaging" class="as b qc sg fb sh si dm s mj">Medical Imaging</a></li><li class="cy se ii sf"><a href="https://towardsdatascience.com/tagged/python" class="as b qc sg fb sh si dm s mj">Python</a></li><li class="cy se ii sf"><a href="https://towardsdatascience.com/tagged/pytorch" class="as b qc sg fb sh si dm s mj">Pytorch</a></li></ul></div></div></div><div><div class="n p"><div class="ag ah ai aj ak fu am v"><div class="s sj sk ic"></div></div></div><div class="s ic"><div class="sl sm s py"><div class="n p"><div class="ag ah ai aj ak fu am v"><div class="n o bn"><h2 class="as ji sn qg so jl sp qi sq jo sr qk ss js st qm su jw sv qo sw ka eg ef sx sy sz ta gu"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="co cp ay az ba bb bc bd be bf ie if bi ig ih" rel="noopener">More from Towards Data Science</a></h2><div class="cy" aria-hidden="false"><button class="as b at au fc od da fd oe fe ff of bf dd de og oh oi di dj dk dl cy dm"><div class="n ao">Follow</div></button></div></div><div class="ld tb s"><h4 class="as b at au fb">A Medium publication sharing concepts, ideas, and codes.</h4></div></div></div></div></div><div class="tc s py ic"><div class="n p"><div class="ko kq ks td te tf am v"><div class="tg pt s"><div class="tf s lu"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="as b at au qr cz qt qu oe qv of bf dd de qw oi di dj dk dl cy dm" rel="noopener">Read more from <!-- -->Towards Data Science</a></div></div></div></div></div><div class="s th ic"><div class="n p"><div class="ag ah ai aj ak al am v"></div></div></div></div></div></div><div class="ti s tj tk"><div class="n p"><div class="ag ah ai aj ak al am v"><div class="n x"><div class="n o bn"><a href="https://medium.com/?source=post_page-----e9f32273dcb5--------------------------------" aria-label="Go to homepage" class="co cp ay az ba bb bc bd be bf tl tm bi tn to" rel="noopener"><svg viewBox="0 0 3940 610" class="qt tp"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><h4 class="as b at au tq"><div class="qe tr n bn ts tt"><h4 class="as b cd ce tu"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf tv bi tn to" rel="noopener">About</a></h4><h4 class="as b cd ce tu"><a href="https://help.medium.com/hc/en-us?source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf tv bi tn to" rel="noopener">Help</a></h4><h4 class="as b cd ce tu"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf tv bi tn to" rel="noopener">Legal</a></h4></div></h4></div><div class="z tw tx tt"><h4 class="as b cd ce tq">Get the Medium app</h4></div><div class="z tw ty tt tz"><div class="ua s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf tl tm bi tn to" rel="noopener nofollow"><img alt="A button that says 'Download on the App Store', and if clicked it will lead you to the iOS App store" class="" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----e9f32273dcb5--------------------------------" class="co cp ay az ba bb bc bd be bf tl tm bi tn to" rel="noopener nofollow"><img alt="A button that says 'Get it on, Google Play', and if clicked it will lead you to the Google Play store" class="" src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__ = "main-20201204-191518-0ced06a182"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"auroraPage":{"isAuroraPageEnabled":true},"config":{"nodeEnv":"production","version":"main-20201204-191518-0ced06a182","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20201204-191518-0ced06a182"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","context":{"deployment":{"target":"production","tag":"main-20201204-191518-0ced06a182","commit":"0ced06a182716869f1c3c0d30aa6d9ef3361fb44"}},"datacenter":"us"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e","9dc80918cc93","8a9336e5bb4","cef6983b292","54c98c43354d","193b68bd4fba","b7e45b22fec3","55760f21cdc5"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"performanceTags":[],"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","errorTracking":"bugsnag"},"debug":{"requestId":"c1dd09d7-041a-42df-ade0-943489580ad6","branchDeployConfig":null,"originalSpanCarrier":{"ot-tracer-spanid":"0cb9a9a72ada40e4","ot-tracer-traceid":"19314aed8dffe688","ot-tracer-sampled":"true"}},"session":{"user":{"id":"a0286b59c498"},"xsrf":"14092c280199","isSpoofed":false},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register"},"postRead":false},"client":{"isBot":false,"isEu":true,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"inAppBrowserName":"","routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true},"supportsWebp":true,"useNeedForSpeed":false},"multiVote":{"clapsPerPost":{}},"tracing":{}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_resume_reading_toast","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_admin_braintree_discounts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_follow_pages","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_client_error_tracking","valueType":{"__typename":"VariantFlagString","value":"bugsnag"}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_curation_priority_queue_experiment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_daily_read_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_dedicated_series_tab_api_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_detailed_billing_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_different_grid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_disregard_trunc_state_for_footer","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_following_publications_list","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_ranked_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_write_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_post_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_daily_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_lo_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pay_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_cd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_post_highlights_view_only","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_profile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_header_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_more_on_coronavirus","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_checkout_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_suspended_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_user_onboarding_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_seo_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_table_of_contents","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_highlight","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sohne","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ticks_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_topic_lifecycle_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unbound","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_unfiltered_cf","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_embed_commands","valueType":{"__typename":"VariantFlagString","value":"control"}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_iceland_paywall","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"STRIPE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:a0286b59c498"},"meterPost({\"postId\":\"e9f32273dcb5\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"e9f32273dcb5\"})":{"__ref":"Post:e9f32273dcb5"}},"User:a0286b59c498":{"id":"a0286b59c498","__typename":"User","username":"vesaalexandru95","name":"Vesa Alexandru","imageId":"0*UPBPhpvitZdcnAku.","mediumMemberAt":1586155340000,"hasPastMemberships":true,"isPartnerProgramEnrolled":false,"email":"vesaalexandru95@gmail.com","unverifiedEmail":"","createdAt":1527340254190,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasDomain":false,"dismissableFlags":[]},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":3},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:396be67fd6cb":{"id":"396be67fd6cb","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"ff"},"postBackgroundColor":null,"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"START","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"START","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"backgroundImage":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_MEDIUM","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"postBody":null,"navigation":null,"postBodyOLI":null,"postBodyULI":null,"postBodyIframe":null,"postBodyImage":null,"postHeaderTitle":null,"postHeaderSubtitle":null,"postHeaderKicker":null,"postBodyP":null,"postBodyBQ":null,"postBodyPQ":null,"postBodyH2":null,"postBodyH3":null,"postBodyHR":null,"postBodyPRE":null},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-daily-pick","isSubscribed":false,"showPromo":true,"name":"The Daily Pick","description":"Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.","type":"NEWSLETTER_TYPE_COLLECTION","user":{"__typename":"User","name":"Ludovic Benistant"},"collection":{"__ref":"Collection:7f60cf5620c9"}},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:396be67fd6cb"},"tagline":"A Medium publication sharing concepts, ideas, and codes.","isAuroraEligible":true,"viewerIsEditor":false,"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:895063a310f4"},"subscriberCount":505453,"avatar":{"__ref":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png"},"isEnrolledInHightower":false,"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"canToggleEmail":true,"isUserSubscribedToCollectionEmails":false,"viewerIsMuting":false,"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"description":"A Medium publication sharing concepts, ideas, and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null},"User:d6cf1d0b0aa7":{"id":"d6cf1d0b0aa7","__typename":"User","isSuspended":false,"name":"Jacob Reinhold","hasCompletedProfile":false,"bio":"Electrical engineering Ph.D. student | jcreinhold.com | @JacobCReinhold","imageId":"1*fEG3lcZeY_zHxK2piY856A.jpeg","customStyleSheet":null,"hasDomain":false,"username":"jcreinhold","isAuroraVisible":true,"socialStats":{"__typename":"SocialStats","followerCount":169},"isBlocking":false,"mediumMemberAt":0,"isMuting":false,"isFollowing":false,"allowNotes":true,"newsletterV3":null,"viewerIsUser":false,"twitterScreenName":"JacobCReinhold","isPartnerProgramEnrolled":false},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Paragraph:824ceaaf7600_0":{"id":"824ceaaf7600_0","__typename":"Paragraph","name":"d086","text":"Deep Learning with Magnetic Resonance and Computed Tomography Images","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_1":{"id":"824ceaaf7600_1","__typename":"Paragraph","name":"74d1","text":"An introduction to the data, preprocessing techniques and deep network design for medical images","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_2":{"id":"824ceaaf7600_2","__typename":"Paragraph","name":"ca62","text":"Getting started with applying deep learning to magnetic resonance (MR) or computed tomography (CT) images is not straightforward; finding appropriate data sets, preprocessing the data, and creating the data loader structures necessary to do the work is a pain to figure out. In this post I hope to alleviate some of that pain for newcomers. To do so, I’ll link to several freely-available datasets, review some common\u002Fnecessary preprocessing techniques specific to MR and CT, and show how to use the fastai library to load (structural) MR images and train a deep neural network for a synthesis task.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":500,"end":506,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Ffastai\u002Ffastai","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_3":{"id":"824ceaaf7600_3","__typename":"Paragraph","name":"8171","text":"Overview of MR and CT images","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_4":{"id":"824ceaaf7600_4","__typename":"Paragraph","name":"96e5","text":"Before we get into the meat and bones of this post, it will be useful to do a quick overview of the medical images that we’ll be talking about and some idiosyncrasies of the types of images in discussion. I’ll only be talking about structural MR images and (to a lesser degree) computed tomography (CT) images. Both of these types of imaging modalities are used to view the structure of the tissue; this is opposed to functional MR images (fMRI) or positron emission tomography (PET) scans which image blood flow activity and metabolic activity, respectively.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":232,"end":243,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":374,"end":383,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_5":{"id":"824ceaaf7600_5","__typename":"Paragraph","name":"5ea6","text":"For people not acquainted with medical images at all, note that medical image statistics are different from natural image statistics. For example, a mammography image looks nothing like any picture that a human would take with their smart phone. This is obvious of course; however, I think it is important to have this in mind when designing networks and working with the data to make some sort of machine learning (ML) algorithm. That is not to say that using common networks or transfer learning from domains outside medical imaging won’t work; it is only to say that knowing the characteristics of common issues regarding medical imaging will help you debug your algorithm. I’ll discuss specific examples of these characteristics in the preprocessing section below and show ways to reduce the impact of some of these unique problems.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":64,"end":132,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_6":{"id":"824ceaaf7600_6","__typename":"Paragraph","name":"f660","text":"I’m not going to go into much detail about the intricacies of structural MR imaging. A good place to start for more in-depth details of MR is this website, which goes into depth regarding any topic that an ML practitioner working with MR would care about. I’ll note that there are many different types of MR images that an MR scanner can produce. For instance, there are T1-weighted, T2-weighted, PD-weighted, FLuid-Attenuated Inversion Recovery (FLAIR), among others. To make things more complicated, there are sub-types of those types of images (e.g., T1-weighted images come in the flavors: MPRAGE, SPGR, etc.). Depending on your task, this information may be extremely useful because of the unique characteristics of each of these types and sub-types of images. The reason for all these different types of images is because MR scanners are flexible machines that can be programmed to collect different information according to different pulse sequences. The upshot is that all of these images are not just redundant information; they contain useful and unique information regarding clinical markers that radiologists (or us as image processors) care about. Again, I’ll discuss more details regarding unique aspects of MR in the preprocessing section.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":142,"end":155,"type":"A","href":"https:\u002F\u002Fwww.mriquestions.com\u002Findex.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":941,"end":956,"type":"A","href":"http:\u002F\u002Fmriquestions.com\u002Fhellippulse-sequences.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_7":{"id":"824ceaaf7600_7","__typename":"Paragraph","name":"7b5d","text":"While there is contrast and non-contrast CT, there are not as many varieties of images that can be created with a CT scanner. Vaguely, the CT scanner shoots high-energy photons through you whose energy is calculated via a detector on the other side of your body which the photons hit. When images like this are taken from a variety of angles, we can use our knowledge of the geometry at which the images were acquired to reconstruct the image into a 3D volume. The physical representation of the energy lets us map the found intensity values to a standard scale which also simplifies our life and is discussed more in the preprocessing section. I should note that while MR is good at soft-tissue contrast (e.g., the ability to discern between gray-matter and white-matter in the brain), CT has somewhat poor soft-tissue contrast. See the below head scans from an MR image and a CT image as an example, noting the contrast between the grey-matter (along the outside of the brain) and white-matter (the brighter tissue interior to the grey-matter) as well as the general noise level present in the brain for both images.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_8":{"id":"824ceaaf7600_8","__typename":"Paragraph","name":"b195","text":"","type":"IMG","href":null,"layout":"OUTSET_ROW","metadata":{"__ref":"ImageMetadata:1*BaCHM97Tmm5p292b4xk1yw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_9":{"id":"824ceaaf7600_9","__typename":"Paragraph","name":"4d95","text":"Example of a CT scan (left) and (T1-weighted) MR scan (right) [from the Qure.ai and Kirby 21 dataset, respectively]","type":"IMG","href":null,"layout":"OUTSET_ROW_CONTINUE","metadata":{"__ref":"ImageMetadata:1*jTX0eIVUVngUmiP8GN-lPg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":72,"end":79,"type":"A","href":"http:\u002F\u002Fheadctstudy.qure.ai\u002Fdataset","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":84,"end":92,"type":"A","href":"https:\u002F\u002Fwww.nitrc.org\u002Fprojects\u002Fmultimodal\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_10":{"id":"824ceaaf7600_10","__typename":"Paragraph","name":"118f","text":"Some of the reasons that MR scans are not always used are: 1) some people can’t due to a variety of reasons (e.g., no access, certain types of metal implants, etc.), 2) MR scans take a relatively long time compared to CT scans and 3) radiologists are interested in the particular measurements that CT can provide (e.g., looking at bone structure). Now that we have a basic understanding of the data and some of the intricacies of the imaging modalities, let’s discuss some datasets.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_11":{"id":"824ceaaf7600_11","__typename":"Paragraph","name":"f988","text":"Datasets","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_12":{"id":"824ceaaf7600_12","__typename":"Paragraph","name":"d99c","text":"Labeled data is somewhat sparse for medical images because radiologists are expensive, hospitals are concerned about lawsuits, and researchers are (often overly) protective of their data. As a result, there is not an ImageNet-equivalent in MR or CT. However, there are many commonly used datasets depending on the application domain. Since I mostly work with brain MR images, I’ll supply a small list of easily accessible datasets for MR and CT (brain) images along with the data format in parenthesis at the end of the bullet:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_13":{"id":"824ceaaf7600_13","__typename":"Paragraph","name":"bf81","text":"Brainweb: Simulated normal and MS brains with tissue\u002Flesion segmentations (MINC)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"A","href":"http:\u002F\u002Fbrainweb.bic.mni.mcgill.ca\u002Fbrainweb\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_14":{"id":"824ceaaf7600_14","__typename":"Paragraph","name":"27b8","text":"Kirby 21: Set of 21 healthy patients scanned twice (NIfTI)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"A","href":"https:\u002F\u002Fwww.nitrc.org\u002Fprojects\u002Fmultimodal\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_15":{"id":"824ceaaf7600_15","__typename":"Paragraph","name":"cff5","text":"IXI dataset: Set of 600 healthy subject scans (NIfTI)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":11,"type":"A","href":"https:\u002F\u002Fbrain-development.org\u002Fixi-dataset\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_16":{"id":"824ceaaf7600_16","__typename":"Paragraph","name":"966e","text":"Qure.ai CT head scan data: Set of 491 head CT scans with pathology [no segmentation, but radiology report] (DICOM)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":25,"type":"A","href":"http:\u002F\u002Fheadctstudy.qure.ai\u002Fdataset","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_17":{"id":"824ceaaf7600_17","__typename":"Paragraph","name":"c960","text":"Here is a list of not so easy to download (but very useful) datasets.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":18,"end":29,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_18":{"id":"824ceaaf7600_18","__typename":"Paragraph","name":"9456","text":"BraTS 2018 Brain Tumor data: Large set of patients with brain tumors along with the tumor segmentation (mha)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":27,"type":"A","href":"http:\u002F\u002Fbraintumorsegmentation.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_19":{"id":"824ceaaf7600_19","__typename":"Paragraph","name":"25cd","text":"ISBI 2015 Multiple Sclerosis Challenge data: Set of 20 patients with multiple sclerosis (MS) [only 5 have lesion segmentations] (NIfTI)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":43,"type":"A","href":"https:\u002F\u002Fsmart-stats-tools.org\u002Flesion-challenge","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_20":{"id":"824ceaaf7600_20","__typename":"Paragraph","name":"a61e","text":"I have not worked with the set of datasets below, but I know people who have and am including them for completeness.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_21":{"id":"824ceaaf7600_21","__typename":"Paragraph","name":"c017","text":"OASIS","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":5,"type":"A","href":"https:\u002F\u002Fwww.oasis-brains.org\u002F#dictionary","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_22":{"id":"824ceaaf7600_22","__typename":"Paragraph","name":"b4be","text":"Low Dose CT","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":11,"type":"A","href":"https:\u002F\u002Fwww.aapm.org\u002FGrandChallenge\u002FLowDoseCT\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_23":{"id":"824ceaaf7600_23","__typename":"Paragraph","name":"3562","text":"fastMRI","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":7,"type":"A","href":"https:\u002F\u002Ffastmri.med.nyu.edu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_24":{"id":"824ceaaf7600_24","__typename":"Paragraph","name":"7152","text":"ADNI","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":4,"type":"A","href":"http:\u002F\u002Fadni.loni.usc.edu\u002Fdata-samples\u002Faccess-data\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_25":{"id":"824ceaaf7600_25","__typename":"Paragraph","name":"2dd4","text":"Another place to look for datasets is in OpenNeuro which is a repository for researchers to host their brain imaging datasets; it mostly consists of fMRI from what I can tell. If your passion lies somewhere besides MR and CT brain images, then I’m unfortunately not a great resource. My first guess would be to look at the “grand challenges” listed here and see if it is possible to gain access to the data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":41,"end":50,"type":"A","href":"https:\u002F\u002Fopenneuro.org","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":349,"end":353,"type":"A","href":"https:\u002F\u002Fgrand-challenge.org\u002Fchallenges\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":225,"end":231,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_26":{"id":"824ceaaf7600_26","__typename":"Paragraph","name":"9b8b","text":"Not to bury the lede too much, but perhaps the easiest way to get access to some of the above data is through this website. I’m not sure that everything is sanctioned to be on there, which is why I have delayed to bring this up.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":110,"end":122,"type":"A","href":"http:\u002F\u002Facademictorrents.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_27":{"id":"824ceaaf7600_27","__typename":"Paragraph","name":"4091","text":"Preprocessing","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_28":{"id":"824ceaaf7600_28","__typename":"Paragraph","name":"da1e","text":"The amount of data wrangling and preprocessing required to work with MR and CT can be considerable. I’ll outline the bare necessities below.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_29":{"id":"824ceaaf7600_29","__typename":"Paragraph","name":"42bc","text":"The first thing to consider is how to load the images into python. The simplest route is to use nibabel. Then you can simply use","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":96,"end":103,"type":"A","href":"http:\u002F\u002Fnipy.org\u002Fnibabel\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_30":{"id":"824ceaaf7600_30","__typename":"Paragraph","name":"7d38","text":"import nibabel as nib\ndata = nib.load('mydata.nii.gz').get_data()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_31":{"id":"824ceaaf7600_31","__typename":"Paragraph","name":"07fa","text":"to get a numpy array containing the data inside the mydata.nii.gz file. Note that I’ll refer to the indices of this 3D volume as a voxel which is the 3D-equivalent of a pixel for a 2D image. For work with brain images at least, I’d recommend always converting the files to NIfTI (which corresponds to the .nii or .nii.gz extension). I find converting everything to NIfTI first makes my life easier since I can assume all input images are of type NIfTI. Here is a tool to convert DICOM to NIfTI, here is a script to convert MHA to NIfTI and here is a script to convert PAR\u002FREC files to NIfTI. There are more file formats that you’ll probably need to work with, and you can use some of those scripts as inspiration to convert those file types.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":52,"end":65,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":453,"end":467,"type":"A","href":"http:\u002F\u002Fnipy.org\u002Fnibabel\u002Fdicom\u002Fdcm2nii_algorithms.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":495,"end":511,"type":"A","href":"https:\u002F\u002Fgist.github.com\u002Fjcreinhold\u002Fa26d6555b0e7aa28b79757f766640dd6","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":540,"end":575,"type":"A","href":"https:\u002F\u002Fgist.github.com\u002Fjcreinhold\u002Ffdd701211191450284c5718502eabbd4","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_32":{"id":"824ceaaf7600_32","__typename":"Paragraph","name":"d275","text":"We’ll first outline resampling, bias-field correction and registration which are staples of any medical image analysis. For these preprocessing steps, I’d recommend ANTs and specifically the ANTsPy variety (assuming you are coming from a python background). ANTs is actively maintained and has reliable tools to solve all of these (and many more) problems. Unfortunately, ANTsPy is not always easy to install, but I believe work is being done on it to solve some of the issues and once you are up and running with it you can access most of the tools ANTs offers natively from python. In particular, it supports the resampling, bias-field correction and registration preprocessing steps I’ll be discussing next.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_33":{"id":"824ceaaf7600_33","__typename":"Paragraph","name":"df11","text":"As with natural images, MR and CT images do not have a standard resolution or standard image size. I’d argue that this fact is of greater importance in MR and CT though and must be considered for optimal ML performance. Consider the following: you train a 3D convolutional neural network with data acquired at 1x1x3 mm³ resolution and then you input an image into the network with 1x1x1 mm³. I would expect the result to be sub-optimal since the convolutional kernels will not be using the same spatial information. This is debatable and I haven’t examined the problem closely, but the non-standard resolution is something to keep in mind if you run into problems at test time. We can naively address the non-standard resolution problem by resampling the image to a desired, standard resolution (with cubic B-splines, of course, for the best quality).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_34":{"id":"824ceaaf7600_34","__typename":"Paragraph","name":"f691","text":"For many applications, both MR and CT often require a process called registration in order to align objects across a set of images for direct comparison. Why would we want to do this? Let’s say you want to learn a function that takes an MR image and outputs an estimate of what the CT image would look like. If you have paired data (that is, an MR and CT image from the same patient), then a simple way of approaching this problem would be to learn the voxel-wise map between the image intensities. However, if the anatomy is not aligned in the image space, then we cannot learn this map in a supervised way. We solve this problem by registering the images and, in fact, we examine this problem in the experiment section.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":69,"end":81,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_35":{"id":"824ceaaf7600_35","__typename":"Paragraph","name":"dd09","text":"The next two problems (described in the next two paragraphs) are specific to MR. First is that we have inhomogeneous image intensities due to the scanner in MR images. Since this inhomogeneity is not a biological feature, we generally want to remove it and we do so with a process referred to as bias-field correction (I’ll discuss one solution in the experiment section).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":103,"end":153,"type":"A","href":"http:\u002F\u002Fjohnmuschelli.com\u002Fimaging_in_r\u002Finhomogeneity_correction_ms\u002Findex.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":296,"end":318,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_36":{"id":"824ceaaf7600_36","__typename":"Paragraph","name":"f9ed","text":"Another issue in MR are inconsistent tissue intensities across different MR scanners. While CT images have a standard intensity scale (see Hounsfield units), we are not so lucky with MR images. MR images absolutely do not have a standard scale, and the impact on algorithm performance can be quite large if not accounted for in preprocessing. See the images below for an example where we plot the histograms of a set of T1-weighted MR images without any intensity normalization applied (see the image with “Raw” in the title). This variation is due to effects caused by the scanner and not due to the biology, which is the thing we generally care about.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":139,"end":155,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FHounsfield_scale","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":292,"end":303,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1812.04652.pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":217,"end":222,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_37":{"id":"824ceaaf7600_37","__typename":"Paragraph","name":"aa53","text":"","type":"IMG","href":null,"layout":"OUTSET_ROW","metadata":{"__ref":"ImageMetadata:1*emSiC9f7Osn8ixkqUvOK9A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_38":{"id":"824ceaaf7600_38","__typename":"Paragraph","name":"efa2","text":"Histograms of image intensities from a cohort of T1-weighted brain images (left: white-matter mean normalized, right: no normalization applied)","type":"IMG","href":null,"layout":"OUTSET_ROW_CONTINUE","metadata":{"__ref":"ImageMetadata:1*e-05WKBZqnWyce97pbAjYw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_39":{"id":"824ceaaf7600_39","__typename":"Paragraph","name":"3184","text":"There are a litany of intensity normalization techniques that attempt to remove this scanner variation (several of which I have collected in this repository called intensity-normalization). The techniques range from the very simple (e.g., simple standardization which I’ll refer to as z-score normalization) to the fairly technical (e.g., RAVEL). For neuroimaging, a good combination of speed and quality can be found in the Fuzzy C-Means (FCM) normalization technique which creates a rough tissue-class segmentation between the white-matter (WM), grey-matter and cerebrospinal fluid based on the T1-weighted image. The WM segmentation mask is then used to calculate the mean of the WM in the image which is set to some user-defined constant. This normalization technique seems to almost always produce the desired result in brain images. If you are not working on brain images, then you may want to look at either the Nyúl & Udupa method or simple z-score normalization. All of these normalization methods are available as command-line interfaces (or importable modules) in the intensity-normalization repository.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":164,"end":187,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":1079,"end":1102,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":141,"end":156,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fintensity-normalization","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":246,"end":261,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FFeature_scaling#Standardization","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":339,"end":344,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FJfortin1\u002FRAVEL","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":919,"end":938,"type":"A","href":"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F10571928","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":949,"end":970,"type":"A","href":"https:\u002F\u002Fintensity-normalization.readthedocs.io\u002Fen\u002Flatest\u002Falgorithm.html#z-score","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_40":{"id":"824ceaaf7600_40","__typename":"Paragraph","name":"80b1","text":"The last preprocessing step we’ll consider is specific to brain images. In brain images, we generally only care about the brain and not necessarily the tissues outside of brain (e.g., the skull, fat and skin surrounding the brain). Furthermore, this extraneous tissue can complicate the learning procedure and trip up classification, segmentation, or regression tasks. To get around this we can use skull-stripping algorithms to create a mask of the brain and zero out the background. The simplest way to go about this (in MR) — with reasonable results — is with ROBEX: a command-line tool that generally does a good job at extracting the brain from the image. I’ve seen it fail a few times on some data containing large pathologies or imaging artifacts, but other than that it is usually good enough for most machine learning tasks. For what it’s worth, I’d try to avoid skull-stripping your data since it is just another point of possible failure in your preprocessing routine, but sometimes it substantially helps.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":563,"end":568,"type":"A","href":"https:\u002F\u002Fwww.nitrc.org\u002Fprojects\u002Frobex","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_41":{"id":"824ceaaf7600_41","__typename":"Paragraph","name":"8387","text":"Since MR and CT images aren’t standard like JPEG, your computer doesn’t have a native way to display it. If you want to visualize your data, take a look at MIPAV for non-DICOM images (e.g., NIfTI) and Horos for DICOM images. It is always good to look at your data, especially after preprocessing so we can verify that everything looks reasonable. For instance, perhaps the registration failed (it often does) or perhaps the skull-stripping failed (again, it often occurs). If you pipe your crappy data into your ML algorithm, you’re probably going to get crappy output and you’ll waste a lot of time doing unnecessary debugging. So be kind to yourself and examine the data.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":156,"end":161,"type":"A","href":"https:\u002F\u002Fmipav.cit.nih.gov\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":201,"end":206,"type":"A","href":"https:\u002F\u002Fhorosproject.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_42":{"id":"824ceaaf7600_42","__typename":"Paragraph","name":"a658","text":"Training a deep network for MR or CT applications","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_43":{"id":"824ceaaf7600_43","__typename":"Paragraph","name":"ae21","text":"While deep neural networks applied to MR and CT are increasingly moving to 3D models, there has been good success with 2D models. If you have limited memory on your GPU or you have very limited training data, you may want to use a 2D network to squeeze the most performance out of the network. If you use a 3D network, you will quickly run into memory issues when passing a full image or patches through the network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_44":{"id":"824ceaaf7600_44","__typename":"Paragraph","name":"4ecf","text":"If you decide a 2D network is the way to go for your application (a reasonable choice), you’ll need to figure out\u002Fdesign a data loader to handle this. After fussing around with complicated data loaders that take the 3D image to a 2D image patch or slice for a while, I realized that that was all an unnecessary burden that made it harder to use pre-built data loader\u002Fdata augmentation tools that aid in training. Thus my recommended solution to this problem is to simply convert the 3D volumes to 2D images. Since the original volumes are floating point numbers, I went with the TIFF image format which supports such types. Here is a command-line script which takes a directory of NIfTI images and creates a directory of corresponding 2D TIFF images (with some options to create slices based on axis and to only create slices from a portion of the image in order to avoid background slices).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":634,"end":653,"type":"A","href":"https:\u002F\u002Fgist.github.com\u002Fjcreinhold\u002F01daf54a6002de7bd8d58bad78b4022b","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_45":{"id":"824ceaaf7600_45","__typename":"Paragraph","name":"c72e","text":"In the following section, I’ll build a deep neural network with 3D convolutional layers. I’m doing this as opposed to using 2D convolutional layers because — once you convert the 3D volume to 2D images like TIFF — you can basically just use any 2D architecture you have lying around substituting the head for the appropriate application. Since the 3D problem is slightly more tricky to approach, I’ll dig into it below.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_46":{"id":"824ceaaf7600_46","__typename":"Paragraph","name":"e329","text":"Experiment","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_47":{"id":"824ceaaf7600_47","__typename":"Paragraph","name":"78aa","text":"*** If you are just coming to this blog post (after 05\u002F07\u002F20), note that the fastai package has changed significantly and the code below may not work as expected. However, the code examples and general experimental setup below should still be useful for learning purposes. For what it’s worth, I’d recommend using PyTorch over fastai for future deep learning projects. If you want NIfTI support in PyTorch, I have an actively maintained package which has working code examples and importable functions here. ***","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":502,"end":506,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fniftidataset","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":511,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":511,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_48":{"id":"824ceaaf7600_48","__typename":"Paragraph","name":"c6c5","text":"In this section, I’ll outline the steps required to train a 3D convolutional neural network for a MR-to-MR synthesis task using pytorch and fastai. If you just want to look at the code, then there is also a notebook which contains most of the experiment (excluding preprocessing) here.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":128,"end":135,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":140,"end":146,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Ffastai\u002Ffastai","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":280,"end":284,"type":"A","href":"https:\u002F\u002Fnbviewer.jupyter.org\u002Fgist\u002Fjcreinhold\u002F78943cdeca1c5fca4a5af5d066bd8a8d","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_49":{"id":"824ceaaf7600_49","__typename":"Paragraph","name":"98f9","text":"The setup is as follows: we’ll train a very small resnet to take an entire 3D volume from one MR contrast to another MR contrast; we’ll be learning the transform to map T1-weighted images to FLAIR images. This task is called MR image synthesis and we’ll refer to the network as a synthesis network. There are a variety of applications for this type of synthesis, but motivation for this problem is mostly that: MR scan time is limited, so not all contrasts can be collected. But we want to eat our cake and have it too, and we sometimes want those uncollected contrasts for image processing purposes. Thus we create some fake data using the data that actually was collected, where the fake data will be the result of our synthesis network.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":50,"end":56,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1512.03385","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":225,"end":244,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":280,"end":297,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_50":{"id":"824ceaaf7600_50","__typename":"Paragraph","name":"d3e7","text":"In this experiment, I’ll be using 11 and 7 images as training and validation, respectively, from the Kirby 21 dataset. All images have been resampled to 1x1x1 mm³, bias-field corrected using N4, and the FLAIR images have been (affine) registered to the T1-weighted images using ANTsPy. Look here and here for the actual code I used to do the preprocessing (both are available as command-line interfaces when the intensity-normalization package is installed along with ANTsPy). Finally, all the images were individually z-score normalized using the entire image.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":101,"end":109,"type":"A","href":"https:\u002F\u002Fwww.nitrc.org\u002Fprojects\u002Fmultimodal\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":191,"end":193,"type":"A","href":"https:\u002F\u002Fwww.ncbi.nlm.nih.gov\u002Fpubmed\u002F20378467","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":291,"end":295,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fintensity-normalization\u002Fblob\u002Fmaster\u002Fintensity_normalization\u002Futilities\u002Fpreprocess.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":300,"end":304,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fintensity-normalization\u002Fblob\u002Fmaster\u002Fintensity_normalization\u002Fexec\u002Fcoregister.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":412,"end":435,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fintensity-normalization","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":519,"end":537,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fjcreinhold\u002Fintensity-normalization\u002Fblob\u002Fmaster\u002Fintensity_normalization\u002Fnormalize\u002Fzscore.py","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_51":{"id":"824ceaaf7600_51","__typename":"Paragraph","name":"faaf","text":"Now that we have motivated the problem somewhat and talked about the data we will use, let’s get to the code. The code block below defines some necessary constructs to work with fastai, specifically to use the data_block API.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":210,"end":220,"type":"A","href":"https:\u002F\u002Fdocs.fast.ai\u002Fdata_block.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_52":{"id":"824ceaaf7600_52","__typename":"Paragraph","name":"2dc3","text":"Dataloaders classes and functions for NIfTI images in fastai","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:0cc5f39601abccec68a8b60f22519107"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_53":{"id":"824ceaaf7600_53","__typename":"Paragraph","name":"2be5","text":"There is nothing particular to remark on here, except that once you figure out how to setup these types of structures, they are quite convenient (see the ItemList tutorial for more details). Note that not all functionality is supported with the current setup — I stripped it down to make it as simple as possible — but it’ll get the job done. I’ll show how this creates the training and validation dataloaders below. First, let’s define a preprocessing transform:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":154,"end":171,"type":"A","href":"https:\u002F\u002Fdocs.fast.ai\u002Ftutorial.itemlist.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_54":{"id":"824ceaaf7600_54","__typename":"Paragraph","name":"7944","text":"Relevant data transforms for our application","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:97088d3a3c93c250e5a74802284c392d"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_55":{"id":"824ceaaf7600_55","__typename":"Paragraph","name":"00eb","text":"Why am I defining this odd cropping function? The reason is two-fold. The first reason is that the neck is not present in the FLAIR images but is present in the T1-weighted images. I don’t want the network to learn to take tissue to zero, so I’m removing that part of the data by only using the data in the 20–80 percent range along the axis corresponding to the axial plane. The second reason is that I can fit twice as many samples into a batch (that means a batch size of 2). The reason for the small batch size is that, like I previously mentioned, 3D networks with large images are memory-intensive. Why no other data augmentation? Unfortunately, 3D transforms are not natively supported with pytorch or fastai so I’d have to incorporate my own and I am not doing this for simplicity. Now let’s use the data_block API of fastai to create the training and validation dataloaders:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":363,"end":374,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FAnatomical_plane","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":143,"end":145,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_56":{"id":"824ceaaf7600_56","__typename":"Paragraph","name":"6c31","text":"Define the dataloader with the data block API in fastai","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:1e89d7fb1b9917ed4bc9c82423543014"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_57":{"id":"824ceaaf7600_57","__typename":"Paragraph","name":"346f","text":"You can see the notebook for more details, but essentially I have the T1-weighted images in one directory with train, valid, test subdirectories and a parallel directory with the FLAIR images. The get_y_fn function grabs the FLAIR image corresponding to the source T1-weighted image. Look here for more in-depth explanation of the remaining commands. Note that the (tfms,tfms) means that I am applying the previously defined crop to both the training and validation set. Applying that transform to the validation set isn’t ideal, but is required because of memory-constraints. Now let’s create some 3D convolutional and residual block layers which we’ll use to define our model:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":111,"end":129,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":197,"end":205,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":365,"end":376,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":16,"end":24,"type":"A","href":"https:\u002F\u002Fnbviewer.jupyter.org\u002Fgist\u002Fjcreinhold\u002F78943cdeca1c5fca4a5af5d066bd8a8d","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":289,"end":293,"type":"A","href":"https:\u002F\u002Fdocs.fast.ai\u002Fdata_block.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_58":{"id":"824ceaaf7600_58","__typename":"Paragraph","name":"adc9","text":"Define some custom 3D layers","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:b72213082b11c20c7be7332f8ef0b043"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_59":{"id":"824ceaaf7600_59","__typename":"Paragraph","name":"0f4a","text":"I’m closely following the definition of the 2D convolutional and residual block layers as defined in the fastai repository. As a side note, I left the spectral normalization and weight normalization routines in the conv3d definition, but disappointingly received worse results with those methods than when using batch norm (and I’m still not sure whether batch norm is applied before or after the activation). Now let’s define our model using the above layers:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":215,"end":221,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":105,"end":122,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Ffastai\u002Ffastai\u002Fblob\u002Fmaster\u002Ffastai\u002Flayers.py#L94","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_60":{"id":"824ceaaf7600_60","__typename":"Paragraph","name":"c82c","text":"Define our network","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:4f7f552f9c4b7b3ea9bb6c06a7ca48ab"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_61":{"id":"824ceaaf7600_61","__typename":"Paragraph","name":"14fd","text":"Here I have just defined the very small resnet model. Why so few layers? I am using as large of a network as my GPU can contain in memory. The creation of many channels with the entire 3D volume and the residual connections are burdens on the GPU memory. The only other thing of possible intrigue is that I use a 1x1x1 kernel at the end, which empirically produces crisper images (and I think is fairly standard). As a note, I realize that I should have removed the activation from the final layer; however, it is not a problem because I am z-score normalizing (i.e., mean subtracting and dividing by the standard deviation) the images with their backgrounds. The backgrounds, which are approximately zero, take up the vast majority of the volume of the image. Thus the z-score normalization essentially puts the background (corresponding to the mean) at zero, which makes the intensities of the head greater than zero. A fine result for the ReLU. Now let’s train this network:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":636,"end":640,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_62":{"id":"824ceaaf7600_62","__typename":"Paragraph","name":"4141","text":"Training","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:0ee74d82b296576c9ff2b6ade3a5bc5b"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_63":{"id":"824ceaaf7600_63","__typename":"Paragraph","name":"5c94","text":"Again fairly normal. Mean square error is used because we want each voxel intensity in our source T1-weighted image to match the voxel intensity of the corresponding target FLAIR image. We use lr_find to help us pick a larger learning rate (as described here) for faster training, in addition to using the one-cycle policy. I always collect my training and validation data into a CSV file to look at how the network is converging, especially on machines where launching a jupyter notebook is a pain. I picked 100 epochs because I ran this a couple times and did not notice a great amount performance gain with more epochs.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":193,"end":200,"type":"CODE","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":254,"end":258,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.01186","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":306,"end":315,"type":"A","href":"https:\u002F\u002Fsgugger.github.io\u002Fthe-1cycle-policy.html","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":316,"end":322,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1803.09820.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_64":{"id":"824ceaaf7600_64","__typename":"Paragraph","name":"6965","text":"After training completes, we input an entire image (not seen in either training or validation) into the network. An example is shown in below figure, where the synthesis result is the right-most image (with the title “Syn”).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_65":{"id":"824ceaaf7600_65","__typename":"Paragraph","name":"5f2f","text":"Example synthesis result with the network defined above (from left to right: the input T1-weighted image, the true FLAIR image, and the synthesized FLAIR image)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*vG9FkLP9QctoxmI6j72gUQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_66":{"id":"824ceaaf7600_66","__typename":"Paragraph","name":"e029","text":"While the above figure could have been had better window\u002Flevel settings for better comparison, we see that the T1-weighted image does take on many of the characteristics of the true FLAIR image. Most notably, inside the brain, we see that the white-matter becomes less bright than the grey-matter while the cerebrospinal fluid remains dark. The noise characteristics are not the same though and there are some bright spots in the true FLAIR that are not captured in the synthesized image.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":50,"end":71,"type":"A","href":"http:\u002F\u002Fwww.upstate.edu\u002Fradiology\u002Feducation\u002Frsna\u002Fintro\u002Fdisplay.php","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":371,"end":375,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_67":{"id":"824ceaaf7600_67","__typename":"Paragraph","name":"f6fd","text":"This result is not state-of-the-art by any means, but it’s interesting to see that we can learn an approximate transform with such an incredibly small dataset, no data augmentation, and a very small network. This network would assuredly be better with more data, data augmentation and a larger network, but this is just a simple, pedagogical toy example. I should note that unless you have a particularly large GPU (and contradicting my last statement), you may not be able to train this network with the full images. You’ll probably have to use either 3D patches or 2D slices (or 2D patches).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_68":{"id":"824ceaaf7600_68","__typename":"Paragraph","name":"9e54","text":"Conclusion","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_69":{"id":"824ceaaf7600_69","__typename":"Paragraph","name":"8f8a","text":"Hopefully this post provided you with a starting point for applying deep learning to MR and CT images with fastai. Like most machine learning tasks, there is a considerable amount of domain-specific knowledge, data-wrangling and preprocessing that is required to get started, but once you have this under your belt, it is fairly easy to get up-and-running with training a network with pytorch and fastai. Where to go from here? I’d download a dataset from one of the links I posted in the Datasets section and try to do something similar to what I showed above, or even just try to recreate what I did. If you can get to that stage, you’ll be in a comfortable place to apply deep learning to other problems in MR and CT.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:824ceaaf7600_70":{"id":"824ceaaf7600_70","__typename":"Paragraph","name":"d72f","text":"I should note that there is work being done to create standard code bases from which you can apply deep learning to MR and CT images. The two that I am aware of are NiftyNet and medicaltorch. NiftyNet abstracts away most of the neural network design and data handling, so that the user only has to call some command-line interfaces by which to download a pre-trained network, fine-tune it, and do whatever. So if that is good enough for your needs, then go right ahead; it seems like a great tool and has some pre-trained networks available. medicaltorch provides some dataloaders and generic deep learning models with medical images in pytorch. I have not tested either extensively, so I cannot comment on their utility.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":165,"end":173,"type":"A","href":"http:\u002F\u002Fwww.niftynet.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":178,"end":190,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fperone\u002Fmedicaltorch","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_71":{"id":"824ceaaf7600_71","__typename":"Paragraph","name":"562e","text":"If you don’t like python, there is neuroconductor in R or NIfTI.jl and Flux.jl packages in Julia which can read NIfTI images and build neural networks, respectively. There are countless other relevant software packages, but those are the ones the first come to mind and that I’ve worked with.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":35,"end":49,"type":"A","href":"https:\u002F\u002Fneuroconductor.org\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":58,"end":66,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FJuliaIO\u002FNIfTI.jl","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":71,"end":78,"type":"A","href":"https:\u002F\u002Ffluxml.ai\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:824ceaaf7600_72":{"id":"824ceaaf7600_72","__typename":"Paragraph","name":"ff52","text":"As a final note, if you have luck creating a nice application for MR or CT make sure to share your work! Write a paper\u002Fblog post, put it up on a forum, share the network weights. It would be great to see more people apply deep learning techniques to this domain and push the boundary of the field where possible. Best of luck.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":152,"end":177,"type":"A","href":"https:\u002F\u002Fpytorch.org\u002Fdocs\u002Fmaster\u002Fhub.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*BaCHM97Tmm5p292b4xk1yw.jpeg":{"id":"1*BaCHM97Tmm5p292b4xk1yw.jpeg","__typename":"ImageMetadata","originalHeight":512,"originalWidth":512,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*jTX0eIVUVngUmiP8GN-lPg.png":{"id":"1*jTX0eIVUVngUmiP8GN-lPg.png","__typename":"ImageMetadata","originalHeight":256,"originalWidth":204,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*emSiC9f7Osn8ixkqUvOK9A.png":{"id":"1*emSiC9f7Osn8ixkqUvOK9A.png","__typename":"ImageMetadata","originalHeight":1000,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*e-05WKBZqnWyce97pbAjYw.png":{"id":"1*e-05WKBZqnWyce97pbAjYw.png","__typename":"ImageMetadata","originalHeight":1000,"originalWidth":1200,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:0cc5f39601abccec68a8b60f22519107":{"id":"0cc5f39601abccec68a8b60f22519107","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"dataloaders for fastai"},"MediaResource:97088d3a3c93c250e5a74802284c392d":{"id":"97088d3a3c93c250e5a74802284c392d","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"transforms for fastai"},"MediaResource:1e89d7fb1b9917ed4bc9c82423543014":{"id":"1e89d7fb1b9917ed4bc9c82423543014","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"databunch example definiton for fastai"},"MediaResource:b72213082b11c20c7be7332f8ef0b043":{"id":"b72213082b11c20c7be7332f8ef0b043","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"layers for fastai"},"MediaResource:4f7f552f9c4b7b3ea9bb6c06a7ca48ab":{"id":"4f7f552f9c4b7b3ea9bb6c06a7ca48ab","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"define basic resnet"},"MediaResource:0ee74d82b296576c9ff2b6ade3a5bc5b":{"id":"0ee74d82b296576c9ff2b6ade3a5bc5b","__typename":"MediaResource","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"training of 3D resnet"},"ImageMetadata:1*vG9FkLP9QctoxmI6j72gUQ.png":{"id":"1*vG9FkLP9QctoxmI6j72gUQ.png","__typename":"ImageMetadata","originalHeight":1054,"originalWidth":2554,"focusPercentX":null,"focusPercentY":null,"alt":null},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning"},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning"},"Tag:medical-imaging":{"id":"medical-imaging","__typename":"Tag","displayTitle":"Medical Imaging"},"Tag:python":{"id":"python","__typename":"Tag","displayTitle":"Python"},"Tag:pytorch":{"id":"pytorch","__typename":"Tag","displayTitle":"Pytorch"},"Post:e9f32273dcb5":{"id":"e9f32273dcb5","__typename":"Post","canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:824ceaaf7600_0"},{"__ref":"Paragraph:824ceaaf7600_1"},{"__ref":"Paragraph:824ceaaf7600_2"},{"__ref":"Paragraph:824ceaaf7600_3"},{"__ref":"Paragraph:824ceaaf7600_4"},{"__ref":"Paragraph:824ceaaf7600_5"},{"__ref":"Paragraph:824ceaaf7600_6"},{"__ref":"Paragraph:824ceaaf7600_7"},{"__ref":"Paragraph:824ceaaf7600_8"},{"__ref":"Paragraph:824ceaaf7600_9"},{"__ref":"Paragraph:824ceaaf7600_10"},{"__ref":"Paragraph:824ceaaf7600_11"},{"__ref":"Paragraph:824ceaaf7600_12"},{"__ref":"Paragraph:824ceaaf7600_13"},{"__ref":"Paragraph:824ceaaf7600_14"},{"__ref":"Paragraph:824ceaaf7600_15"},{"__ref":"Paragraph:824ceaaf7600_16"},{"__ref":"Paragraph:824ceaaf7600_17"},{"__ref":"Paragraph:824ceaaf7600_18"},{"__ref":"Paragraph:824ceaaf7600_19"},{"__ref":"Paragraph:824ceaaf7600_20"},{"__ref":"Paragraph:824ceaaf7600_21"},{"__ref":"Paragraph:824ceaaf7600_22"},{"__ref":"Paragraph:824ceaaf7600_23"},{"__ref":"Paragraph:824ceaaf7600_24"},{"__ref":"Paragraph:824ceaaf7600_25"},{"__ref":"Paragraph:824ceaaf7600_26"},{"__ref":"Paragraph:824ceaaf7600_27"},{"__ref":"Paragraph:824ceaaf7600_28"},{"__ref":"Paragraph:824ceaaf7600_29"},{"__ref":"Paragraph:824ceaaf7600_30"},{"__ref":"Paragraph:824ceaaf7600_31"},{"__ref":"Paragraph:824ceaaf7600_32"},{"__ref":"Paragraph:824ceaaf7600_33"},{"__ref":"Paragraph:824ceaaf7600_34"},{"__ref":"Paragraph:824ceaaf7600_35"},{"__ref":"Paragraph:824ceaaf7600_36"},{"__ref":"Paragraph:824ceaaf7600_37"},{"__ref":"Paragraph:824ceaaf7600_38"},{"__ref":"Paragraph:824ceaaf7600_39"},{"__ref":"Paragraph:824ceaaf7600_40"},{"__ref":"Paragraph:824ceaaf7600_41"},{"__ref":"Paragraph:824ceaaf7600_42"},{"__ref":"Paragraph:824ceaaf7600_43"},{"__ref":"Paragraph:824ceaaf7600_44"},{"__ref":"Paragraph:824ceaaf7600_45"},{"__ref":"Paragraph:824ceaaf7600_46"},{"__ref":"Paragraph:824ceaaf7600_47"},{"__ref":"Paragraph:824ceaaf7600_48"},{"__ref":"Paragraph:824ceaaf7600_49"},{"__ref":"Paragraph:824ceaaf7600_50"},{"__ref":"Paragraph:824ceaaf7600_51"},{"__ref":"Paragraph:824ceaaf7600_52"},{"__ref":"Paragraph:824ceaaf7600_53"},{"__ref":"Paragraph:824ceaaf7600_54"},{"__ref":"Paragraph:824ceaaf7600_55"},{"__ref":"Paragraph:824ceaaf7600_56"},{"__ref":"Paragraph:824ceaaf7600_57"},{"__ref":"Paragraph:824ceaaf7600_58"},{"__ref":"Paragraph:824ceaaf7600_59"},{"__ref":"Paragraph:824ceaaf7600_60"},{"__ref":"Paragraph:824ceaaf7600_61"},{"__ref":"Paragraph:824ceaaf7600_62"},{"__ref":"Paragraph:824ceaaf7600_63"},{"__ref":"Paragraph:824ceaaf7600_64"},{"__ref":"Paragraph:824ceaaf7600_65"},{"__ref":"Paragraph:824ceaaf7600_66"},{"__ref":"Paragraph:824ceaaf7600_67"},{"__ref":"Paragraph:824ceaaf7600_68"},{"__ref":"Paragraph:824ceaaf7600_69"},{"__ref":"Paragraph:824ceaaf7600_70"},{"__ref":"Paragraph:824ceaaf7600_71"},{"__ref":"Paragraph:824ceaaf7600_72"}],"sections":[{"__typename":"Section","name":"735f","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"creator":{"__ref":"User:d6cf1d0b0aa7"},"customStyleSheet":{"__ref":"CustomStyleSheet:396be67fd6cb"},"firstPublishedAt":1546792699587,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":3,"primaryTopic":{"__typename":"Topic","name":"Machine Learning","slug":"machine-learning","isFollowing":null},"title":"Deep Learning with Magnetic Resonance and Computed Tomography Images","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fdeep-learning-with-magnetic-resonance-and-computed-tomography-images-e9f32273dcb5","isLimitedState":false,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:medical-imaging"},{"__ref":"Tag:python"},{"__ref":"Tag:pytorch"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning","slug":"machine-learning"},{"__typename":"Topic","topicId":"ae5d4995e225","name":"Data Science","slug":"data-science"},{"__typename":"Topic","topicId":"decb52b64abf","name":"Programming","slug":"programming"}],"viewerClapCount":0,"showSubscribeToProfilePromo":false,"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1588856679680,"readingTime":17.591823899371068,"previewContent":{"__typename":"PreviewContent","subtitle":"An introduction to the data, preprocessing techniques and deep network design for medical images"},"previewImage":{"__ref":"ImageMetadata:1*BaCHM97Tmm5p292b4xk1yw.jpeg"},"creatorPartnerProgramEnrollmentStatus":"PERMISSION_DENIED","clapCount":814,"lockedSource":"LOCKED_POST_SOURCE_NONE","isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"shareKey":null,"responsesCount":10,"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","updatedAt":1588856680013,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","postResponses":{"__typename":"PostResponses","count":10},"latestPublishedVersion":"824ceaaf7600","readingList":"READING_LIST_NONE","voterCount":204,"recommenders":[]}}</script><script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/manifest.js"></script><script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/9121.js"></script><script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/main.js"></script><script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/5573.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/instrumentation.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/reporting.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1826.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/4464.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/8342.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1148.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/5064.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/9274.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2846.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/4328.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/7993.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/6839.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/353.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/8751.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2054.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/8127.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/7131.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/8825.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/5279.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/9978.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/3721.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2514.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/2602.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/6585.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/1838.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/9889.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/3981.js"></script>
<script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/Post.js"></script><script>window.main();</script><script src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/p.js" async="" id="parsely-cf"></script><iframe src="Deep%20Learning%20with%20Magnetic%20Resonance%20and%20Computed%20Tomography%20Images%20by%20Jacob%20Reinhold%20Towards%20Data%20Science_fi%C8%99iere/a16180790160.htm" tabindex="-1" title="Optimizely Internal Frame" style="display: none;" width="0" hidden="" height="0"></iframe></body></html>